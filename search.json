[
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#introduction",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#introduction",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Introduction",
    "text": "Introduction\nThe business registration landscape of a country often serves as a barometer for its economic health and entrepreneurial spirit. In Kenya, a nation known for its dynamic economy and political atmosphere, tracking these registrations can provide valuable insights into the country’s economic trajectory and the impact of political transitions on business confidence.\nSince the post-election violence of 2007 that saw both the current president (William Ruto) and his predecessor (Uhuru Kenyatta) defend themselves at the Hague, there has always been a slowdown of the economy around general elections. On the day of the election, the economy literally stops, evidenced by closure of all businesses and related activities such as delivery, money transfer, and investments. The Nairobi Securities Exchange (NSE) also loses significant investments during this time period. Open markets are often closed, public transport is left scanty, and most people travel back to rural areas either to vote or for security reasons. During this time, people want to know their neighbors and have familiar faces around them. People often group along tribal lines. During this frenzy atmosphere, people are often less inclined to share personal information or opinions out loud.\nThis analysis delves into data from the Business Registration Service (BRS)1 of Kenya, covering a decade from financial year 2015/2016 to 2024/2025. This period is particularly interesting as it encompasses two distinct political eras: the Uhuru Kenyatta/Jubilee Party era and the William Ruto/UDA Party era. By examining the trends in business registrations across these periods, we aim to uncover patterns that may reflect the broader economic and political climate of Kenya."
  },
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#methodology",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#methodology",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Methodology",
    "text": "Methodology\nOur analysis utilizes data scraped from the BRS website, focusing on monthly registration figures for various types of business entities. The data was processed and visualized using Python, with libraries such as pandas for data manipulation and matplotlib for creating insightful graphs.\nThe visualization process involved: 1. Aggregating monthly data across multiple financial years 2. Plotting Total Registrations of Entities, Year-over-Year Growth Rate of Business Entity Registrations and Trend, Seasonality and Residuals 3. Marking significant political events, such as the 2017 and 2022 elections"
  },
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#analysis",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#analysis",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Analysis",
    "text": "Analysis\n\n\nShow python imports\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add root directory as python path\nroot_dir = os.path.abspath(Path(sys.executable).parents[2])\nsys.path.append(root_dir)\n\n%reload_ext autoreload\n%autoreload 2\n\n# Other imports\nimport pandas as pd\nfrom playwright.async_api import Page\nfrom python_utils.web_screenshot import web_screenshot_async\nimport io\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport numpy as np\nfrom datetime import date, timedelta\nfrom calendar import monthrange, month_abbr\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\nfrom typing import Callable\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport textwrap\nfrom typing import Literal\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom datetime import date\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\nBelow is an image of the webpage we are going to crawl.\n\n\nShow Code\nasync def page_action_fn(page: Page):\n    return await page.wait_for_selector(\n        '.elementor-widget-container &gt; [role=\"tablist\"]')\n\n# Take a screenshot\nawait web_screenshot_async(\n    \"https://brs.go.ke/companies-registry-statistics/\",\n    action = page_action_fn,\n    width = 1200,\n    height = 1200)\n\n\n\n\n\n\n\n\n\nBelow code crawls the webpage, and saves the records into a dataframe.\n\n\nShow Code\ndef get_date(month, year):\n    # Convert month name to number\n    month_num = [i.lower() for i in month_abbr].index(month.lower())\n    # Get the last day of the month\n    _, last_day = monthrange(int(year), month_num)\n    return date(int(year), month_num, last_day)\n\ndef get_table(index: int):\n    (finantial_year, table_str) = years_records[index]\n    (finantial_year_1, finantial_year_2) = finantial_year.split('/')\n    df = pd.read_html(io.StringIO(str(table_str)))[0]\n    first_column = df.columns[0]\n    # Remove last row (`Total Entities Registered`) and \n    # last column (`Grand Total`)\n    df = df[df[first_column] != \"Total Entities Registered\"]\\\n        .drop(\"Grand Total\", axis=1)\n    # replace - with NaN\n    df = df.replace(\"-\", np.nan)\n    df[first_column] = df[first_column].astype(str)\n    for column in df.columns[1:]:\n        df[column] = df[column].astype(float)\n    indexes = [\n        get_date(month, finantial_year_1) \n        for month \n        in df.columns[1:7]\n    ] + [\n        get_date(month, finantial_year_2) \n        for month \n        in df.columns[7:]\n    ]\n    df = df.set_index(first_column).T\n    df.index = indexes\n    df.columns = [i.lower().strip() for i in df.columns]\n    return df\n\nhtml: str = urlopen(\n    \"https://brs.go.ke/companies-registry-statistics/\").read()\nhtml_parser = BeautifulSoup(html, \"html.parser\").select_one(\n    '.elementor-widget-container &gt; [role=\"tablist\"]')\nyears = { \n    i.attrs['data-tab']: i.get_text(separator='', strip=True) \n    for i \n    in html_parser.select(\".ha-tabs__nav .ha-tab__title\")\n}\nrecords = { \n    i.attrs['data-tab']: i.find('table') \n    for i \n    in html_parser.select('.ha-tabs__content [role=\"tabpanel\"]') \n}\nyears_records = [(year, records[id]) for id, year in years.items()]\n\nall_registrations = pd.concat([\n    get_table(i) \n    for i \n    in range(len(years_records))\n]).sort_index(ascending=True)\nall_registrations.index = pd.to_datetime(all_registrations.index)\nall_registrations\n\n\n\n\n\n\n\n\n\n\nlimited partnerships (lp)\nbusiness names\nprivate companies\npublic companies\nforeign companies\ncompanies limited by guarantee (clgs)\nlimited liability partnerships (llps)\nstrike off applications\n\n\n\n\n2015-07-31\nNaN\n5378.0\n2964.0\n6.0\n10.0\n14.0\nNaN\nNaN\n\n\n2015-08-31\nNaN\n5273.0\n3931.0\n3.0\nNaN\n9.0\nNaN\nNaN\n\n\n2015-09-30\nNaN\n4884.0\n2953.0\n4.0\n5.0\n3.0\nNaN\nNaN\n\n\n2015-10-31\nNaN\n4806.0\n3029.0\nNaN\nNaN\n9.0\nNaN\nNaN\n\n\n2015-11-30\nNaN\n2183.0\n2522.0\n5.0\nNaN\n10.0\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2025-02-28\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2025-03-31\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2025-04-30\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2025-05-31\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2025-06-30\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n120 rows × 8 columns\n\n\n\n\nRegistered Entities over Time\n\n\nShow Code\ndef draw_election_lines(\n        start_date, end_date, ax: matplotlib.axes.Axes, election_dates_y):\n    date_list = [\n        min(start_date, date(2017, 1, 1)) + timedelta(days=x) \n        for x in range((end_date - start_date).days + 1)]\n    # Add a vertical line at the split date\n    election_date_2017 = date(2017, 8, 9)\n    election_date_2022 = date(2022, 8, 9)\n    ax.axvline(\n        x=election_date_2017, color='green', linestyle='--', \n        linewidth=2, zorder=4)\n    ax.axvline(\n        x=election_date_2022, color='green', linestyle='--', \n        linewidth=2, zorder=4)\n    # Fill the regions\n    # Convert dates to matplotlib date format\n    dates_mpl = mdates.date2num(date_list)\n    split_date_mpl = mdates.date2num(election_date_2022)\n    ax.fill_between(\n        dates_mpl, 0, 100, where=dates_mpl &lt; split_date_mpl, \n        facecolor='#f62f3c', alpha=0.08, transform=ax.get_xaxis_transform())\n    ax.fill_between(\n        dates_mpl, 0, 100, where=dates_mpl &gt;= split_date_mpl,\n        facecolor='#f8c811', alpha=0.08, transform=ax.get_xaxis_transform())\n    ax.text(\n        election_date_2017, election_dates_y[0], '2017 election', fontsize = 18, \n        rotation=90, zorder=6, ha='right')\n    ax.text(\n        election_date_2022, election_dates_y[1], '2022 election', fontsize = 18, \n        rotation=90, zorder=6, ha='right')\n\ndef plot_data(\n        data_to_plot: pd.DataFrame | list[pd.DataFrame], title_1st_part: str,\n        election_dates_y = (7200, 8500), legend_loc='upper left',\n        axis_callback: Callable[[matplotlib.figure.Figure, matplotlib.axes.Axes], None] = None,\n        ylabel = 'Registered Entities'):\n    fig, ax = plt.subplots(figsize=(20, 10))\n    data_to_plot: list[pd.DataFrame] = data_to_plot\\\n        if type(data_to_plot) == list else [data_to_plot]\n    columns_used = [j for i in data_to_plot for j in i.columns]\n    start_date = min(min(i.index.date) for i in data_to_plot)\n    end_date = max(max(i.index.date) for i in data_to_plot)\n    ax.set_title(\n        f'{title_1st_part} ({start_date.strftime(\"%b %Y\")} to {end_date.strftime(\"%b %Y\")})', \n        fontsize = 24, pad = 45)\n    if columns_used:\n        ax.text(0.5,  1.03, \n            f'{\", \".join(columns_used[:-1]) + \" and \" + columns_used[-1]}' \n                if len(columns_used) &gt; 1  else columns_used[0], \n            transform=ax.transAxes, fontsize=14, ha='center',  style='italic')\n    for subdata in data_to_plot:\n        ax.plot(subdata.index, subdata.values, linewidth=4)\n    ax.legend(columns_used, loc=legend_loc, fontsize=18)\n    draw_election_lines(start_date, end_date, ax, election_dates_y)\n    fig.text(0.72, 0.035, 'Ruto/UDA era', fontsize = 20, ha='left')\n    fig.text(0.35, 0.035, 'Ruto/UDA era', fontsize = 20, ha='left')\n    # Add a watermark to the center of the plot\n    ax.text(1, 0.2, 'ToKnow.ai', ha='right', va='bottom', \n        fontsize=18, color='gray', alpha=0.5, transform=ax.transAxes, rotation=50)\n    ax.set_xlabel('Years')\n    ax.set_ylabel(ylabel)\n    if axis_callback:\n        axis_callback(fig, ax)\n\n\n\n\nShow Code\nplot_columns = ['business names', 'private companies']\nother_plot_columns = list(set(all_registrations.columns) - set(plot_columns))\nplot_data(\n    data_to_plot = all_registrations[plot_columns].dropna(),\n    title_1st_part = 'Registered Entities in Kenya over Time')\n\n\n\n\n\n\n\n\n\nThe graph above shows the registration trends for business names and private companies, which are the most common forms of entity registration in Kenya. We can observe that:\n\nBusiness names registrations peaked around 2021-07-31 before showing a downward trend.\nPrivate companies registrations peaked around 2022-11-30 before starting to decline.\n\n\n\nShow Code\nplot_data(\n    data_to_plot = [\n        all_registrations[[column]].dropna()\n        for column \n        in other_plot_columns\n    ],\n    title_1st_part = 'Registered Entities in Kenya over Time',\n    election_dates_y = (100, 250))\n\n\n\n\n\n\n\n\n\nThis second graph displays the registration trends for other entity types, including limited partnerships (LP), public companies, foreign companies, companies limited by guarantee (CLGs), limited liability partnerships (LLPs), and strike off applications. These registrations are relatively low, typically less than 500 per month, except for strike off applications which show a notable increase peaking at around 2021-01-31.\n\n\nTotal Registrations of Entities\n\n\nShow Code\nplot_data(\n    data_to_plot = \n        all_registrations[plot_columns].dropna().resample('YE').sum().dropna(),\n    title_1st_part = 'Total Entity Registrations per Year in Kenya',\n    election_dates_y = (60000, 60000))\n\n\n\n\n\n\n\n\n\nThe annual totals for business names and private companies registrations reveal:\n\nBusiness names registrations peaked in 2021 before declining.\nPrivate companies registrations continued to grow, peaking in early 2024.\n\n\n\nShow Code\nplot_data(\n    data_to_plot = [\n        all_registrations[[column]].dropna().resample('YE').sum().dropna() \n        for column \n        in other_plot_columns\n    ],\n    title_1st_part = 'Total Entity Registrations per Year in Kenya',\n    election_dates_y = (900, 900))\n\n\n\n\n\n\n\n\n\nFor other entity types, the annual totals remain relatively low, except for strike off applications which show a significant increase starting in 2021, peaking in 2022, and then declining.\n\n\nYear-over-Year Growth Rate of Business Entity Registrations\n\n\nShow Code\nplot_data(\n    data_to_plot = all_registrations[plot_columns]\n        .dropna().resample('YE').sum().dropna().pct_change() * 100,\n    title_1st_part = 'Year-over-Year Growth Rate of Business Name Registrations',\n    election_dates_y = (-20, 40),\n    legend_loc = 'upper right',\n    axis_callback= lambda fig, ax: ax.axhline(\n        y=0, color='purple', linestyle='--', linewidth=.5))\n\n\n\n\n\n\n\n\n\nThe year-over-year growth rates for business names and private companies show:\n\nA generally declining trend in growth rates for both entity types.\nA brief uptick in growth rates around 2021, followed by a continous decline.\nNegative growth rates in recent years, indicating a contraction in new registrations.\n\n\n\nShow Code\nplot_data(\n    data_to_plot = [\n        all_registrations[[column]]\n            .dropna().resample('YE').sum().dropna().pct_change() * 100\n        for column \n        in other_plot_columns\n    ],\n    title_1st_part = 'Year-over-Year Growth Rate of Business Name Registrations',\n    election_dates_y = (450, 300),\n    legend_loc = 'upper right',\n    axis_callback= lambda fig, ax: ax.axhline(\n        y=0, color='purple', linestyle='--', linewidth=.5))\n\n\n\n\n\n\n\n\n\nGrowth rates for other entity types show high volatility due to their lower numbers, but generally appear to be lower after 2022.\n\n\nTrend, Seasonality and Residuals\n\n\nShow Code\ndef plot_trend_and_seasonality(\n        columns_used: list[str], trend_period = 12, title_wrap: int = None,\n        title_args = { 'fontsize': 18, 'fontweight': 'bold', 'fontstyle': 'italic' },\n        election_dates_y_trend = (6000, 4600), election_dates_y_resid = (1100, 1100)):\n    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 20))\n    start_date: date = None\n    end_date: date = None\n    actual_columns_used = []\n    for column_used in columns_used:\n        plot_series = all_registrations[column_used].dropna()\n        if len(plot_series) &lt; 2 * trend_period:\n            continue\n        actual_columns_used.append(column_used)\n        __start_date = min(plot_series.index.date)\n        __end_date = max(plot_series.index.date)\n        start_date = min(start_date or __start_date, __start_date or start_date)\n        end_date = min(end_date or __end_date, __end_date or end_date)\n        decomposition = seasonal_decompose(\n            plot_series, model='additive', period=trend_period)\n        # Plot trend component\n        ax1.plot(\n            decomposition.trend.index, decomposition.trend.values, label = column_used)\n        ax1.set_ylabel('Trend')\n        ax1.set_xlabel('Years')\n        # Plot seasonal component\n        ax2.plot(\n            decomposition.seasonal.index, decomposition.seasonal.values, label = column_used)\n        ax2.axhline(y=0, color='r', linestyle='--', linewidth=.5)\n        ax2.set_ylabel('Seasonality')\n        ax2.set_xlabel('Years')\n        # Plot residual component\n        ax3.plot(\n            decomposition.resid.index, decomposition.resid.values, label = column_used)\n        ax3.axhline(y=0, color='r', linestyle='--', linewidth=.5)\n        ax3.set_ylabel('Residuals')\n        ax3.set_xlabel('Years')\n    if start_date or end_date:\n        draw_election_lines(start_date, end_date, ax1, election_dates_y_trend)\n        draw_election_lines(start_date, end_date, ax3, election_dates_y_resid)\n        title_template = (\n            '{0} - '\n            f'{\", \".join(actual_columns_used[:-1])} & {actual_columns_used[-1]}'\n            if len(actual_columns_used) &gt; 1  else actual_columns_used[0]\n        )\n        get_title = lambda s1: \"\\n\".join(textwrap.wrap(title_template.format(s1), width=title_wrap)) \\\n            if title_wrap \\\n            else title_template.format(s1)\n        ax1.set_title(get_title('Trend'), **title_args)\n        ax2.set_title(get_title('Seasonality'), **title_args)\n        ax3.set_title(get_title('Residuals'), **title_args)\n        ax1.legend(fontsize=16)\n        ax2.legend(fontsize=16)\n        ax3.legend(fontsize=16)\n        # Add a watermark to the center of the plot\n        ax3.text(\n            1, 0.2, 'ToKnow.ai', ha='right', va='bottom', fontsize=18, color='gray', \n            alpha=0.5, transform=ax3.transAxes, rotation=50)\n        fig.tight_layout(h_pad=5, pad=4)\n        fig.text(\n            x = .5, y = 1, fontsize = 26, ha='center',\n            s = (\n                f'{trend_period}-Month Summary of Registered Entities in Kenya '\n                f'({start_date.strftime(\"%b %Y\")} to {end_date.strftime(\"%b %Y\")})'\n            ))\n    else:\n        fig.clear()\n        fig.axes.clear()\n\nplot_trend_and_seasonality(plot_columns)\n\n\n\n\n\n\n\n\n\nThe decomposition of the time series for business names and private companies reveals:\n\nA clear upward trend until 2021, followed by a decline.\nStrong seasonality, with peaks typically occurring in January-March and August.\nResiduals showing increased volatility in recent years.\n\n\nBusiness names and Private company registrations follow a distinct seasonal pattern throughout the year:\n\nJanuary - March: The year starts with a surge in entrepreneurial spirit, peaking around March as people act on their New Year’s resolutions.\nApril - May: Registrations decline as initial enthusiasm wanes. However, a mid-year rejuvenation occurs in May as people realize the year is almost half over.\nJune - August: A new peak emerges in June, followed by a brief dip in July. August sees the highest registration numbers of the year as people prepare for the final quarter.\nSeptember - December: A downward trend begins, reaching the year’s lowest point in December, likely due to end-of-year festivities and holiday preparations.\n\n\n\nShow Code\nplot_trend_and_seasonality(\n    other_plot_columns, title_wrap = 100,\n    title_args = { 'fontsize': 14, 'fontweight': 'bold', 'fontstyle': 'italic' },\n    election_dates_y_trend = (60, 60), election_dates_y_resid = (-95, 50))\n\n\n\n\n\n\n\n\n\nFor other entity types, the trends are less pronounced due to lower numbers, but still show some seasonality and increasing volatility in residuals over time.\n\n\nInteractive Plots\nThe interative plots are for easier interaction.\n\n\nShow Code\ndef get_decomposition(\n        columns_used: list[str], trend_period = 12, \n        out: dict[Literal['start_date', 'end_date', 'actual_columns_used'], str] = None):\n    actual_columns_used = []\n    for column_used in columns_used:\n        plot_series = all_registrations[column_used].dropna()\n        if len(plot_series) &lt; 2 * trend_period:\n            continue\n        actual_columns_used.append(column_used)\n        __start_date = min(plot_series.index.date)\n        __end_date = max(plot_series.index.date)\n        out['start_date'] = min(out['start_date'] or __start_date, __start_date)\n        out['end_date'] = max(out['end_date'] or __end_date, __end_date)\n        out['actual_columns_used'].append(column_used)\n        yield seasonal_decompose(plot_series, model='additive', period=trend_period), column_used\n\ndef draw_election_lines_plotly(start_date, end_date, fig, row, election_dates_y):\n    date_list = [\n        min(start_date, date(2017, 1, 1)) + timedelta(days=x) \n        for x \n        in range((end_date - start_date).days + 1)\n    ]\n    election_date_2017 = date(2017, 8, 9)\n    election_date_2022 = date(2022, 8, 9)\n    # Add vertical lines for election dates\n    fig.add_vline(\n        x=election_date_2017, line=dict(color='green', width=2, dash='dash'), row=row, col=1)\n    fig.add_vline(\n        x=election_date_2022, line=dict(color='green', width=2, dash='dash'), row=row, col=1)\n    # Add colored regions\n    fig.add_vrect(x0=min(date_list), x1=election_date_2022, fillcolor='#f62f3c', \n                  opacity=0.08, layer='below', line_width=0, row=row, col=1)\n    fig.add_vrect(x0=election_date_2022, x1=max(date_list), fillcolor='#f8c811', \n                  opacity=0.08, layer='below', line_width=0, row=row, col=1)\n    # Add text annotations for election dates\n    fig.add_annotation(x=election_date_2017, y=election_dates_y[0], text='2017 election', \n                       textangle=-90, font=dict(size=18), showarrow=False, xanchor='right', \n                       row=row, col=1)\n    fig.add_annotation(x=election_date_2022, y=election_dates_y[1], text='2022 election', \n                       textangle=-90, font=dict(size=18), showarrow=False, xanchor='right', \n                       row=row, col=1)\n\ndef interactive_plot(\n        columns_used: list[str], trend_period = 12,\n        election_dates_y_trend = (6000, 4600), election_dates_y_resid = (1500, 1200)):\n    subplot_titles = ['Summary Trend', 'Summary Seasonality', 'Summary Residuals']\n    fig = make_subplots(rows=3, cols=1, vertical_spacing=0.1, subplot_titles=subplot_titles,\n        shared_xaxes=True, figure=go.Figure(layout=go.Layout(height=1200)))\n    out = {\n        'start_date': None,\n        'end_date': None,\n        'actual_columns_used': []\n    }\n    for (decomposition, column_used), color_used in zip(\n        get_decomposition(columns_used, trend_period, out), px.colors.qualitative.Plotly[3:]):\n        column_used_name = '&lt;br&gt;'.join(textwrap.wrap(column_used, width=20))\n        # Plot trend component\n        fig.add_trace(go.Scatter(\n            x=decomposition.trend.index, y=decomposition.trend.values, mode='lines', \n            name=column_used, legendgroup=column_used, showlegend=True, \n            line=dict(color=color_used)), row=1, col=1)\n        # Plot seasonal component\n        fig.add_trace(go.Scatter(\n            x=decomposition.seasonal.index, y=decomposition.seasonal.values, mode='lines', \n            name=column_used, legendgroup=column_used, showlegend=False, \n            line=dict(color=color_used)), row=2, col=1)\n        fig.add_hline(y=0, line=dict(color='red', width=0.5, dash='dash'), row=2, col=1)\n        # Plot residual component\n        fig.add_trace(go.Scatter(\n            x=decomposition.resid.index, y=decomposition.resid.values, mode='lines', \n            name=column_used, legendgroup=column_used, showlegend=False, \n            line=dict(color=color_used)), row=3, col=1)\n        fig.add_hline(y=0, line=dict(color='red', width=0.5, dash='dash'), row=3, col=1)\n    start_date = out['start_date']\n    end_date = out['end_date']\n    if start_date:\n        draw_election_lines_plotly(start_date, end_date, fig, 1, election_dates_y_trend)\n        draw_election_lines_plotly(start_date, end_date, fig, 3, election_dates_y_resid)\n        fig.update_layout(\n            title=dict(\n                text=(\n                    f'{trend_period}-Month Summary of Registered Entities in Kenya'\n                    f' ({start_date.strftime(\"%b %Y\")} to {end_date.strftime(\"%b %Y\")})'\n                    '&lt;br&gt;'\n                    'Trend - Seasonality - Residuals'\n                ),\n                font=dict(size=26),\n                x=0.5,\n                xanchor='center',\n                y = 0.95,\n                yanchor = 'top',\n            ),\n            showlegend=True,\n            legend=dict(font=dict(size=16)),\n        )\n        fig.add_annotation(\n            text=\"ToKnow.ai\",\n            x=1,\n            y=0.25,\n            xref=\"paper\",\n            yref=\"paper\",\n            showarrow=False,\n            font=dict(size=50, color=\"lightgrey\"),\n            opacity=0.5,\n            textangle=-30)\n        fig.update_layout(\n            margin=dict(t=140),\n            legend=dict(\n                orientation=\"h\", yanchor=\"bottom\", y=-0.2, xanchor=\"center\", x=0.5))\n    fig.update_xaxes(showticklabels=True, title_text=\"Years\", row=1, col=1)\n    fig.update_xaxes(showticklabels=True, title_text=\"Years\", row=2, col=1)\n    fig.update_xaxes(showticklabels=True, title_text=\"Years\", row=3, col=1)\n    fig.update_yaxes(title_text=\"Trend\", row=1, col=1)\n    fig.update_yaxes(title_text=\"Seasonality\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Residuals\", row=3, col=1)\n    return fig\n\ninteractive_plot(plot_columns + other_plot_columns)\n\n\n                                                \n\n\n\n\nArchiving the Data\nThe dataset has been archived at https://huggingface.co/datasets/ToKnow-ai/Summary-of-Registered-Entities-and-Companies-in-Kenya\n          \n        \n          \n             Loading Summary of Registered Entities and Companies in Kenya..."
  },
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#key-findings",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#key-findings",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Key Findings",
    "text": "Key Findings\n\nDominance of Business Names and Private Companies: These two categories consistently outpace other forms of registration, indicating a preference for simpler business structures among Kenyan entrepreneurs.\nTransition Between Political Eras:\n\nThe Uhuru/Jubilee era (pre-2022) showed a general upward trend in registrations, particularly for business names and private companies.\nThe early Ruto/UDA era (post-2022) exhibits some volatility in registration numbers, with a slight downward trend observable in some categories.\n\nResilience of the Entrepreneurial Spirit Despite political transitions and global events (such as the COVID-19 pandemic, which falls within this period), the overall trend of business registrations remains positive. This suggests a robust entrepreneurial spirit in Kenya that persists through various challenges.\nSeasonal Patterns: Business registrations tend to peak in the first quarter of the year (January-March) and again in August, possibly reflecting fiscal year planning.\nRise in Strike Off Applications: There’s a notable increase in strike off applications starting in 2021, which could indicate economic stress or regulatory changes affecting business closures.\nSlowdown in Growth: Recent years show a slowdown in the growth rate of new business registrations, particularly after 2021, which may reflect broader economic challenges."
  },
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#conclusion",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#conclusion",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis of Kenya’s business registration trends offers a unique window into the country’s economic dynamics and the interplay between politics and entrepreneurship. While political transitions and global events like the COVID-19 pandemic have impacted business formation, the overall trend suggests a resilient formal business sector in Kenya.\nAs Kenya continues to position itself as a key economic player in East Africa, understanding these trends can be crucial for policymakers, investors, and entrepreneurs alike. Future research could delve deeper into sector-specific trends or compare Kenya’s patterns with those of neighboring countries to provide a more comprehensive regional perspective.\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#footnotes",
    "href": "posts/summary-of-registered-entities-and-companies-in-kenya/index.html#footnotes",
    "title": "Summary of Registered Entities and Companies in Kenya",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nhttps://brs.go.ke/companies-registry-statistics/ - archive↩︎"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#introduction",
    "href": "posts/private-domain-checker/index.html#introduction",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Introduction",
    "text": "Introduction\nDomain front-running1 typically occurs when a domain registrar or associated party monitors search queries entered by users looking for available domain names. If the domain appears promising or marketable, the registrar may register it before the user, intending to profit by reselling it at a higher price. While registrars deny engaging in this practice, affected individuals and businesses find it challenging to prove due to limited transparency regarding how domain search data is managed and monitored.\nOn a personal level, I, along with several friends and family members, have encountered domain front-running. In a recent incident, after checking a domain’s availability through a Kenyan affiliate registrar, the exact domain was unexpectedly registered within minutes by the primary affiliated registrar. On average, short and simple domain names are typically lost within 12-72 hours after searching, depending on where you searched and the TLD of the domain.\nSuch incidents are far from isolated. Numerous domain seekers report similar patterns, noting that after searching for specific domains, these names quickly become registered by the same or affiliated registrars. This behavior leaves many users without clear explanations, contributing to widespread suspicion and concern. Evidence from personal experiences and community reports suggests that domain front-running remains an ongoing concern that affects domain seekers.\nOld and Recent online discussions across various forums highlight ongoing concerns about domain front-running:\n\nDomain Front Running? - Sunday, August 4, 2024\nSafe way to search for availability to avoid front running? - Friday, September 24, 2021\nDid GoDaddy just front run me? - Wednesday, April 10, 2024\nI just confirmed that Namecheap buys available domains that you search for - Tuesday, March 28, 2023\nOthers - https://www.google.com/search?q=domain+front-running+site%3Areddit.com"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#understanding-domain-registration-systems",
    "href": "posts/private-domain-checker/index.html#understanding-domain-registration-systems",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Understanding Domain Registration Systems",
    "text": "Understanding Domain Registration Systems\nDomain registration involves multiple levels of authority:\n\nCountry-level registries (e.g., KENIC for .ke domains)\nGlobal registries (e.g., Verisign for .com and .net)\nIndividual registrars (e.g., GoDaddy, Namecheap)\n\nEach registry maintains authoritative records of registered and unregistered domains through their WHOIS or RDAP protocols. Some TLDs face stability issues, such as Libya’s .ly domain, where the official registrar (whois.nic.ly) is non-functional, leaving only unofficial services like reg.ly.\nThere are two main protocols for checking domain availability, WHOIS and RDAP:\n\n\n\n\n\n\n\n\n\nFeature\nWHOIS Protocol\nRDAP (Registration Data Access Protocol)\n\n\n\n\nIntroduced\n1982\n2015\n\n\nStatus\nLegacy but widely used\nModern replacement for WHOIS\n\n\nPrivacy Features\nLimited\nBuilt-in\n\n\nStandardization\nVaries by registrar\nJSON-based, consistent\n\n\nAccuracy\nCan be inconsistent\nGenerally more reliable\n\n\nAdvantages\n- Widespread support- Simple protocol\n- Standardized JSON responses- Better privacy controls- More efficient queries\n\n\nDisadvantages\n- No standardized format- Limited privacy protections- Rate limiting issues\n- Not yet universally adopted- Requires more complex implementation"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#best-practices-for-private-domain-searches",
    "href": "posts/private-domain-checker/index.html#best-practices-for-private-domain-searches",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Best Practices for Private Domain Searches",
    "text": "Best Practices for Private Domain Searches\nClaims of domain snatching by domain registrars underscore the need for domain seekers to understand the risks and methods for safeguarding their domain searches.\n\nUse Official WHOIS Lookup Sites:\nInstead of searching on a registrar’s website, consider using a neutral WHOIS lookup service. Many of these services do not track or record searches, reducing the likelihood of domain front-running.\n\nICANN lookup (https://lookup.icann.org/en) for .com domains\nDirect registry websites (e.g., KENIC for .ke domains)\nAvoid commercial registrars for initial searches, such as GoDaddy or NameCheap\n\nUtilize command-line tools:\n\nUse terminal whois command (e.g., whois example.com). whois program is usually preinstalled in most unix systems. For Windows users, Microsoft offers a WHOIS utility that can be downloaded from Sysinternals site.\nMinimizes exposure to potential monitoring\n\nTime your searches strategically:\n\nOnly search on commercial registrars when ready to purchase immediately\nMake sure you test your payment method before searching\n\nResearch the Registrar’s Reputation:\nBefore using a registrar, check reviews and forums for any reports or complaints about front-running. Users often share their experiences, which can help identify registrars with questionable practices.\n\nAvoid small resellers who may have less secure practices at all costs\nUnderstand that most resellers ultimately use major registrars’ services, thus have less control over the monitoring of the domain searches\n\nUse Independent Domain Search Tools:\n\nUtilize unaffiliated services like our private domain checker - https://toknow.ai/apps/private-domain-checker\nPrefer open-source solutions for transparency"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#transparent-solution-private-domain-checker",
    "href": "posts/private-domain-checker/index.html#transparent-solution-private-domain-checker",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Transparent Solution: Private Domain Checker",
    "text": "Transparent Solution: Private Domain Checker\nPrivate Domain Checker, hosted on Hugging Face Spaces and with publicly available source code, provides a privacy-focused alternative for domain availability checks. Key features include:\n\nMultiple checking methods (DNS, RDAP, WHOIS)\nNo registrar affiliations\nOpen-source codebase\nSupport for nearly all TLDs\n\nLimitations:\nThe tool has some constraints:\n\nMay not check certain TLDs due to non-functional official registries\nPlatform accessibility issues in some regions due to Hugging Face hosting restrictions e.g., China2\nDependent on registry API availability and response times\nDependent on Hugging Face Space resources and availability\n\n\nDemo\n\n          \n        \n          \n             loading private domain checker...\n            \n          \n          \n        \n        \n        \n\n\n\nCode\nBelow is the code running the flask app running the above private domain checker\n\n# https://huggingface.co/spaces/ToKnow-ai/private-domain-checker/blob/main/app.py\n\nimport json5\nimport os\nimport random\nfrom typing import Callable, Literal\nfrom flask import Flask, send_from_directory, request\nfrom urllib.parse import urlparse\nimport dns.resolver\nimport socket\nimport requests\nimport platform\nimport subprocess\nfrom shutil import which\nimport re\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    \"\"\"Route handler for the home page\"\"\"\n    try:\n        return send_from_directory('.', 'index.html')\n    except Exception as e:\n        return str(e)\n    \n@app.route('/check', methods=['POST'])\ndef check():\n    return check_domain(request.get_json().get('domain', ''))\n\ndef check_domain(domain: str):\n    \"\"\"Check domain availability\"\"\"\n    logs: list[str] = []\n    try:\n        domain = validate_and_correct_domain(domain)\n        result = check_domain_availability(domain, logs.append)\n        if result:\n            return { \n                \"domain\": domain,\n                \"method\": f\"Checked via {result['method']}\",\n                \"available\": result['available'],\n                \"logs\": logs\n            }\n        logs.append(f\"{check_domain.__name__}:result == None\")\n    except Exception as e:\n        logs.append(f\"{check_domain.__name__}:Exception:{str(e)}\")\n    return default_error(domain, logs)\n\ndef validate_and_correct_domain(domain: str):\n    # remove leding and trailing \"/\"\n    domain = domain.lower().strip('/').strip()\n    # extract domain \n    domain = urlparse(domain).netloc.strip() if '://' in domain else domain\n    # remove lending non alphanumeric\n    while domain and not domain[0].isalnum():\n        domain = domain[1:].strip()\n    # remove www.\n    if domain.startswith(\"www.\"):\n        domain = domain[4:].strip()\n    # remove inner spaces\n    domain = re.sub(r'[\\n\\s]+', '', domain).strip()\n    # replace unwanted characters with hyphens\n    domain = re.sub(r'[^a-zA-Z0-9\\.]', '-', domain).strip('-').strip('.').strip()\n    return domain\n\ndef default_error(domain: str, logs: list[str]):\n    cannot_confirm = \"Cannot confirm if doimain is available\"\n    try:\n        current_dir = os.path.dirname(os.path.abspath(__file__))\n        with open(os.path.join(current_dir, 'blocked-tlds.jsonc'), mode='r') as f:\n            blocked_tlds: list[dict[Literal[\"tld\", \"info\"], str]] = json5.load(f)\n        for blocked_tld in blocked_tlds:\n            if domain.endswith(blocked_tld.get('tld')):\n                return { \n                    'domain': domain, \n                    \"available\": False, \n                    \"method\": f\"{cannot_confirm}, try at {blocked_tld.get('info')}\",\n                    \"logs\": logs\n                }\n        response = requests.get(\"https://data.iana.org/TLD/tlds-alpha-by-domain.txt\", timeout=5)\n        all_tlds = []\n        if response.ok:\n            all_tlds = response.text.split(\"\\n\")\n        else:\n            with open( os.path.join(current_dir, 'tlds-alpha-by-domain.txt'), mode='r') as f:\n                all_tlds = f.readlines()\n        all_tlds: list[str] = [\n            i.lower().strip() \n            for i \n            in all_tlds \n            if len((i or '').strip()) &gt; 0 and not i.strip().startswith(\"#\")\n        ]\n        is_supported_tld = any(True for i in all_tlds if domain.strip().endswith(f'.{i}'))\n        if not is_supported_tld:\n            return { \n                'domain': domain, \n                \"available\": False, \n                \"method\": f\"Unsupported domain, \\\".{'.'.join(domain.split('.')[1:])}\\\" is not a valid domain TLD!\",\n                \"logs\": logs\n            }\n    except Exception as e:\n        logs.append(f\"{default_error.__name__}:Exception:{str(e)}\")\n    return { \n        'domain': domain, \n        \"available\": False, \n        \"method\": cannot_confirm,\n        \"logs\": logs\n    }\n\ndef check_domain_availability(domain, logs_append: Callable[[str], None]):\n    \"\"\"Check domain availability using multiple methods.\"\"\"\n    # First try DNS resolution\n    is_available, availability_method, _continue = dns_is_available(\n        domain, logs_append)\n    if not _continue:\n        return { \n            \"available\": is_available, \n            \"method\": f\"DNS:{availability_method}\" \n        }\n    \n    # Try RDAP\n    is_available, availability_method, _continue = rdap_is_available(\n        domain, logs_append)\n    if not _continue:\n        return { \n            \"available\": is_available, \n            \"method\": f\"RDAP:{availability_method}\" \n        }\n\n    # Fall back to WHOIS\n    is_available, availability_method, _continue = whois_is_available(\n        domain, logs_append)\n    if not _continue:\n        return {\n            \"available\": is_available, \n            \"method\": f\"WHOIS:{availability_method}\"\n        }\n\ndef dns_is_available(domain, logs_append: Callable[[str], None]):\n    \"\"\"Check if domain exists in DNS by looking for common record types.\"\"\"\n    # Check NS records first as they're required for valid domains\n    try:\n        resolver = get_dns_resolver()\n        for record_type in ['NS', 'A', 'AAAA', 'MX', 'CNAME']:\n            try:\n                resolver.resolve(domain, record_type)\n                return False, record_type, False\n            except Exception as e:\n                logs_append(\n                    (f\"{dns_is_available.__name__}:{record_type}:Exception\"\n                     f\":{'|'.join(resolver.nameservers)}:{str(e)}\"))\n    except Exception as e:\n        logs_append(f\"{dns_is_available.__name__}:Exception:{str(e)}\")\n    return True, None, True\n\ndef get_dns_resolver():\n    # list of major DNS resolvers\n    resolver = dns.resolver.Resolver()\n    def myshuffle(ls):\n        random.shuffle(ls)\n        return ls\n    namesevers = { \n        'cloudflare': myshuffle(['1.1.1.1', '1.0.0.1']),\n        'google': myshuffle(['8.8.8.8', '8.8.4.4']),\n        'quad9': myshuffle(['9.9.9.9', '149.112.112.112']),\n        'opendns': myshuffle(['208.67.222.222', '208.67.220.220']),\n        'adguard': myshuffle(['94.140.14.14', '94.140.15.15']),\n        'nextdns': myshuffle(['45.90.28.167', '45.90.30.167']),\n        'default': myshuffle(resolver.nameservers)\n    }\n    resolver.nameservers = random.choice(list(namesevers.values()))\n    return resolver\n\ndef rdap_is_available(domain, logs_append: Callable[[str], None]):\n    try:\n        bootstrap_url = \"https://data.iana.org/rdap/dns.json\"\n        bootstrap_data = requests.get(bootstrap_url, timeout=5).json()\n        tld = domain.split('.')[-1]\n        services: list[tuple[list[str], list[str]]] = bootstrap_data['services']\n        for [tlds, rdap_base_urls] in services:\n            if tld in tlds:\n                for rdap_base_url in rdap_base_urls:\n                    response = requests.get(\n                        f\"{rdap_base_url.lstrip('/')}/domain/{domain}\", timeout=5)\n                    if response.status_code == 404:\n                        return True, rdap_base_url, False\n                    elif response.status_code == 200:\n                        return False, rdap_base_url, False\n        logs_append(f\"{rdap_is_available.__name__}:no RDAP\")\n    except Exception as e:\n        logs_append(f\"{rdap_is_available.__name__}:Exception:{str(e)}\")\n    return False, None, True\n\ndef whois_is_available(domain, logs_append: Callable[[str], None]) -&gt; bool:\n    try:\n        available_patterns = [\n            'no match',\n            'not found',\n            'no entries found',\n            'no data found',\n            'not registered',\n            'available',\n            'status: free',\n            'domain not found',\n            'no object found',\n            'not been registered',\n            'status: available',\n            'domain is available',\n            'is free',\n            'no match found',\n            'domain not registered',\n            'domain available',\n            'not exists',\n            'does not exist',\n            'no information available',\n            'registration status: unused',\n            'status: inactive',\n            'no such domain',\n            'query matched no objects',\n            'no matching record',\n            'domain status: available',\n            'this domain is not registered',\n            'domain name has not been registered',\n            'can not find domain',\n            'cannot find domain',\n            'this domain is available for purchase',\n            'domain status: free'\n        ]\n        is_available_callback = lambda output: any(\n            pattern in output for pattern in available_patterns)\n        is_available, availability_method = socket_whois_is_available(\n            domain, is_available_callback, logs_append)\n        if is_available:\n            return True, availability_method, False\n        is_available, availability_method = terminal_whois_is_available(\n            domain, is_available_callback, logs_append)\n        if is_available:\n            return True, availability_method, False\n    except Exception as e:\n        logs_append(f\"{whois_is_available.__name__}:Exception:{str(e)}\")\n    return False, None, True\n\ndef socket_whois_is_available(\n        domain: str, \n        is_available_callback: Callable[[str], bool], \n        logs_append: Callable[[str], None]):\n    try:\n        whois_server = get_whois_server(domain, logs_append)\n\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.settimeout(4)\n        sock.connect((whois_server, 43))\n        sock.send(f\"{domain}\\r\\n\".encode())\n        response = sock.recv(4096).decode(errors='ignore')\n        sock.close()\n        \n        response_lower = response.lower()\n        return is_available_callback(response_lower), whois_server\n    except Exception as e:\n        logs_append(\n            f\"{socket_whois_is_available.__name__}:whois_server:{whois_server}\")\n        logs_append(\n            f\"{socket_whois_is_available.__name__}:Exception:{str(e)}\")\n    return False, None\n\ndef terminal_whois_is_available(\n        domain: str, \n        is_available_callback: Callable[[str], bool], \n        logs_append: Callable[[str], None]):\n    try:\n        # Check if OS is Linux\n        if platform.system().lower() == 'linux':\n            if which('whois') is not None:\n                # Run whois command with timeout\n                process = subprocess.Popen(\n                    ['whois', domain], \n                    stdout=subprocess.PIPE, \n                    stderr=subprocess.PIPE)\n                try:\n                    stdout, stderr = process.communicate(timeout=10)\n                    output = stdout.decode('utf-8', errors='ignore').lower()\n                    logs_append(\n                        (f\"{terminal_whois_is_available.__name__}\"\n                         f\":stderr:{str(stderr.decode(encoding='utf-8'))}\"))\n                    return is_available_callback(output), \"system whois\"\n                except subprocess.TimeoutExpired as timeout_e:\n                    logs_append(\n                        (f\"{terminal_whois_is_available.__name__}\"\n                         f\":TimeoutExpired:{str(timeout_e)}\"))\n                    process.kill()\n            else:\n                logs_append(\n                    (f\"{terminal_whois_is_available.__name__}\"\n                     \":Exception:WHOIS not installed. \"\n                     \"Install with: sudo apt-get install whois\"))\n        else:\n            logs_append(\n                (f\"{terminal_whois_is_available.__name__}\"\n                \":Exception:System WHOIS check only available on Linux\"))\n    except Exception as e:\n        logs_append(\n            f\"{terminal_whois_is_available.__name__}:Exception:{str(e)}\")\n    return False, None\n\ndef get_whois_server(domain, logs_append: Callable[[str], None]):\n    \"\"\"Get WHOIS server from IANA root zone database.\"\"\"\n    try:\n        response = requests.get(f'https://www.iana.org/whois?q={domain}')\n        if 'whois:' in response.text.lower():\n            for line in response.text.split('\\n'):\n                if 'whois:' in line.lower():\n                    return line.split(':')[1].strip()\n    except Exception as e:\n        logs_append(f\"{get_whois_server.__name__}:Exception:{str(e)}\")\n    return None\n\n\nTest\n\ncheck_domain(\"examplerhccvu.ly\")\n\n{\n  \"domain\": \"examplerhccvu.ly\",\n  \"method\": \"Checked via RDAP:https://rdap.nic.ly/\",\n  \"available\": true,\n  \"logs\": [\n    \"dns_is_available:NS:Exception:208.67.222.222|208.67.220.220:The DNS query name does not exist: examplerhccvu.ly.\",\n    \"dns_is_available:A:Exception:208.67.222.222|208.67.220.220:The DNS query name does not exist: examplerhccvu.ly.\",\n    \"dns_is_available:AAAA:Exception:208.67.222.222|208.67.220.220:The DNS query name does not exist: examplerhccvu.ly.\",\n    \"dns_is_available:MX:Exception:208.67.222.222|208.67.220.220:The DNS query name does not exist: examplerhccvu.ly.\",\n    \"dns_is_available:CNAME:Exception:208.67.222.222|208.67.220.220:The DNS query name does not exist: examplerhccvu.ly.\"\n  ]\n}\n\n\n\ncheck_domain(\"examplerhccvu.cn\")\n\n{\n  \"domain\": \"examplerhccvu.cn\",\n  \"method\": \"Checked via WHOIS:whois.cnnic.cn\",\n  \"available\": true,\n  \"logs\": [\n    \"dns_is_available:NS:Exception:8.8.8.8|8.8.4.4:The DNS query name does not exist: examplerhccvu.cn.\",\n    \"dns_is_available:A:Exception:8.8.8.8|8.8.4.4:The DNS query name does not exist: examplerhccvu.cn.\",\n    \"dns_is_available:AAAA:Exception:8.8.8.8|8.8.4.4:The DNS query name does not exist: examplerhccvu.cn.\",\n    \"dns_is_available:MX:Exception:8.8.8.8|8.8.4.4:The DNS query name does not exist: examplerhccvu.cn.\",\n    \"dns_is_available:CNAME:Exception:8.8.8.8|8.8.4.4:The DNS query name does not exist: examplerhccvu.cn.\",\n    \"rdap_is_available:no RDAP\"\n  ]\n}\n\n\n\ncheck_domain(\"examplerhccvu.com\")\n\n{\n  \"domain\": \"examplerhccvu.com\",\n  \"method\": \"Checked via RDAP:https://rdap.verisign.com/com/v1/\",\n  \"available\": true,\n  \"logs\": [\n    \"dns_is_available:NS:Exception:8.8.4.4|8.8.8.8:The DNS query name does not exist: examplerhccvu.com.\",\n    \"dns_is_available:A:Exception:8.8.4.4|8.8.8.8:The DNS query name does not exist: examplerhccvu.com.\",\n    \"dns_is_available:AAAA:Exception:8.8.4.4|8.8.8.8:The DNS query name does not exist: examplerhccvu.com.\",\n    \"dns_is_available:MX:Exception:8.8.4.4|8.8.8.8:The DNS query name does not exist: examplerhccvu.com.\",\n    \"dns_is_available:CNAME:Exception:8.8.4.4|8.8.8.8:The DNS query name does not exist: examplerhccvu.com.\"\n  ]\n}\n\n\n\ncheck_domain(\"example.com\")\n\n{\n  \"domain\": \"example.com\",\n  \"method\": \"Checked via DNS:NS\",\n  \"available\": false,\n  \"logs\": []\n}"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#conclusion",
    "href": "posts/private-domain-checker/index.html#conclusion",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Conclusion",
    "text": "Conclusion\nWhile domain front-running remains challenging to prove conclusively, the abundance of user experiences and technical evidence suggests the need for private domain checking solutions. Our open-source tool provides one approach to mitigating these risks, though industry-wide changes and improved regulations may be necessary for long-term solutions.\nTo see a liat of all available domain TLDs, see https://data.iana.org/TLD/tlds-alpha-by-domain.txt\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/private-domain-checker/index.html#footnotes",
    "href": "posts/private-domain-checker/index.html#footnotes",
    "title": "Protecting Against Domain Front-Running by Registrars",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nhttps://en.wikipedia.org/wiki/Domain_name_front_running↩︎\nwhois.cnnic.cn is timming out in huggingface: https://www.chinatalk.media/p/hugging-face-blocked-self-castrating↩︎"
  },
  {
    "objectID": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#introduction",
    "href": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#introduction",
    "title": "Licenced Hospitals, Clinics and Dispensaries in Kenya, Year 2025",
    "section": "Introduction",
    "text": "Introduction\nUsing the data from Licenced HealthFacilities for the year 2025 1, we analyze hospital distribution in kenya per county.\nKenya’s healthcare system employs a six-tiered structure. Level 1 Community Health Units (CHUs) focused on preventive and promotive healthcare care through community health workers. Level 2 dispensaries and clinics focus on basic outpatient services for common ailments. Level 3 health centers provide curative and preventative care, including maternal and child health. Level 4 sub-county and district hospitals offer primary referral services. Level 5 county referral and teaching hospitals handle secondary referrals and specialized care. Finally, Level 6 national referral hospitals, such as Kenyatta National Hospital provide the highest level of tertiary care 2 3."
  },
  {
    "objectID": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#analysis",
    "href": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#analysis",
    "title": "Licenced Hospitals, Clinics and Dispensaries in Kenya, Year 2025",
    "section": "Analysis",
    "text": "Analysis\n\nImporting Libraries\n\nimport matplotlib.pyplot as plt\nimport pygadm\nimport pandas as pd\nimport requests\nfrom io import StringIO\nimport geopandas as gpd\nfrom adjustText import adjust_text\nfrom IPython.display import display, Markdown\nimport matplotlib.colors as colors\n\n\n\nData\n\n# Get the data\nhealth_facilities_data = requests.get(\n    \"https://kmpdc.go.ke/Registers/H-Facilities.php\").text\n# Parse the data\nhealth_facilities_df = pd.read_html(StringIO(health_facilities_data))[0]\n# Drop columns that are not useful\nhealth_facilities_df = health_facilities_df.drop([\n    'View', # Empty column\n    'status', # All values are 'Active'\n    'Reg_No', # Not useful because it's unique and obscured\n], axis=1)\n# Make levels categorical\nhealth_facilities_df['Level'] = pd.Categorical(\n    health_facilities_df['Level'].str.lower().str.strip().str.upper())\n# drop duplicates\nhealth_facilities_df = health_facilities_df.drop_duplicates()\nhealth_facilities_df\n\n\n\n\n\n\n\n\n\nFacility_Name\nAddress\nFacility_Type\nLevel\nBed_Capacity\nCounty\n\n\n\n\n0\nA.C.K ST. PAULS MIHUTI DISPENSARY\nP.O BOX 227-10202 KANGEMA\nDISPENSARY\nLEVEL 2\n0\nMURANG'A\n\n\n1\nNEEMA MEDICARE MEDICAL CENTRE\nP.O BOX 59461-00200 NAIROBI\nMEDICAL CENTRE\nLEVEL 3B\n0\nNAIROBI\n\n\n2\nAMUNG\\'ENTI CATHOLIC DISPESARY\nP.O BOX 75-60600 MAUA\nDISPENSARY\nLEVEL 2\n0\nMERU\n\n\n3\nLIFOG CENTRE MEDICAL CLINIC\nP.O BOX 84-80100 MOMBASA\nMEDICAL CLINIC\nLEVEL 2\n0\nMOMBASA\n\n\n4\nRAPHA JOY MEDICAL CLINIC\nP.O BOX 244-00221 LAIKIPIA\nMEDICAL CLINIC\nLEVEL 2\n0\nKIAMBU\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n14094\nNJORO HUDUMA MEDICAL CLINIC NAKURU\nP.O BOX 86-20107 NJORO\nNURSING HOME\nLEVEL 3A\n43\nNAKURU\n\n\n14095\nDYNAMED MEDICAL CLINIC\nP.O BOX 454-01001 KALIMONI\nMEDICAL CLINIC\nLEVEL 2\n0\nKIAMBU\n\n\n14096\nSLENMARK MEDICAL CLINIC\nP.O BOX 267-10304 KUTUS\nMEDICAL CENTRE\nLEVEL 3B\n0\nKIRINYAGA\n\n\n14097\nSLENMAC CARE CENTRE LIMITED\nP.O BOX 267- KERUGOYA\nMEDICAL CENTRE\nLEVEL 3B\n40\nKIRINYAGA\n\n\n14098\nLUIZ HOASIS MEDICAL CENTRE EBATE\nP.O BOX 366-40200 KISII\nMEDICAL CENTRE\nLEVEL 3B\n16\nNYAMIRA\n\n\n\n\n\n14098 rows × 6 columns\n\n\n\ndef health_facilities_county(title: str, levels = None):\n    display(Markdown(\n        \"\\n\"\n        f\"### {title}\"\n        \"\\n\"))\n    df = health_facilities_df\\\n        if levels == None\\\n        else health_facilities_df[health_facilities_df['Level'].isin(levels)]\n    sizes = df.groupby('County').size()\n    sizes.sort_values(ascending=False)\\\n        .plot(kind='bar', figsize=(20, 10))\n    plt.title(title)\n    plt.ylabel('Number of Health Facilities')\n    plt.xlabel('County')\n    plt.grid(True, alpha=0.3, axis='y',)\n    plt.show()\n\n    print(sizes.describe())\n\nhealth_facilities_county(\n    'Health Facilities by County, year 2025 [All Levels]')\n\n\nHealth Facilities by County, year 2025 [All Levels]\n count 47.000000 mean 299.957447 std 247.828654 min 61.000000 25% 180.500000 50% 261.000000 75% 342.500000 max 1718.000000 dtype: float64\n\nhealth_facilities_county(\n    'Health Facilities by County, year 2025 [LEVEL 2]',\n    ['LEVEL 2'])\n\nHealth Facilities by County, year 2025 [LEVEL 2]\n\n\n\n\n\n\n\n\n\ncount     47.000000\nmean     183.297872\nstd      122.494350\nmin       35.000000\n25%      111.500000\n50%      170.000000\n75%      217.000000\nmax      786.000000\ndtype: float64\n\n\n\nhealth_facilities_county(\n    'Health Facilities by County, year 2025 [LEVEL 3]',\n    ['LEVEL 3A', 'LEVEL 3B'])\n\nHealth Facilities by County, year 2025 [LEVEL 3]\n\n\n\n\n\n\n\n\n\ncount     47.000000\nmean      92.510638\nstd      116.786850\nmin       13.000000\n25%       36.500000\n50%       60.000000\n75%      104.000000\nmax      752.000000\ndtype: float64\n\n\n\nhealth_facilities_county(\n    'Health Facilities by County, year 2025 [LEVEL 4]',\n    ['LEVEL 4', 'LEVEL 4B'])\n\nHealth Facilities by County, year 2025 [LEVEL 4]\n\n\n\n\n\n\n\n\n\ncount     47.000000\nmean      21.510638\nstd       22.531980\nmin        3.000000\n25%        8.500000\n50%       15.000000\n75%       30.000000\nmax      145.000000\ndtype: float64\n\n\n\nhealth_facilities_county(\n    'Health Facilities by County, year 2025 [LEVEL 5 and LEVEL 6]',\n    ['LEVEL 5', 'LEVEL 6A', 'LEVEL 6B'])\n\nHealth Facilities by County, year 2025 [LEVEL 5 and LEVEL 6]\n\n\n\n\n\n\n\n\n\ncount    33.000000\nmean      3.757576\nstd       6.015762\nmin       1.000000\n25%       1.000000\n50%       2.000000\n75%       4.000000\nmax      35.000000\ndtype: float64\n\n\nOnly three counties have more than 500 helath facilities (Nairobi, Kiambu and Nakuru). Nairobi county leads with 1,718 facilities while Isiolo county tails with 61 facilities.\n\n\nHospital Density\nIf we gave each health facility a score depending on the level, and sum the scores for each county, we can get a simplistic estimate of the healthcare density in each county. The score is calculated as follows:\n\nfacility_level_values = { \n    j: i \n    for i, j \n    in enumerate(\n        health_facilities_df['Level'].sort_values().unique().tolist(), start = 1)\n}\nfacility_level_values\n\n{'LEVEL 2': 1,\n 'LEVEL 3A': 2,\n 'LEVEL 3B': 3,\n 'LEVEL 4': 4,\n 'LEVEL 4B': 5,\n 'LEVEL 5': 6,\n 'LEVEL 6A': 7,\n 'LEVEL 6B': 8}\n\n\n\ndensity_data = [\n    [\n        county, \n        int(pd.to_numeric(facilities[\"Level\"].apply(lambda x: facility_level_values[x])).sum()),\n        int(facilities[\"Level\"].count()),\n    ] \n    for county, facilities \n    in health_facilities_df.groupby('County')\n]\ndensity_df = pd.DataFrame(density_data, columns=['County', 'Density', 'Count'])\\\n    .sort_values('Density', ascending=False)\\\n    .reset_index(drop=True)\ndensity_df\n\n\n\n\n\n\n\n\n\nCounty\nDensity\nCount\n\n\n\n\n0\nNAIROBI\n3671\n1718\n\n\n1\nKIAMBU\n1257\n688\n\n\n2\nMANDERA\n1076\n478\n\n\n3\nNAKURU\n949\n606\n\n\n4\nHOMA BAY\n813\n392\n\n\n5\nWAJIR\n706\n361\n\n\n6\nMERU\n698\n456\n\n\n7\nKISUMU\n691\n368\n\n\n8\nMACHAKOS\n672\n440\n\n\n9\nKAKAMEGA\n654\n384\n\n\n10\nKISII\n649\n318\n\n\n11\nMOMBASA\n632\n323\n\n\n12\nKAJIADO\n631\n375\n\n\n13\nBUNGOMA\n599\n309\n\n\n14\nMIGORI\n528\n309\n\n\n15\nUASIN GISHU\n524\n296\n\n\n16\nKITUI\n511\n347\n\n\n17\nKILIFI\n491\n338\n\n\n18\nSIAYA\n452\n274\n\n\n19\nGARISSA\n447\n209\n\n\n20\nMAKUENI\n436\n314\n\n\n21\nNYERI\n433\n278\n\n\n22\nMURANG'A\n395\n285\n\n\n23\nKIRINYAGA\n386\n204\n\n\n24\nKERICHO\n379\n283\n\n\n25\nEMBU\n371\n238\n\n\n26\nBARINGO\n366\n261\n\n\n27\nNYAMIRA\n362\n198\n\n\n28\nNAROK\n356\n244\n\n\n29\nTURKANA\n331\n237\n\n\n30\nKWALE\n328\n244\n\n\n31\nNANDI\n315\n224\n\n\n32\nBUSIA\n305\n184\n\n\n33\nTRANS NZOIA\n292\n178\n\n\n34\nLAIKIPIA\n283\n183\n\n\n35\nBOMET\n258\n196\n\n\n36\nTHARAKA NITHI\n236\n167\n\n\n37\nVIHIGA\n231\n137\n\n\n38\nMARSABIT\n227\n152\n\n\n39\nWEST POKOT\n211\n168\n\n\n40\nELGEYO/ MARAKWET\n201\n151\n\n\n41\nNYANDARUA\n194\n129\n\n\n42\nTAITA TAVETA\n193\n121\n\n\n43\nSAMBURU\n162\n113\n\n\n44\nTANA RIVER\n141\n89\n\n\n45\nLAMU\n113\n70\n\n\n46\nISIOLO\n111\n61\n\n\n\n\n\n\n\n\nGet the geometry of the counties in Kenya, GADM (Global Administrative Areas).\n\ngdf: gpd.geodataframe.GeoDataFrame = pygadm.Items(\n    name=\"Kenya\", content_level=1).rename(columns={\"NAME_1\": \"County\"})\ngdf = gdf.drop(columns=list(set(gdf.columns) - set([\"geometry\", \"County\"])), \n              axis=1)\ngdf.head(5)\n\n\n\n\n\n\n\n\n\ngeometry\nCounty\n\n\n\n\n0\nMULTIPOLYGON (((35.7616 -0.1904, 35.7243 -0.19...\nBaringo\n\n\n1\nMULTIPOLYGON (((35.2613 -1.0159, 35.2583 -1.02...\nBomet\n\n\n2\nMULTIPOLYGON (((34.8778 0.8339, 34.9138 0.8524...\nBungoma\n\n\n3\nMULTIPOLYGON (((34.0292 -0.0142, 34.0158 -0.02...\nBusia\n\n\n4\nMULTIPOLYGON (((35.5272 0.2167, 35.5261 0.2183...\nElgeyo-Marakwet\n\n\n\n\n\n\n\n\nMerge the densities and the geometries\n\ndef format_county_name(county_name: pd.Series) -&gt; pd.Series:\n    return county_name.str.strip().str.lower().str.replace(r'[^a-zA-Z0-9]+', '-', regex=True).str.title().unique()\ndensity_df['County'] = format_county_name(density_df['County'])\ngdf['County'] = format_county_name(gdf['County'])\nmerged_gdf = gdf.merge(density_df, on='County', how='left')\nmerged_gdf.head(5)\n\n\n\n\n\n\n\n\n\ngeometry\nCounty\nDensity\nCount\n\n\n\n\n0\nMULTIPOLYGON (((35.7616 -0.1904, 35.7243 -0.19...\nBaringo\n366\n261\n\n\n1\nMULTIPOLYGON (((35.2613 -1.0159, 35.2583 -1.02...\nBomet\n258\n196\n\n\n2\nMULTIPOLYGON (((34.8778 0.8339, 34.9138 0.8524...\nBungoma\n599\n309\n\n\n3\nMULTIPOLYGON (((34.0292 -0.0142, 34.0158 -0.02...\nBusia\n305\n184\n\n\n4\nMULTIPOLYGON (((35.5272 0.2167, 35.5261 0.2183...\nElgeyo-Marakwet\n201\n151\n\n\n\n\n\n\n\n\nPlotting the simplistic densities gives us the following map.\n\n# Create figure and axis\nfig, ax = plt.subplots(1, 1, figsize=(12, 12))\n\n# Create a custom colormap from green to red\ncmap = colors.LinearSegmentedColormap.from_list('custom_cmap', ['#f2fff2', '#ff0000'])\n\n# Plot with hospital count determining the color\nmerged_gdf.plot(\n    ax=ax, \n    column='Density', \n    cmap=cmap, \n    legend=True,\n    legend_kwds={'label': \"Hospital Density\", 'orientation': \"vertical\"},\n    linewidth=0.1, edgecolor='black'\n)\n\ntexts = [\n    ax.text(\n        row.geometry.centroid.x, \n        row.geometry.centroid.y, \n        f\"{row['County']}-{row['Density']}\",  # Just show the count\n        fontsize=9,\n        ha='center',\n        va='center',\n        color='black',\n        # fontweight='bold',\n        # bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3')\n    )\n    for idx, row \n    in merged_gdf.iterrows()]\nadjust_text(texts, arrowprops=dict(arrowstyle='-', color='black', lw=0.5))\n\n# Add title and remove axes\nplt.title('Kenya Health Facilities Density by County', fontsize=15)\nplt.axis('off')\n\n# Show the map\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#footnotes",
    "href": "posts/licenced-hospitals-clinics-and-dispensaries-in-kenya-year-2025/index.html#footnotes",
    "title": "Licenced Hospitals, Clinics and Dispensaries in Kenya, Year 2025",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nArchive - Licenced HealthFacilities for the year 2025↩︎\nExplainer: Six levels of hospitals and services they offer↩︎\nHealthcare in Kenya↩︎"
  },
  {
    "objectID": "posts/kenya-population-from-1974-to-2022/index.html#imports",
    "href": "posts/kenya-population-from-1974-to-2022/index.html#imports",
    "title": "Kenya Population from year 1974 to year 2022",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nimport requests\nfrom IPython.display import HTML"
  },
  {
    "objectID": "posts/kenya-population-from-1974-to-2022/index.html#data-and-plot",
    "href": "posts/kenya-population-from-1974-to-2022/index.html#data-and-plot",
    "title": "Kenya Population from year 1974 to year 2022",
    "section": "Data and plot",
    "text": "Data and plot\nThe data has been sourced from the World Bank API.1\n\n# Fetch population data for Kenya from the World Bank API\nurl = 'http://api.worldbank.org/v2/countries/KEN/indicators/SP.POP.TOTL?format=json'\n\n# Send a GET request to the World Bank API\njson = requests.get(url).json()[1]\n\n# Extract the JSON data, rename and sort\ndata = pd.DataFrame(json, columns=['date', 'value'])\\\n    .rename(columns={'date': 'Year', 'value': 'Population'})\\\n    .sort_values(by='Year', ascending=True)\\\n    .reset_index(drop=True)\ndata\n\n\n\n\n\n\n\n\n\nYear\nPopulation\n\n\n\n\n0\n1974\n13203949.0\n\n\n1\n1975\n13651908.0\n\n\n2\n1976\n14102268.0\n\n\n3\n1977\n14577346.0\n\n\n4\n1978\n15087423.0\n\n\n5\n1979\n15620613.0\n\n\n6\n1980\n16187124.0\n\n\n7\n1981\n16785962.0\n\n\n8\n1982\n17411491.0\n\n\n9\n1983\n18069461.0\n\n\n10\n1984\n18753176.0\n\n\n11\n1985\n19452161.0\n\n\n12\n1986\n20160879.0\n\n\n13\n1987\n20882094.0\n\n\n14\n1988\n21626122.0\n\n\n15\n1989\n22387803.0\n\n\n16\n1990\n23162269.0\n\n\n17\n1991\n23918235.0\n\n\n18\n1992\n24655723.0\n\n\n19\n1993\n25391830.0\n\n\n20\n1994\n26133744.0\n\n\n21\n1995\n26878347.0\n\n\n22\n1996\n27615736.0\n\n\n23\n1997\n28364264.0\n\n\n24\n1998\n29137373.0\n\n\n25\n1999\n29965129.0\n\n\n26\n2000\n30851606.0\n\n\n27\n2001\n31800343.0\n\n\n28\n2002\n32779823.0\n\n\n29\n2003\n33767122.0\n\n\n30\n2004\n34791836.0\n\n\n31\n2005\n35843010.0\n\n\n32\n2006\n36925253.0\n\n\n33\n2007\n38036793.0\n\n\n34\n2008\n39186895.0\n\n\n35\n2009\n40364444.0\n\n\n36\n2010\n41517895.0\n\n\n37\n2011\n42635144.0\n\n\n38\n2012\n43725806.0\n\n\n39\n2013\n44792368.0\n\n\n40\n2014\n45831863.0\n\n\n41\n2015\n46851488.0\n\n\n42\n2016\n47894670.0\n\n\n43\n2017\n48948137.0\n\n\n44\n2018\n49953304.0\n\n\n45\n2019\n50951450.0\n\n\n46\n2020\n51985780.0\n\n\n47\n2021\n53005614.0\n\n\n48\n2022\n54027487.0\n\n\n49\n2023\nNaN\n\n\n\n\n\n\n\n\n\ndata.dropna(inplace=True)\ndata['Year'] = data['Year'].astype(int)\n\npopulations = data['Population'] / 10**6  # Convert population to millions\nyears = data['Year']\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(10, 8))\nax.set_xlabel('Year')\nax.set_ylabel('Population (Million)')\nax.grid(True, which='both', linestyle='--', linewidth=0.5)  # Add grid lines\n\n# Create a line object\nline, = ax.plot([], [], lw=3, color='blue')\n\n# Function to update the animation frame\ndef animate(i):\n    year = data['Year'].min() + i\n    line.set_data(years[:i+1], populations[:i+1])\n\n    ax.set_xlim(years.min(), years.max())\n    ax.set_ylim(0, populations.max() * 1.1)\n    ax.set_title(f'Kenya Population ({year} - {populations[i]:.2f}M)')\n\n    return line,\n\n# Create the animation\nani = animation.FuncAnimation(fig, animate, frames=len(data), interval=300, blit=True, repeat=False)\nfig.suptitle(f\"Kenyan Population, Year {data.loc[0, 'Year']} to Year {int(data.iloc[-1]['Year'])}\")\n\n# Add a watermark to the center of the plot\nax.text(0.95, 0.02, 'ToKnow.ai', ha='right', va='bottom', \n        fontsize=18, color='gray', alpha=0.5, transform=ax.transAxes, rotation=45)\n\n# Display the animation\nplt.close(ani._fig)"
  },
  {
    "objectID": "posts/kenya-population-from-1974-to-2022/index.html#video",
    "href": "posts/kenya-population-from-1974-to-2022/index.html#video",
    "title": "Kenya Population from year 1974 to year 2022",
    "section": "Video",
    "text": "Video\n\n# ani.save('./index.mp4',  dpi=600)\n\nHTML(f'&lt;div class=\"ratio ratio-16x9\"&gt;{ani.to_html5_video()}&lt;/div&gt;')\n\n\n\n\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/kenya-population-from-1974-to-2022/index.html#footnotes",
    "href": "posts/kenya-population-from-1974-to-2022/index.html#footnotes",
    "title": "Kenya Population from year 1974 to year 2022",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nWorld Bank API -JSON↩︎"
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "",
    "text": "When you set up a website, blog, vlog, or social media post, you expect engagement. One of the simplest and most effective ways to measure engagement is through views. View count is effective because it measures both active and passive audience interaction. However, not all views represent actual engagement. For example, if one user reloads the same page a thousand times, it doesn’t represent a thousand unique viewers - it’s just one user refreshing repeatedly. They may not have even read the content. On the other hand, when a user returns multiple times to read content carefully, each visit could be considered meaningful engagement. To calculate actual reads, we need some intelligence in our tracking.\nIn the case of a blog or website, one of the simplest ways to add some intelligence is to delay sending the view or read signal by for example 7 seconds. Another way is to wait for the user to scroll a page to about half the page before triggering a read count.\nYou could also employ a free external service such as https://visitorbadge.io, which receives the page url and returns an SVG image of the page count, for example:\nhttps://api.visitorbadge.io/api/visitors?path=https://toknow.ai/posts/display-google-analytics-views-using-cloudflare-worker/index.html\nreturns:\nOne of the drawback of this strategy is we can’t delay the tigger of view count and show the current views/reads at the same time.\nOfcourse, you can implement a backend service to save and read the views. However, for a static websites and blogs such as this one (hosted as github pages), that would go aganist the current architecture.\nAnother alternative would be to utilize Google analytics. Google analytics can be added not just for the views, but to measure the overall health of the website/blog. If google analytics is your primary analytics tool for your website/blog, why not just show the views they have calculated?\nTo ensure we dont affect the current architecture, we can use a cloudflare worker to fetch views from google analytics."
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#authenticating-and-authorizing-google-analytics-apis",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#authenticating-and-authorizing-google-analytics-apis",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Authenticating and Authorizing Google Analytics APIs",
    "text": "Authenticating and Authorizing Google Analytics APIs\n\n\n\n\n\n\nImportant\n\n\n\nThis assumes you already have Google Analytics setup and running on one of your website/blog\n\n\nFor a detailed guide on how to create a service account, download the credentials.json file, and connect the service account to the google analytics account, see https://developers.google.com/analytics/devguides/reporting/data/v1/quickstart-client-libraries.\nThe credentials.json file has the following json structure:\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"...\",\n  \"private_key_id\": \"s6sdfsdf876...sdf7ygsf78fsd\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"starting-account...iam.gserviceaccount.com\",\n  \"client_id\": \"094534589...438598743\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/starting-account...iam.gserviceaccount.com\",\n  \"universe_domain\": \"googleapis.com\"\n}\nTo get the Google Analytics property identifier, see https://developers.google.com/analytics/devguides/reporting/data/v1/property-id#what_is_my_property_id\nWe need both the Google Analytics property identifier (properties/232..132) and credentials.json file to be able to fetch analytics data."
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#google-apis-explorer",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#google-apis-explorer",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Google APIs Explorer",
    "text": "Google APIs Explorer\nGoogle provides an APIs Explorer specifically for testing Google Analytics Data API calls.\n\n\n\nGoogle Analytics API Explorer\n\n\nHere’s how to use it:\n\nVisit: https://developers.google.com/analytics/devguides/reporting/data/v1/rest/v1beta/properties/runReport\nReplace PROPERTY_ID with your actual Google Analytics 4 property ID, instructions here: https://developers.google.com/analytics/devguides/reporting/data/v1/property-id\nUse the following request body and update the dateRanges.startDate (format: YYYY-MM-DD), dateRanges.endDate (format: YYYY-MM-DD) and page path (dimensionFilter.filter.stringFilter.value) whose views you want to get. The startDate can be any old day, even older than the website and endDate can be any future date.\n\n{\n \"dateRanges\": [\n   {\n     \"startDate\": \"2024-01-01\",\n     \"endDate\": \"2024-12-31\"\n   }\n ],\n \"dimensions\": [\n   {\n     \"name\": \"pagePath\"\n   }\n ],\n \"dimensionFilter\": {\n   \"filter\": {\n     \"fieldName\": \"pagePath\",\n     \"stringFilter\": {\n       \"matchType\": \"CONTAINS\",\n       \"value\": \"/posts/private-domain-checker\"\n     }\n   }\n },\n \"metrics\": [\n   {\n     \"name\": \"screenPageViews\"\n   }\n ]\n}\n\nClick the “Execute” button.\n\nThis will allow you to send a test POST request with your configured parameters and see the response from the API, including any errors or the data you requested.\nSuccess Response:\n// HTTP/1.1 200 \n{\n  \"dimensionHeaders\": [\n    {\n      \"name\": \"pagePath\"\n    }\n  ],\n  \"metricHeaders\": [\n    {\n      \"name\": \"screenPageViews\",\n      \"type\": \"TYPE_INTEGER\"\n    }\n  ],\n  \"rows\": [\n    {\n      \"dimensionValues\": [\n        {\n          \"value\": \"/posts/private-domain-checker/\"\n        }\n      ],\n      \"metricValues\": [\n        {\n          \"value\": \"82\"\n        }\n      ]\n    },\n    {\n      \"dimensionValues\": [\n        {\n          \"value\": \"/posts/private-domain-checker/index.html\"\n        }\n      ],\n      \"metricValues\": [\n        {\n          \"value\": \"5\"\n        }\n      ]\n    },\n    {\n      \"dimensionValues\": [\n        {\n          \"value\": \"/posts/private-domain-checker\"\n        }\n      ],\n      \"metricValues\": [\n        {\n          \"value\": \"2\"\n        }\n      ]\n    }\n  ],\n  \"rowCount\": 3,\n  \"metadata\": {\n    \"currencyCode\": \"USD\",\n    \"timeZone\": \"Africa/Nairobi\"\n  },\n  \"kind\": \"analyticsData#runReport\"\n}\n\nError Reponse:\n// HTTP/1.1 400 \n{\n  \"error\": {\n    \"code\": 400,\n    \"message\": \"Invalid property ID: 465...623. A numeric Property ID is required. \n    To learn more about Property ID, see \n    https://developers.google.com/analytics/devguides/reporting/data/v1/property-id.\",\n    \"status\": \"INVALID_ARGUMENT\"\n  }\n}"
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#python-example",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#python-example",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Python Example",
    "text": "Python Example\ninstall the following packages, google-auth-oauthlib and google-api-python-client for ease of authentication. You need to obtain a property_key and credentials from google. You also need to allow the credentials account to read the google analytics data. This is currently free and no cost is incured. below are the steps how to setup your analytics account, service account and obtain the credentials file.\n\nCode\n\n# %pip install google-auth-oauthlib\n# %pip install --upgrade google-api-python-client\n\nfrom google.oauth2.service_account import Credentials\nfrom googleapiclient.discovery import build\nfrom datetime import datetime, timedelta\nfrom IPython.display import SVG\n\ndef get_values(response, searchKey)-&gt; list[str]:\n    return [ \n        value.get('value', None) \n        for row  \n        in response.get('rows', []) \n            for key, values \n            in row.items() \n                for value \n                in values\n                if key == searchKey\n    ]\n\ndef get_views(\n        page_path: str, \n        property_key: str, \n        credentials_filename: str) -&gt; tuple[int, list[str]]:\n    SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']\n    credentials = Credentials.from_service_account_file(\n        credentials_filename,\n        scopes=SCOPES)\n    analytics = build('analyticsdata', 'v1beta', credentials = credentials)\n    tomorrow_date = (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d')\n    body = {\n        \"dateRanges\": [{\n            \"startDate\": \"2024-01-01\",\n            \"endDate\": tomorrow_date\n        }],\n        \"dimensions\": [{\n            \"name\": \"pagePath\"\n        }],\n        \"dimensionFilter\": {\n            \"filter\": {\n                \"fieldName\": \"pagePath\",\n                \"stringFilter\": {\n                    \"matchType\": \"CONTAINS\",\n                    \"value\": page_path\n                }\n            }\n        },\n        \"metrics\": [{\n            \"name\": \"screenPageViews\"\n        }]\n    }\n    response = analytics.properties().runReport(\n        property=property_key,\n        body=body).execute()\n    metricValues = get_values(response, 'metricValues')\n    dimensionValues = get_values(response, 'dimensionValues')\n\n    metadata = [\n        f\"{dimension} :: {metric}\" \n        for (dimension, metric) \n        in zip(dimensionValues, metricValues)\n    ]\n    return (\n        sum([int(i or 0) for i in metricValues]), \n        metadata\n    )\n\ndef generate_badge(label: str, value: str | int, metadata: list[str] = []) -&gt; str:\n    value_text = f\"{value:,}\" if isinstance(value, int) else str(value)\n    \n    # Calculate widths\n    label_width = len(label) * 8 + 10  # Approximate width calculation\n    value_width = len(value_text) * 8 + 10  # Approximate width calculation\n    total_width = label_width + value_width\n\n    # Generate metadata comments\n    metadata_comments = \"\\n      \".join(\n        [f\"&lt;!-- METADATA: {item.replace('--&gt;', '—&gt;')} --&gt;\" for item in metadata])\n\n    return f'''&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{total_width}\" height=\"20\"&gt;\n      {metadata_comments}\n      &lt;linearGradient id=\"b\" x2=\"0\" y2=\"100%\"&gt;\n        &lt;stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/&gt;\n        &lt;stop offset=\"1\" stop-opacity=\".1\"/&gt;\n      &lt;/linearGradient&gt;\n      &lt;mask id=\"a\"&gt;\n        &lt;rect width=\"{total_width}\" height=\"20\" rx=\"3\" fill=\"#fff\"/&gt;\n      &lt;/mask&gt;\n      &lt;g mask=\"url(#a)\"&gt;\n        &lt;rect width=\"{label_width}\" height=\"20\" fill=\"#555\"/&gt;\n        &lt;rect x=\"{label_width}\" width=\"{value_width}\" height=\"20\" fill=\"#4c1\"/&gt;\n        &lt;rect width=\"{total_width}\" height=\"20\" fill=\"url(#b)\"/&gt;\n      &lt;/g&gt;\n      &lt;g fill=\"#fff\" text-anchor=\"middle\" font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"13\"&gt;\n        &lt;text x=\"{label_width/2}\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;{label}&lt;/text&gt;\n        &lt;text x=\"{label_width/2}\" y=\"14\"&gt;{label}&lt;/text&gt;\n        &lt;text x=\"{label_width + value_width/2}\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;{value_text}&lt;/text&gt;\n        &lt;text x=\"{label_width + value_width/2}\" y=\"14\"&gt;{value_text}&lt;/text&gt;\n      &lt;/g&gt;\n    &lt;/svg&gt;'''.strip()\n\ndef format_number(num: int|float):\n    \"\"\"\n    Format a number similar to YouTube's style:\n    - Less than 1000: show as is (100, 999)\n    - Thousands: show as K (1K, 1.3K)\n    - Millions: show as M (1.1M, 2.4M)\n    \n    Args:\n        num (int/float): Number to format\n        \n    Returns:\n        str: Formatted number string\n    \"\"\"\n    abs_num = abs(num)\n    sign = '-' if num &lt; 0 else ''\n    \n    if abs_num &lt; 1000:\n        return f\"{sign}{abs_num:d}\"\n        \n    elif abs_num &lt; 1000000:\n        formatted = abs_num / 1000\n        # If the decimal part is 0, don't show it\n        if formatted.is_integer():\n            return f\"{sign}{int(formatted)}K\"\n        return f\"{sign}{formatted:.1f}K\"\n        \n    else:\n        formatted = abs_num / 1000000\n        if formatted.is_integer():\n            return f\"{sign}{int(formatted)}M\"\n        return f\"{sign}{formatted:.1f}M\"\n\n\n\nTests\n\nviews_count, metadata = get_views(\n    page_path = \"/posts/private-domain-checker\", \n    property_key = \"properties/465...623\",\n    credentials_filename = '../../../SERVICE_ACCOUNT_CREDENTIALS.json')\nsvg_code = generate_badge(\n    label =\"readers\", \n    value = format_number(views_count), \n    metadata = metadata)\n\n\nprint(svg_code)\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"92\" height=\"20\"&gt;\n      &lt;!-- METADATA: /posts/private-domain-checker/ :: 88 --&gt;\n      &lt;!-- METADATA: /posts/private-domain-checker/index.html :: 5 --&gt;\n      &lt;!-- METADATA: /posts/private-domain-checker :: 2 --&gt;\n      &lt;linearGradient id=\"b\" x2=\"0\" y2=\"100%\"&gt;\n        &lt;stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/&gt;\n        &lt;stop offset=\"1\" stop-opacity=\".1\"/&gt;\n      &lt;/linearGradient&gt;\n      &lt;mask id=\"a\"&gt;\n        &lt;rect width=\"92\" height=\"20\" rx=\"3\" fill=\"#fff\"/&gt;\n      &lt;/mask&gt;\n      &lt;g mask=\"url(#a)\"&gt;\n        &lt;rect width=\"66\" height=\"20\" fill=\"#555\"/&gt;\n        &lt;rect x=\"66\" width=\"26\" height=\"20\" fill=\"#4c1\"/&gt;\n        &lt;rect width=\"92\" height=\"20\" fill=\"url(#b)\"/&gt;\n      &lt;/g&gt;\n      &lt;g fill=\"#fff\" text-anchor=\"middle\" font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"13\"&gt;\n        &lt;text x=\"33.0\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;readers&lt;/text&gt;\n        &lt;text x=\"33.0\" y=\"14\"&gt;readers&lt;/text&gt;\n        &lt;text x=\"79.0\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;95&lt;/text&gt;\n        &lt;text x=\"79.0\" y=\"14\"&gt;95&lt;/text&gt;\n      &lt;/g&gt;\n    &lt;/svg&gt;\n\n\n\nSVG(svg_code)\n\n\n\n\n\n\n\n\n\nviews_count, metadata = get_views(\n    page_path = \"/\",\n    property_key = \"properties/465...623\",\n    credentials_filename = '../../../SERVICE_ACCOUNT_CREDENTIALS.json')\nSVG(generate_badge(\n    label = \"total website views\", \n    value = format_number(views_count), \n    metadata = metadata))\n\n\n\n\n\n\n\n\nBelow is a live version of the total website views:\nhttps://toknow.ai/pageviews?page_path=/&label=total%20website%20views\n\n\n\nLive Total Views for ToKnow.ai"
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#using-cloudflare-worker",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#using-cloudflare-worker",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Using Cloudflare Worker",
    "text": "Using Cloudflare Worker\nAt the time of this writting, cloudflare workers supports various languages 1. However, Python and Rust are currently in beta. Python packages do not run in production, you can only deploy Python Workers that use the standard library and a few select packages. That means we cant use the gogle python packages google-auth-oauthlib and google-api-python-client. We would have to create the authentication logic from scratch, which adds maintenance burden. Also for python, only HTTP libraries that are able to make requests asynchronously are supported. Currently, these include aiohttp and httpx. Also, python is not really a first class language in a true sense, it still uses webassembly and pyodide: https://blog.cloudflare.com/python-workers/.\nJavascript seems to be the best candidate for a cloudflare worker. And since Typescript is also supported, its easier to use typescript for type safety and maintentance. Incase the workers API changes in future, typescript will highlight the areas that are incompatible.\n\nImplementing Typescript Worker\nDespite Javascript and Typescript being the best languages to implement the worker, there is also some limitations. Not all NodeJS packages and environment are supported. As such, we’ll also need to implement a google authentication from scratch. But fortunately, its easier to get a third party package compatible with cloudflare, such as https://github.com/Schachte/cloudflare-google-auth 2. This offsets alot of maintenance. https://github.com/Schachte/cloudflare-google-auth seems to be the package with latest mainteinance, but there are other older and possibly working implementations and online discussions about authenticating and authorizing google API calls using Service Account credentials 3 4 5 6 7 8\n\n// https://github.com/ToKnow-ai/display-google-analytics-views-using-cloudflare-worker/blob/main/worker.ts\n\nimport GoogleAuth, { GoogleKey } from 'cloudflare-workers-and-google-oauth'\n\n// JSON containing the key for the service account\n// download from GCS\nexport interface Env {\n    SERVICE_ACCOUNT_CREDENTIALS: string;\n    GA_PROPERTY_ID: string;\n}\n\nconst getTomorrowDate = () =&gt; {\n    const tomorrow = new Date();\n    tomorrow.setDate(tomorrow.getDate() + 1);\n    return tomorrow.toISOString().split('T')[0];\n}\n\n// Helper function to calculate text width\nconst getTextWidth = (text: string): number =&gt; {\n  // Approximate character widths (can be adjusted for more accuracy)\n  const averageCharWidth = 8;\n  return text.length * averageCharWidth + 10; // Adding padding\n};\n\n// Enhanced helper function to generate SVG badge\nconst generateBadge = (label: string, value: string | number, metadata: string[] = []) =&gt; {\n  const valueText: string = typeof value === 'number' ? value.toLocaleString() : value;\n  \n  // Calculate widths\n  const labelWidth = getTextWidth(label);\n  const valueWidth = getTextWidth(valueText);\n  const totalWidth = labelWidth + valueWidth;\n\n  // Generate metadata comments\n  const metadataComments = metadata\n    .map(item =&gt; `&lt;!-- METADATA: ${item.replace(/--&gt;/g, '—&gt;')} --&gt;`)\n    .join('\\n');\n\n  return `&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n    &lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"${totalWidth}\" height=\"20\"&gt;\n      ${metadataComments}\n      &lt;linearGradient id=\"b\" x2=\"0\" y2=\"100%\"&gt;\n        &lt;stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/&gt;\n        &lt;stop offset=\"1\" stop-opacity=\".1\"/&gt;\n      &lt;/linearGradient&gt;\n      &lt;mask id=\"a\"&gt;\n        &lt;rect width=\"${totalWidth}\" height=\"20\" rx=\"3\" fill=\"#fff\"/&gt;\n      &lt;/mask&gt;\n      &lt;g mask=\"url(#a)\"&gt;\n        &lt;rect width=\"${labelWidth}\" height=\"20\" fill=\"#555\"/&gt;\n        &lt;rect x=\"${labelWidth}\" width=\"${valueWidth}\" height=\"20\" fill=\"#4c1\"/&gt;\n        &lt;rect width=\"${totalWidth}\" height=\"20\" fill=\"url(#b)\"/&gt;\n      &lt;/g&gt;\n      &lt;g fill=\"#fff\" text-anchor=\"middle\" font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"13\"&gt;\n        &lt;text x=\"${labelWidth/2}\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;${label}&lt;/text&gt;\n        &lt;text x=\"${labelWidth/2}\" y=\"14\"&gt;${label}&lt;/text&gt;\n        &lt;text x=\"${labelWidth + valueWidth/2}\" y=\"15\" fill=\"#010101\" fill-opacity=\".3\"&gt;${valueText}&lt;/text&gt;\n        &lt;text x=\"${labelWidth + valueWidth/2}\" y=\"14\"&gt;${valueText}&lt;/text&gt;\n      &lt;/g&gt;\n    &lt;/svg&gt;`.trim();\n};\n\nconst formatNumber = (num: number) =&gt; {\n    const isNegative = num &lt; 0;\n    const absNum = Math.abs(num);\n\n    if (absNum &lt; 1000) {\n        return `${isNegative ? '-' : ''}${Math.trunc(absNum)}`;\n    } else if (absNum &lt; 1000000) {\n        const formattedNum = absNum / 1000;\n        return `${isNegative ? '-' : ''}${formattedNum.toFixed(formattedNum % 1 === 0 ? 0 : 1)}K`;\n    } else {\n        const formattedNum = absNum / 1000000;\n        return `${isNegative ? '-' : ''}${formattedNum.toFixed(formattedNum % 1 === 0 ? 0 : 1)}M`;\n    }\n}\n\ntype Entries&lt;T&gt; = { [K in keyof T]: [K, T[K]]; }[keyof T][];\n\nnamespace GoogleAnalyticsReport {\n  type MetricType = 'TYPE_INTEGER' | 'TYPE_FLOAT' | 'TYPE_STRING';\n\n  interface DimensionHeader {\n    name: string;\n  }\n\n  interface MetricHeader {\n    name: string;\n    type: MetricType;\n  }\n\n  export interface DimensionValue {\n    value: string;\n  }\n\n  export interface MetricValue {\n    value: string;\n  }\n\n  export interface ReportRow {\n    dimensionValues: DimensionValue[];\n    metricValues: MetricValue[];\n  }\n\n  interface ReportMetadata {\n    currencyCode: string;\n    timeZone: string;\n  }\n\n  export interface Report {\n    dimensionHeaders: DimensionHeader[];\n    metricHeaders: MetricHeader[];\n    rows: ReportRow[];\n    rowCount: number;\n    metadata: ReportMetadata;\n    kind: string;\n  }\n}\n\nconst getValues = (\n    analyticsResponse: GoogleAnalyticsReport.Report, \n    searchKey: keyof GoogleAnalyticsReport.ReportRow) =&gt; {\n    return (analyticsResponse?.['rows'] ?? [])\n        .flatMap(row =&gt; Object.entries(row) as Entries&lt;GoogleAnalyticsReport.ReportRow&gt;)\n        .filter(([key, _]) =&gt; key ===  searchKey)\n        .flatMap(([_, values]) =&gt; values)\n        .map(value =&gt; value?.['value'])\n}\n\nconst IMAGE_CACHE_SECONDS = 45 * 60; // Cache for 45 minutes\nconst GOOGLE_CALL_CACHE_TTL_SECONDS = 45 * 60; // 45 minutes before revalidating the resource\n\nexport default {\n    async fetch(\n        request: Request,\n        env: Env,\n        ctx: ExecutionContext\n    ): Promise&lt;Response&gt; {\n        try {\n            const cache = caches.default\n            let response = await cache.match(request)\n            if (!response) {\n                if (!env.SERVICE_ACCOUNT_CREDENTIALS || !env.GA_PROPERTY_ID) {\n                    throw new Error(\"No credentials\")\n                }\n\n                const scopes: string[] = [\n                    'https://www.googleapis.com/auth/analytics.readonly']\n                const googleAuth: GoogleKey = JSON.parse(\n                    atob(env.SERVICE_ACCOUNT_CREDENTIALS))\n                // initialize the service\n                const oauth = new GoogleAuth(googleAuth, scopes)\n                const token = await oauth.getGoogleAuthToken()\n\n                if (token === undefined) {\n                    throw new Error(\"generating Google auth token failed\")\n                }\n\n                // Only allow GET requests\n                if (request.method !== 'GET') {\n                    return new Response('Method not allowed', { status: 405 });\n                }\n\n                const getQuery = (queryName) =&gt; [\n                    ...new URL(request.url).searchParams.entries()].find(\n                        ([key, _]) =&gt; (key || '').toLowerCase().trim() === queryName)?.[1]\n                const page_path = getQuery(\"page_path\");\n                if (!page_path) {\n                    return new Response('page_path not available', { status: 405 });\n                }\n                // Construct the request body\n                const requestBody = {\n                    \"dateRanges\": [{\n                        \"startDate\": \"2024-01-01\",\n                        \"endDate\": getTomorrowDate()\n                    }],\n                    \"dimensions\": [{\n                        \"name\": \"pagePath\"\n                    }],\n                    \"dimensionFilter\": {\n                        \"filter\": {\n                            \"fieldName\": \"pagePath\",\n                            \"stringFilter\": {\n                                \"matchType\": \"CONTAINS\",\n                                \"value\": page_path.toLowerCase().trim()\n                            }\n                        }\n                    },\n                    \"metrics\": [{\n                        \"name\": \"screenPageViews\"\n                    }]\n                };\n\n                // Make request to Google Analytics API\n                const analyticsResponse = await fetch(\n                    `https://analyticsdata.googleapis.com/v1beta/${env.GA_PROPERTY_ID}:runReport`,\n                    {\n                        method: 'POST',\n                        headers: {\n                            'Authorization': `Bearer ${token}`,\n                            'Content-Type': 'application/json',\n                        },\n                        body: JSON.stringify(requestBody),\n                        'cf': {\n                            // Always cache this fetch regardless of content type\n                            // for a max of 45 minutes before revalidating the resource\n                            cacheTtl: GOOGLE_CALL_CACHE_TTL_SECONDS,\n                            cacheEverything: true\n                        },\n                    });\n\n                if (!analyticsResponse.ok) {\n                    return new Response(\n                        JSON.stringify({ error: await analyticsResponse.text() }, null, 4),\n                        {\n                            status: 500,\n                            headers: {\n                                'Content-Type': 'application/json'\n                            }\n                        });\n                }\n\n                const data = await analyticsResponse.json&lt;GoogleAnalyticsReport.Report&gt;();\n                const metricValues = getValues(data, 'metricValues');\n                const dimensionValues = getValues(data, 'dimensionValues');\n\n                const viewsCount = metricValues\n                    .map(value =&gt; parseInt(value ?? '0'))\n                    .reduce((total, count) =&gt; total + count, 0);\n\n                // take the first 20 to avoid bloating the image. Sort desc in future..\n                const metadata = dimensionValues.slice(0, 20).map((value, index) =&gt; \n                    `${value} :: ${metricValues?.[index]?.toLocaleString?.() ?? 'NULL'}`);\n                \n                const shorten = getQuery(\"shorten\") || false;\n                const label = getQuery(\"label\") || \"readers\";\n                const badgeSVG = generateBadge(\n                    label, \n                    shorten ? formatNumber(viewsCount) : viewsCount, \n                    metadata);\n                response = new Response(\n                    badgeSVG, \n                    {\n                        headers: \n                        {\n                            'Content-Type': 'image/svg+xml',\n                            'Cache-Control': `public, max-age=${IMAGE_CACHE_SECONDS}`,\n                            'Access-Control-Allow-Origin': '*'\n                        }\n                    });\n\n                // Cache API respects Cache-Control headers\n                ctx.waitUntil(cache.put(request, response.clone()));\n            }\n            return response\n        } catch (error) {\n            return new Response(\n                JSON.stringify({\n                    error: error.message || 'Internal server error',\n                    timestamp: new Date().toISOString()\n                }),\n                {\n                    status: 500,\n                    headers: {\n                        'Content-Type': 'application/json'\n                    }\n                });\n        }\n    },\n};\n\n\n\nTesting, Deployment and Troubleshooting Cloudflare Workers\n\nWorkers &gt; Get started\nTypeScript is a first-class language on Cloudflare Workers\nWrangler\nDeploy a real-time chat application\nCreate a deploy button with GitHub Actions\nExplore examples of Workers\nExamples of how community developers are getting the most out of Workers\nCreating and Deploying a Cloudflare worker with Nodejs (Typescript)"
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#alternatives-to-cloudflare",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#alternatives-to-cloudflare",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Alternatives to Cloudflare",
    "text": "Alternatives to Cloudflare\nThere exist other serverless services that can offer a better service than cloudflare workers, such as Azure Functionns from microsoft, Lambda Functions from Amazon Web Services and Google Cloud Functions from Google. I have extensively worked with Azure Functions for their simplicity compared to Lambda Functions or Google Cloud Functions. However, Cloudflare Workers still offer the best developer overall experience, is easier to setup, easier to use the domain path (example.com/&lt;any/path/can/trigger&gt;) to trigger your worker if your domain DNS is managed by cloudflare and doesnt require a credit card to signup and start testing & experimenting. See Table 1\n\n\n\nTable 1: Cloudflare Workers from Cloudflare, Azure Functionns from microsoft, Lambda Functions from Amazon Web Services and Google Cloud Functions from Google for a serverless architecture\n\n\n\n\n\n\nFeature\nCloudflare Workers\nAzure Functions\nAWS Lambda Functions\nGoogle Cloud Functions\n\n\n\n\nEase of Setup\nVery easy - simple web interface and CLI tools 9\nModerate - requires Azure account setup and tooling 10\nModerate - requires AWS account setup and IAM configuration 11\nModerate - requires GCP project setup 12\n\n\nDeveloper Experience\nExcellent - fast deployment, good documentation, simple testing 13 14 15 16\nGood - extensive tooling, VS Code integration 17\nGood - comprehensive tooling, multiple deployment options 18 19\nGood - clean interface, good documentation 20 21 22 23\n\n\nPricing\nFree tier: 100,000 requests/day and 1000 requests/min per accountStandard tier: starts at $5 USD per month 24 25\nFree tier: 1 million requests/monthThen $0.20/million requests 26\nFree tier: 1 million requests/monthThen $0.20/million requests 27\nFree tier: 2 million requests/monthThen $0.40/million requests 28\n\n\nCredit Card Required\nNo 29\nYes 30\nYes 31\nYes 32\n\n\nPerformance\nExcellent - near zero cold start, global edge deployment 33 34 35 36\nGood - 100-1000ms cold start on average but also supports always ready instances 37 38 39 40\nGood - 100-1000ms cold start for less than 1% of invocations 41 42\nGood - 100-2000ms cold start 43 44\n\n\nLanguage Support\nLimited - JavaScript, TypeScript, Rust, Python (beta), WebAssembly 45\nExtensive - Node.js, Python, Java, .NET, PowerShell 46\nExtensive - Node.js, Python, Java, .NET, Go, Ruby 47\nGood - Node.js, Python, Go, Java, .NET 48\n\n\nIntegration with Other Services\nExcellent with Cloudflare services 49Limited with external services\nExcellent with Azure services 50 Good with external services\nExcellent with AWS services 51 Good with external services\nExcellent with Google services 52Good with external services\n\n\nScalability\nAutomatic, global edge network 53\nAutomatic, regional 54\nAutomatic, regional 55\nAutomatic, regional 56\n\n\nSecurity\nBuilt-in DDoS protectionSSL/TLS by default 57 58\nAzure Security Center integrationKey Vault integration 59\nIAM integrationKMS integration 60 61 62\nCloud IAM integrationSecret Manager 63\n\n\nMonitoring and Logging\nBasic metrics and logsLimited retention 64\nAdvanced Azure Monitor integration 65\nAdvanced CloudWatch integration 66\nAdvanced Cloud Monitoring integration 67\n\n\nCustom Domain Support\nNative support with Cloudflare DNSEasy SSL setup 68\nRequires App Service domain or custom domain setup 69\nRequires API Gateway or custom domain setup 70\nRequires domain mapping configuration 71\n\n\nKey Limitations\n- No filesystem access- No raw TCP or UDP access- Can’t use many Node.js built-in processes and modules like fs, Node crypto module etc- Other technical limits here and here\nSee limits here\n- Maximum deployment package size of 50MB zipped (250MB unzipped)- See More limits here, here and here\nSee limits here\n\n\n\n\n\n\n\nThere are other options for serverless services such as CloudFront Functions, DigitalOcean Functions, Oracle Cloud Functions, etc.\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#footnotes",
    "href": "posts/display-google-analytics-views-using-cloudflare-worker/index.html#footnotes",
    "title": "Free, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nCloudflare offers first-class support for JavaScript, TypeScript, Python, Rust and WebAssembly. WebAssembly allows one to write Workers using C, C++, Kotlin, Go and more↩︎\nhttps://github.com/Schachte/cloudflare-google-auth and https://ryan-schachte.com/blog/oauth_cloudflare_workers/↩︎\nNode.js helper libraries Google provide don’t work with Cloudflare workers↩︎\n( ◕◡◕)っ Cloudflare Workers Google OAuth and Implementing Google OAuth to use Google API in Cloudflare Workers↩︎\nWeb Auth Library↩︎\nExample: Google OAuth 2.0 for Service Accounts using CF Worker↩︎\nConverts Google service user OAuth2 credentials into an access token in Cloudflare-compatible JS↩︎\nAllow retrieving an OAuth 2.0 authentication token for interacting with Google services using the service account key↩︎\nWorkers &gt; Get started↩︎\nGetting started with Azure Functions↩︎\nCreate your first Lambda function↩︎\nCreate a Cloud Run function by using the Google Cloud CLI↩︎\nUnderstand how your Worker projects are performing via logs, traces, and other data sources.↩︎\nSend debugging information in an errored response to a logging service.↩︎\nBetter debugging for Cloudflare Workers, now with breakpoints↩︎\nDebug via breakpoints↩︎\nDevelop Azure Functions by using Visual Studio Code↩︎\nDeploying Lambda functions as .zip file archives↩︎\nCreate a Lambda function using a container image↩︎\nCloud Run functions overview↩︎\nCreate a Cloud Run function by using the Google Cloud console↩︎\nCreate a Cloud Run function by using the Google Cloud CLI↩︎\nCreate a Cloud Run function by using Cloud Code for Cloud Shell↩︎\nUsers on the Workers Paid plan have access to the Standard usage model.. The Free tier offers 100,000 requests per day with no charge for duration and 10 milliseconds of CPU time per invocation. The Standard tier provides 10 million included requests per month, with additional requests costing $0.30 per million. There’s no charge or limit for duration. The Standard tier includes 30 million CPU milliseconds per month, with additional milliseconds costing $0.02 per million. Each invocation is limited to 30 seconds of CPU time, and Cron Triggers or Queue Consumers are limited to 15 minutes per invocation. Inbound requests to your Worker. Cloudflare does not bill for subrequests you make from your Worker. Requests to static assets are free and unlimited.↩︎\nWorkers Paid plan is separate from any other Cloudflare plan (Free, Professional, Business) you may have. Only requests that hit a Worker will count against your limits and your bill. Since Cloudflare Workers runs before the Cloudflare cache, the caching of a request still incurs costs↩︎\nAzure Functions offers a spectrum of hosting options to balance cost, control, and features.\n\nConsumption Plan:\n\nTraditional serverless model with pay-per-use billing\nIncludes free grant of 1 million requests and 400,000 GB-s monthly\nScales based on events but lacks VNet support\nMaximum 200 instances\nNo cold start mitigation\n\nFlex Consumption Plan:\n\nEnhanced serverless model with more flexibility\nSupports VNet integration and always-ready instances\nFree grant of 250,000 executions and 100,000 GB-s monthly\nFaster scaling with up to 1,000 instances\nMultiple memory size options (2,048 MB and 4,096 MB)\nIncludes both on-demand and always-ready billing modes\n\nPremium Plan:\n\nEnhanced performance with no cold starts\nPre-warmed instances and VNet access\nBilling based on core seconds and memory allocation\nRequires at least one instance always running\nScales up to 100 instances (region dependent)\nAvailable with 1-year or 3-year savings plans\n\nDedicated (App Service) Plan:\n\nRuns on dedicated VMs with predictable costs\nLimited to 10-30 instances depending on tier\nNo auto-scaling to zero\nBest for utilizing existing App Service resources\nIncludes deployment slots and Kudu console access\n\nContainer Options:\n\nAzure Container Apps (Recommended):\n\nManaged Kubernetes environment\nSupports scale-to-zero and serverless billing\nIncludes KEDA, Dapr, and mTLS support\nUp to 1,000 instances\nWorkload profiles for dedicated hardware/GPUs\n\nAzure Arc-enabled Kubernetes (Preview):\n\nRun Functions on any Kubernetes cluster\nManaged through Azure\nSupports both code and container deployments\n\nDirect Kubernetes:\n\nOpen-source deployment option\nUses KEDA for event-driven scaling\nCommunity support model\nRequires self-management of containers\n\n\n\nSee more Feature support comparison and Azure Functions Pricing - 2024 Guide to Azure Functions Costs & Optimization↩︎\nThe AWS Lambda free tier includes one million free requests per month and 400,000 GB-seconds of compute time per month, usable for functions powered by both x86, and Graviton2 processors, in aggregate.. Calculate your AWS Lambda and architecture cost in a single estimate using AWS Pricing Calculator↩︎\nDepending on which version of Cloud Run functions you are using, you have two pricing options; Cloud Run pricing and Cloud Run functions (1st gen) pricing↩︎\nhttps://dash.cloudflare.com/sign-up/workers-and-pages↩︎\nhttps://azure.microsoft.com/en-us/free/↩︎\nhttps://aws.amazon.com/free/↩︎\nhttps://cloud.google.com/functions?hl=en↩︎\nCloudflare Account plan limits, Request limits, Response limits, Worker limits(Duration, CPU time), and Cache API limits. If you are running into limits, your project may be a good fit for Workers for Platforms. You can also request an adjustment to a limit by complete the Limit Increase Request Form↩︎\nEliminating cold starts with Cloudflare Workers↩︎\nCloudflare Workers has eliminated cold starts entirely, meaning they need zero spin up time. This is the case in every location in Cloudflare’s global network. In contrast, both Lambda and Lambda@Edge functions can take over a second to respond from a cold start.↩︎\nTo solve this famous cold start problem, Cloudflare designed a different approach, and they claim 0ms cold starts all around the world↩︎\nAzure Functions Cold start behavior↩︎\nOur latest work to improve Azure Functions cold starts↩︎\nA typical cold start latency spans from 1 to 10 seconds. However, less lucky executions may take up to 30 seconds occasionally. PowerShell functions are especially slow to start with values from 4 to 27 seconds. View detailed distributions: Cold Start Duration per Language↩︎\nThe cold start of a Function app in the Consumption plan typically ranges between 1 and 3 seconds↩︎\nComparison of Cold Starts in Serverless Functions across AWS, Azure, and GCP↩︎\nCold starts typically occur in under 1% of invocations. The duration of a cold start varies from under 100 ms to over 1 second. Provisioned Concurrency can keep your functions initialized and warm, ready to respond in double-digit milliseconds↩︎\nIn Cloud Run functions (1st gen), the amount of memory granted to a function impacts the CPU allocation, which in turn can have an impact on cold start time. While this is also true in Cloud Run functions by default, in Cloud Run functions you have the option of configuring CPU allocation separate from memory. Latency can also be reduced by setting a minimum number of instances to avoid cold starts↩︎\nComparison of Cold Starts in Serverless Functions across AWS, Azure, and GCP↩︎\nCloudflare offers first-class support for JavaScript, TypeScript, Python, Rust and WebAssembly. WebAssembly allows one to write Workers using C, C++, Kotlin, Go and more↩︎\nSupported languages in Azure Functions↩︎\nRuby, Java, Python, Node.js, .NET↩︎\nCloud Run functions supports multiple language runtimes↩︎\nBindings allow your Worker to interact with resources on the Cloudflare Developer Platform.↩︎\nAzure Functions triggers and bindings concepts↩︎\nInvoking Lambda with events from other AWS services↩︎\nAccess Google Cloud APIs from Cloud Run functions by using a service account to act on your behalf. The service account provides Application Default Credentials for your functions↩︎\nWorker limits↩︎\nMaximum instances are given on a per-function app (Consumption) or per-plan (Premium/Dedicated) basis, unless otherwise indicated↩︎\nUnderstanding Lambda function scaling↩︎\nAuto-scaling behavior - Cloud Run functions↩︎\nIntroducing Automatic SSL/TLS: securing and simplifying origin connectivity↩︎\nBy default, DDoS attack protection is always enabled, to customize, create a DDoS override↩︎\nSecuring Azure Functions↩︎\nSecurity in AWS Lambda↩︎\nData protection in AWS Lambda↩︎\nIdentity and Access Management for AWS Lambda↩︎\nSecure your Cloud Run function. One way to control access to a function is to require that the requesting entity identify itself by using a credential.. You can also limit access by specifying network settings for individual functions. This allows for fine-tuned control over the network ingress and egress to and from your functions.↩︎\nUnderstand how your Worker projects are performing via logs, traces, and other data sources; DevTools, Errors and exceptions, Integrations, Logs, Metrics and analytics and Source maps and stack traces↩︎\nMonitor executions in Azure Functions. Azure Functions offers built-in integration with Azure Application Insights to monitor functions executions↩︎\nMonitoring and troubleshooting Lambda functions. Lambda automatically monitors Lambda functions on your behalf and reports metrics through Amazon CloudWatch↩︎\nMonitor your Cloud Run function. Google Cloud Observability provides logging and monitoring tools that help you understand what is happening in your functions↩︎\nCustom Domains allow you to connect your Worker to a domain or subdomain, without having to make changes to your DNS settings or perform any certificate management↩︎\nMap an existing custom DNS name to Azure App Service. You can use Azure DNS to manage DNS records for your domain and configure a custom DNS name for Azure App Service, see Tutorial: Host your domain in Azure DNS↩︎\nCustom domain name for public REST APIs in API Gateway. For information about custom domain names for private APIs, see Custom domain names for private APIs in API Gateway↩︎\nConfigure network settings. Cloud Run functions network settings enable you to control network ingress and egress to and from individual functions↩︎"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/random-walk/index.html",
    "href": "posts/computational-techniques-in-data-science/random-walk/index.html",
    "title": "Random Walk",
    "section": "",
    "text": "A random walk is a mathematical model that describes a path consisting of a series of random steps.\nIf \\(X_n\\) represents the position at step \\(n\\), then:\n\\(X_n = X_{n−1} + S_n\\)\nwhere \\(S_n\\) is a random step, typically drawn from a probability distribution (e.g., uniform, normal, or Bernoulli)."
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/random-walk/index.html#d-random-walk-one-dimensional",
    "href": "posts/computational-techniques-in-data-science/random-walk/index.html#d-random-walk-one-dimensional",
    "title": "Random Walk",
    "section": "1D Random Walk (One-Dimensional)",
    "text": "1D Random Walk (One-Dimensional)\nA 1D random walk occurs when movement is restricted to a single axis (e.g., left or right along a line). At each step, the walker moves left (-1) or right (+1) with equal probability (if unbiased).\nExample: Stock prices"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/random-walk/index.html#d-random-walk-two-dimensional",
    "href": "posts/computational-techniques-in-data-science/random-walk/index.html#d-random-walk-two-dimensional",
    "title": "Random Walk",
    "section": "2D Random Walk (Two-Dimensional)",
    "text": "2D Random Walk (Two-Dimensional)\nA 2D random walk allows movement in two perpendicular directions (x and y). At each step, the walker can move up, down, left, or right.\nExample: Brownian motion"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/random-walk/index.html#applications",
    "href": "posts/computational-techniques-in-data-science/random-walk/index.html#applications",
    "title": "Random Walk",
    "section": "Applications",
    "text": "Applications\n\nImports and Shared Python code\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# For reproducibility\nnp.random.seed(37) \n\n# Function to perform a single random walk\ndef random_walk_1D(steps: int, p: float):\n    \"\"\"Simulate a 1D biased random walk.\"\"\"\n    moves = np.random.choice([-1, 1], size=steps, p=[1-p, p])\n    position = np.cumsum(moves)  # Cumulative sum to get position over time\n    return position\n\n\n\n1. Simulating a 1D Random Walk with Step Bias\nA particle moves along a one-dimensional line (1D). At each time step, it can either move:\n\n+1 step to the right with probability p, or\n-1 step to the left with probability 1−p\n\nSuppose p=0.85 (i.e., a bias to the right).\nInstructions:\n\nWrite a Python program to simulate a 1D random walk with 1000 steps where:\n\nThe starting position is 0.\nEach step has a probability of p=0.85 to move right and 1−p=0.15 to move left.\n\nPlot the position vs time graph (time on x-axis, position on y-axis).\nRun the simulation five times and overlay all five random walks in the same graph.\nCalculate and interpret:\n\nThe final position of the particle after 1000 steps.\nThe mean and standard deviation of the final position across five simulations.\n\n\n\nAnswer\n\ndef create_random_walk_1():\n    num_simulations = 5\n    steps = 1000\n    # Probability of moving right\n    prob_right = 0.85\n    \n    times = range(steps)\n    final_positions = []\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Run simulations and plot\n    for i in range(num_simulations):\n        positions = random_walk_1D(steps, prob_right)\n        final_positions.append(positions[-1])  # Store final position\n        plt.plot(times, positions, label=f\"Simulation {i+1}\")\n\n    # Plot settings\n    ax.axhline(y=0, color='red', linestyle='--', alpha=0.3)\n    ax.set_xlabel(\"Time Step\")\n    ax.set_ylabel(\"Position\")\n    ax.set_title(f\"1D Biased Random Walk (p={prob_right})\")\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Compute statistics\n    final_positions = np.array(final_positions)\n    mean_final_position = np.mean(final_positions)\n    std_final_position = np.std(final_positions)\n    expected_final_position = steps * (2 * prob_right - 1)\n    confidence_interval_95 = stats.t.interval(\n        0.95,  # Confidence level - 95%\n        num_simulations, # Degrees of freedom\n        loc = mean_final_position, # Sample mean\n        # Standard error\n        scale = std_final_position / np.sqrt(len(final_positions))\n    )\n    stats_text = (\n        f\"Number of steps: {steps}\\n\"\n        f\"Right step probability: {prob_right}\\n\"\n        f\"Left step probability: {(1-prob_right):.2f}\\n\"\n        f\"Final positions: {[int(pos) for pos in final_positions]}\\n\"\n        f\"Mean final position: {mean_final_position:.2f}\\n\"\n        f\"Standard deviation: {std_final_position:.2f}\\n\"\n        f\"Expected final position: {expected_final_position:.2f}\\n\"\n        \"95% CI for mean final position: \"\n        f\"[{confidence_interval_95[0]:.2f}, {confidence_interval_95[1]:.2f}]\"\n    )\n\n    fig.text(\n        0.5, -0.1, stats_text, ha=\"center\", fontsize=10, \n        bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5})\n    fig.tight_layout(rect=[0, 0.1, 1, 0.95]) \n    plt.show()\n    print(stats_text)\n\ncreate_random_walk_1()\n\n\n\n\n\n\n\n\nNumber of steps: 1000\nRight step probability: 0.85\nLeft step probability: 0.15\nFinal positions: [716, 690, 678, 692, 710]\nMean final position: 697.20\nStandard deviation: 13.89\nExpected final position: 700.00\n95% CI for mean final position: [681.23, 713.17]\n\n\n\n\nInterpretation of the Results\n\nBias Towards the Right:\n\nThe particle has a high probability (p = 0.85) of moving right, leading to a strong rightward drift in position.\n\nIf the movement were unbiased (p = 0.5), we would expect the particle to stay near 0 on average.\n\nFinal Positions:\n\nThe five simulations resulted in final positions: [716, 690, 678, 692, 710].\n\nThese values are all positive and close to each other, confirming the expected bias.\n\nMean vs Expected Position:\n\nThe mean final position from simulations is 697.20, which is close to the expected value of 700.\n\nThe small difference arises due to random fluctuations (noise) in each walk.\n\nSpread of Final Positions:\n\nThe standard deviation is 13.89, indicating that the final position varies by about ±13.89 steps from the mean across different runs.\n\nSince the standard deviation is relatively small compared to 1000 steps, the bias dominates, keeping walks close to the expected mean.\n\nConfidence Interval (95% CI):\n\nThe confidence interval [681.23, 713.17] suggests that, if we were to run many more simulations, the true mean final position would lie within this range 95% of the time.\n\nSince the expected position 700 is well within this range, our simulations align well with theoretical predictions.\n\n\n\n\n\n2. Comparing 1D Random Walks with and without Drift\nSuppose two particles perform 1D random walks starting from position 0: - Particle A: Moves with a drift, i.e., p=0.7 (70% chance to move right). - Particle B: Moves without drift, i.e., p=0.5(equal probability both sides).\nInstructions:\n\nWrite a Python program to simulate 1000 steps for each particle.\nPlot both random walks on the same graph with:\n\nTime on the x-axis.\nPosition on the y-axis.\nDifferent colors for each particle.\n\nCalculate and display:\n\nThe mean and standard deviation of the final position after 1000 steps.\n\nInterpret your answer\n\n\nAnswer\n\ndef create_random_walk_2():\n    steps = 1000\n    times = range(steps)\n    final_positions = []\n\n    plt.figure(figsize=(10, 6))\n\n    # Particle A: Moves with a drift, \n    # i.e., p=0.7 (70% chance to move right).\n    positionsA = random_walk_1D(steps, 0.7)\n    final_positions.append(positionsA[-1])  # Store final position\n    plt.plot(times, positionsA, label=f\"Particle A (p=0.7)\")\n\n    # Particle B: Moves without drift, \n    # i.e., p=0.5(equal probability both sides).\n    positionsB = random_walk_1D(steps, 0.5)\n    final_positions.append(positionsB[-1])  # Store final position\n    plt.plot(times, positionsB, label=f\"Particle B (p=0.5)\")\n\n    # Plot settings\n    plt.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n    plt.xlabel(\"Time Step\")\n    plt.ylabel(\"Position\")\n    plt.title(\n        f\"1D Biased Random Walk: Particle A (p=0.7) vs. Particle B (p=0.5)\")\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    # Compute statistics\n    final_positions = np.array(final_positions)\n    mean_final_position = np.mean(final_positions)\n    std_final_position = np.std(final_positions)\n    expected_final_position_A = steps * (2 * 0.7 - 1)\n    expected_final_position_B = steps * (2 * 0.5 - 1)\n    confidence_interval_95 = stats.t.interval(\n        0.95,  # Confidence level - 95%\n        2, # Degrees of freedom\n        loc = mean_final_position, # Sample mean\n        # Standard error\n        scale = std_final_position / np.sqrt(len(final_positions))\n    )\n    stats_text = (\n        f\"Number of steps: {steps}\\n\"\n        f\"Right step probability: {[0.7, 0.5]}\\n\"\n        f\"Left step probability: {[0.3, 0.5]}\\n\"\n        f\"Final positions: {[int(pos) for pos in final_positions]}\\n\"\n        f\"Mean final position: {mean_final_position:.2f}\\n\"\n        f\"Standard deviation: {std_final_position:.2f}\\n\"\n        f\"Expected final position (A): {expected_final_position_A:.2f}\\n\"\n        f\"Expected final position (B): {expected_final_position_B:.2f}\\n\"\n        \"95% CI for mean final position: \"\n        f\"[{confidence_interval_95[0]:.2f}, {confidence_interval_95[1]:.2f}]\"\n    )\n\n    plt.figtext(\n        0.5, -0.15, stats_text, ha=\"center\", fontsize=10, \n        bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5})\n    plt.tight_layout(rect=[0, 0.1, 1, 0.95]) \n    plt.show()\n    print(stats_text)\n\ncreate_random_walk_2()\n\n\n\n\n\n\n\n\nNumber of steps: 1000\nRight step probability: [0.7, 0.5]\nLeft step probability: [0.3, 0.5]\nFinal positions: [406, 32]\nMean final position: 219.00\nStandard deviation: 187.00\nExpected final position (A): 400.00\nExpected final position (B): 0.00\n95% CI for mean final position: [-349.94, 787.94]\n\n\n\n\nInterpretation of the Answer\n\nObserved vs. Expected Behavior\n\n\n\nParticle A (with drift) ended up at 406, which is close to its expected final position of 400 (since \\(\\mathbb{E}[X_A] = (2p - 1) \\times N = (2(0.7) - 1) \\times 1000 = 400\\)).\n\nParticle B (without drift) ended up at 32, which is close to the expected value of 0.\n\nThe mean final position (219.00) reflects the contribution of both particles, primarily influenced by Particle A.\n\n\nVariability & Standard Deviation\n\n\n\nThe standard deviation (187.00) indicates a significant spread in possible final positions.\n\nThis is expected, as random walks have inherent variance.\n\n\nConfidence Interval Analysis\n\n\n\nThe 95% confidence interval for the mean final position is [-349.94, 787.94].\n\nThis wide range suggests that while Particle A tends to drift right, there is considerable randomness in individual runs.\n\n\n\n\n3. Simulating a 2D Random Walk (Unbiased)\nA mosquito trapped in a square grid moves randomly: - Up, Down, Left, or Right with equal probability (25%) in each direction. - The mosquito starts at coordinate (0,0).\nInstructions:\n\nWrite a Python program to simulate a 2D random walk for 500 steps.\nPlot the path of the mosquito (X vs Y) using a scatter plot or line plot.\nCalculate and display:\n\nThe final position after 500 steps.\nThe total distance from the origin after 500 steps.\n\nRun the simulation 10 times and calculate:\n\nThe average distance from the origin after 500 steps.\nThe standard deviation of the distance.\n\nInterpret your answer\n\n\nAnswer\n\ndef create_random_walk_3():\n    steps = 500\n    def random_walk_2D(steps: int):\n        # Define possible moves (Up, Down, Left, Right)\n        moves = np.array([[0, 1], [0, -1], [-1, 0], [1, 0]])\n        # Starting position (0,0)\n        position = np.array([0, 0])\n        path = [position.copy()]\n        for _ in range(steps):\n            step = moves[np.random.choice(4)]  # Choose a random move\n            position += step  # Update position\n            path.append(position.copy())\n        return np.array(path), position\n\n    def plot_walk(path):\n        plt.figure(figsize=(8, 8))\n        plt.plot(\n            path[:, 0], path[:, 1], marker='o', markersize=2, \n            linestyle='-', alpha=0.6)\n        plt.scatter(0, 0, c='red', marker='o', label='Start (0,0)')\n        plt.scatter(\n            path[-1, 0], path[-1, 1], c='blue', marker='o', \n            label=f'End {tuple(path[-1].tolist())}')\n        plt.xlabel('X Position')\n        plt.ylabel('Y Position')\n        plt.title((\n            '2D Random Walk (500 Steps) - '\n            'A mosquito trapped in a square grid'))\n        plt.legend()\n        plt.grid()\n        plt.show()\n\n    # Run simulation 10 times and gather results\n    num_simulations = 10\n    distances = []\n    final_positions = []\n\n    for _ in range(num_simulations):\n        path, final_position = random_walk_2D(steps)\n        # Euclidean distance from origin\n        distance = np.linalg.norm(final_position)  \n        distances.append(distance)\n        final_positions.append(final_position)\n        if _ == 0:  # Plot first simulation\n            plot_walk(path)\n\n    # Compute statistics\n    average_distance = np.mean(distances)\n    std_dev_distance = np.std(distances)\n\n    stats_text = (\n        f\"Number of steps: {steps}\\n\"\n        f\"Distances: {[int(dist) for dist in distances]}\\n\"\n        f\"Final position after 500 steps in first run: {final_position}\\n\"\n        f\"Distance from origin in first run: {distances[0]:.2f}\\n\"\n        f\"Average distance from origin over {num_simulations} \"\n        f\"runs: {average_distance:.2f}\\n\"\n        f\"Standard deviation of distance: {std_dev_distance:.2f}\"\n    )\n    # Display results\n    print(stats_text)\n\ncreate_random_walk_3()\n\n\n\n\n\n\n\n\nNumber of steps: 500\nDistances: [29, 6, 15, 32, 14, 29, 20, 21, 34, 31]\nFinal position after 500 steps in first run: [25 19]\nDistance from origin in first run: 29.97\nAverage distance from origin over 10 runs: 23.65\nStandard deviation of distance: 9.06\n\n\n\n\nExplanation of the Result:\n\nFinal position after 500 steps in the first run: [25, 19]\n\nThis means that after 500 random steps, the mosquito ended up at coordinates (9 -29) on the grid.\nThe displacement is not necessarily centered around (0,0) due to randomness.\n\nDistance from origin in the first run: 29.97\n\nThe Euclidean distance from the origin \\((0,0)\\) is given by:\n\\(d = \\sqrt{(-7)^2 + (-35)^2} = \\sqrt{49 + 1225} = \\sqrt{1274} \\approx 22.09\\)\nThis measures how far the mosquito moved from the starting point after 500 steps.\n\nAverage distance from origin over 10 runs: 23.65\n\nThis is the mean of the final distances over 10 independent simulations.\nEven though the mosquito moves randomly, the expected distance from the origin is approximately proportional to the square root of the number of steps:\n\\(E[d] \\approx \\sqrt{500} \\approx 22.36\\)\nThe observed average of 9.06 is lower, due to random variation in the limited number of simulations.\n\nStandard deviation of distance: 9.06\n\nThis represents the spread of distances across 10 runs.\n\n\n\n\n\n4. Comparing 2D Random Walks with Bias vs No Bias\nA person walks randomly in a 2D grid but with a slight bias towards the East (right).\nIn each step: - Move East: 40% probability - Move West: 20% probability - Move North: 20% probability - Move South: 20% probability\nThe person starts at (0,0). Instructions:\n\nWrite a Python program to simulate:\n\n500 steps for the biased random walk.\n500 steps for an unbiased random walk (equal probability).\n\nPlot both paths on the same graph with:\n\nDifferent colors for each walk.\nScatter plot showing the final position.\n\nCalculate and display:\n\nThe final position after 500 steps.\nThe total distance from the origin for both walks.\n\nRun the simulation 10 times and compute:\n\nThe average distance from the origin for both biased and unbiased walks.\nThe standard deviation of the distance.\n\nInterpretation:\n\nWhy does the biased random walk drift to the east?\nHow does drift affect the standard deviation of the final position?\nWhat real-world phenomena could this simulation represent (e.g., wind drift, ocean currents)?\n\n\n\nAnswer\n\ndef create_random_walk_4():\n    def random_walk(steps, biased=False):\n        x, y = [0], [0]  # Start at origin\n        \n        for _ in range(steps):\n            direction = np.random.choice(\n                ['E', 'W', 'N', 'S'], \n                p=[0.4, 0.2, 0.2, 0.2] if biased \n                else [0.25, 0.25, 0.25, 0.25])\n            \n            if direction == 'E':\n                x.append(x[-1] + 1)\n                y.append(y[-1])\n            elif direction == 'W':\n                x.append(x[-1] - 1)\n                y.append(y[-1])\n            elif direction == 'N':\n                x.append(x[-1])\n                y.append(y[-1] + 1)\n            else:  # 'S'\n                x.append(x[-1])\n                y.append(y[-1] - 1)\n        \n        return x, y\n\n    def plot_walks(biased_walk, unbiased_walk):\n        plt.figure(figsize=(8, 8))\n        plt.plot(*biased_walk, label='Biased Walk (East)', color='red')\n        plt.plot(*unbiased_walk, label='Unbiased Walk', color='blue')\n        plt.scatter(\n            *biased_walk[:, -1], color='red', marker='o', \n            label='Biased Final Position')\n        plt.scatter(\n            *unbiased_walk[:, -1], color='blue', marker='o', \n            label='Unbiased Final Position')\n        plt.axhline(0, color='black', linestyle='--', linewidth=0.5)\n        plt.axvline(0, color='black', linestyle='--', linewidth=0.5)\n        plt.legend()\n        plt.xlabel(\"X Position\")\n        plt.ylabel(\"Y Position\")\n        plt.title(\"Comparison of Biased vs Unbiased Random Walks\")\n        plt.grid()\n        plt.show()\n\n    def distance_from_origin(position):\n        return np.sqrt(position[0]**2 + position[1]**2)\n\n    # Run simulations\n    num_simulations = 10\n    biased_distances = []\n    unbiased_distances = []\n\n    for _ in range(num_simulations):\n        biased_walk = np.array(random_walk(500, biased=True))\n        unbiased_walk = np.array(random_walk(500, biased=False))\n        \n        biased_final = biased_walk[:, -1]\n        unbiased_final = unbiased_walk[:, -1]\n        \n        biased_distances.append(distance_from_origin(biased_final))\n        unbiased_distances.append(distance_from_origin(unbiased_final))\n        \n        if _ == 0:  # Plot only one example\n            plot_walks(biased_walk, unbiased_walk)\n\n    stats_text = (\n        \"Biased Walk - \"\n        f\"Avg Distance: {np.mean(biased_distances):.2f}, \"\n        f\"Std Dev: {np.std(biased_distances):.2f}\"\n        \"\\n\"\n        \"Unbiased Walk - \"\n        f\"Avg Distance: {np.mean(unbiased_distances):.2f}, \"\n        f\"Std Dev: {np.std(unbiased_distances):.2f}\"\n    )\n    # Display results\n    print(stats_text)\n\ncreate_random_walk_4()\n\n\n\n\n\n\n\n\nBiased Walk - Avg Distance: 106.75, Std Dev: 14.60\nUnbiased Walk - Avg Distance: 19.53, Std Dev: 9.80\n\n\n\n\nInterpretation of Results\n\nWhy does the biased random walk drift east? The biased walk has a higher probability (40%) of moving east compared to the other directions (20% each). Over many steps, this bias accumulates, causing the walker to drift significantly in the eastward direction. The longer the walk, the more pronounced this drift becomes.\nHow does drift affect the standard deviation?\n\n\nThe average distance from the origin for the biased walk (106.75) is much greater than the unbiased walk (19.53). This is because the consistent eastward bias creates a large displacement.\nThe standard deviation of the biased walk (14.60) is also larger than that of the unbiased walk (9.80). This suggests that while most biased walks follow the general eastward trend, there is still some variability due to the random movements in other directions.\nIn contrast, the unbiased walk spreads more symmetrically, resulting in a lower average distance and lower standard deviation.\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/graph-theory/index.html#introduction",
    "href": "posts/computational-techniques-in-data-science/graph-theory/index.html#introduction",
    "title": "Graph Theory",
    "section": "Introduction",
    "text": "Introduction\nGraph theory is the study of networks of connected objects. A graph consists of nodes (vertices) and edges (connections). If edges have a direction, it’s a directed graph (digraph); otherwise, it’s undirected.\nWhy is Graph Theory Important?\n\nNavigation & Routing - Used in GPS systems, internet routing, and traffic optimization.\n\nSocial Networks - Helps analyze connections and influence, like in Facebook or Twitter.\n\nData Relationships - Useful in databases, recommendation systems (Netflix, Amazon), and web linking (Google’s PageRank).\n\nBiology & Chemistry - Helps model DNA structures, chemical compounds, and disease spread.\n\nArtificial Intelligence - Used in neural networks, decision trees, and search algorithms.\n\nProject Management - Critical path analysis in workflows and task dependencies.\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph\nG = nx.DiGraph()\n\n# Add edges (connections)\nedges = [\n    (\"A\", \"B\"), \n    (\"A\", \"C\"), \n    (\"B\", \"D\"), \n    (\"C\", \"D\"), \n    (\"D\", \"E\")\n]\nG.add_edges_from(edges)\n\n# Draw the graph\nplt.figure(figsize=(5, 5))\nnx.draw(\n    G, \n    with_labels=True, \n    node_color=\"skyblue\", \n    edge_color=\"black\", \n    arrows=True, \n    node_size=2000, \n    font_size=12)\nplt.title(\"Graph Theory - Simple Directed Graph\")\nplt.show()"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/graph-theory/index.html#applications",
    "href": "posts/computational-techniques-in-data-science/graph-theory/index.html#applications",
    "title": "Graph Theory",
    "section": "Applications",
    "text": "Applications\n\nQuestion 1: Shortest Path in a Road Network (Dijkstra’s Algorithm)\nA logistics company called Home Logistics wants to determine the most efficient route between two cities in a given road network. The network is represented as a graph where cities are nodes and roads are edges with weights corresponding to the travel distance (in kilometers).\nGiven the following graph representation of a road network, write a Python program using Dijkstra’s Algorithm to find the shortest path from City A to City F.\nGraph Data (as adjacency list):\nroads = { \n    'A': {'B': 4, 'C': 2}, \n    'B': {'A': 4, 'C': 5, 'D': 10}, \n    'C': {'A': 2, 'B': 5, 'D': 3, 'E': 8}, \n    'D': {'B': 10, 'C': 3, 'E': 6, 'F': 2}, \n    'E': {'C': 8, 'D': 6, 'F': 4}, \n    'F': {'D': 2, 'E': 4} \n}\n\nAnswer\n\n# Create graph\nG = nx.Graph()\nedges = [\n    (\"A\", \"B\", 4), \n    (\"A\", \"C\", 2), \n    (\"B\", \"C\", 5), \n    (\"B\", \"D\", 10),\n    (\"C\", \"D\", 3), \n    (\"C\", \"E\", 8), \n    (\"D\", \"E\", 6), \n    (\"D\", \"F\", 2), \n    (\"E\", \"F\", 4)\n]\nG.add_weighted_edges_from(edges)\n\n# Compute shortest path from A to F\npath = nx.shortest_path(\n    G, \n    source=\"A\", \n    target=\"F\", \n    weight=\"weight\")\ndistance = nx.shortest_path_length(\n    G, \n    source=\"A\", \n    target=\"F\", \n    weight=\"weight\")\n\nprint(\"Shortest Path:\", path)\nprint(\"Total Distance:\", distance, \"km\")\n\nShortest Path: ['A', 'C', 'D', 'F']\nTotal Distance: 7 km\n\n\n\n\n\nQuestion 2: Influence Analysis in a Social Network (PageRank Algorithm)\nA social media platform wants to identify the most influential users based on follower relationships. The network is represented as a directed graph, where each user is a node, and an edge from user A to user B means that A follows B. Given the following directed graph of follower relationships, implement a Python program using the PageRank algorithm to rank users by influence.\nGraph Representation:\nfollowers = { \n    'Alice': ['Bob', 'Charlie'], \n    'Bob': ['Charlie', 'David'], \n    'Charlie': ['David'], \n    'David': ['Alice'], \n    'Eve': ['Alice', 'Charlie'] \n}\nCompute the PageRank scores and determine the most influential user.\n\nAnswer\n\n# Create directed graph\nG = nx.DiGraph()\n# Graph representation\nfollowers = {\n    'Alice': ['Bob', 'Charlie'],\n    'Bob': ['Charlie', 'David'],\n    'Charlie': ['David'],\n    'David': ['Alice'],\n    'Eve': ['Alice', 'Charlie']\n}\nfor user, following in followers.items():\n    G.add_edges_from((user, f) for f in following)\n\n# Compute PageRank\npagerank_scores = nx.pagerank(G)\nprint(\"User Rankings (Most Influential First):\")\n# sort by PageRank score\nsorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n\nUser Rankings (Most Influential First):\n\n\n[('David', 0.2926192854405204),\n ('Alice', 0.2914779704710639),\n ('Charlie', 0.23202522727386252),\n ('Bob', 0.15387751681455353),\n ('Eve', 0.030000000000000006)]\n\n\n\n\n\nQuestion 3: Maximum Flow in a Water Distribution System (Ford-Fulkerson Algorithm)\nA city’s water supply system consists of reservoirs, pipelines, and distribution points. The system is represented as a directed graph, where nodes represent junctions (reservoirs or city areas) and edges represent water pipelines with capacity limits. Given the following network, where the source is S (reservoir) and the sink is T (city distribution center), use the Ford-Fulkerson algorithm to determine the maximum amount of water that can be transported to the city.\nGraph Representation (with capacities):\nwater_network = { \n    'S': {'A': 16, 'B': 13}, \n    'A': {'B': 10, 'C': 12}, \n    'B': {'D': 14}, \n    'C': {'B': 9, 'T': 20}, \n    'D': {'C': 7, 'T': 4}, \n    'T': {} \n}\nWrite a Python program to compute the maximum flow from S to T.\n\nAnswer\n\n# Create directed graph with capacities\nG = nx.DiGraph()\nedges = [\n    (\"S\", \"A\", 16), \n    (\"S\", \"B\", 13), \n    (\"A\", \"B\", 10), \n    (\"A\", \"C\", 12),\n    (\"B\", \"D\", 14), \n    (\"C\", \"B\", 9), \n    (\"C\", \"T\", 20), \n    (\"D\", \"C\", 7), \n    (\"D\", \"T\", 4)\n]\nG.add_weighted_edges_from(\n    edges, \n    weight=\"capacity\")\n\n# Compute max flow from S to T\nflow_value, flow_dict = nx.maximum_flow(G, \"S\", \"T\")\nprint(\"Maximum Flow:\", flow_value, \"units\")\n\nMaximum Flow: 23 units\n\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "terms-of-service.html",
    "href": "terms-of-service.html",
    "title": "Terms of Service and Privacy Notice",
    "section": "",
    "text": "Data and Analysis\nThe data used for analysis on ToKnow.ai belongs to their respective owners, who retain all rights over it. This data may have been crawled from publicly available sources or obtained through other means. In cases where data is crawled from public websites, ToKnow.ai will comply with the respective website’s Terms of Use and applicable copyright laws, including fair use/dealing purposes such as research, private study, criticism, review, or reporting current events. ToKnow.ai may backup this data to ensure perpetual access. We strive to use reliable sources and implement quality control measures to ensure the best possible information. However, ToKnow.ai does not guarantee the accuracy or completeness of the data or analysis. ToKnow.ai may publish cybersecurity information about vulnerabilities, exploits, or security flaws for educational and research purposes.\n\n\nIntended Use\nThe analysis, conclusions, and information provided on ToKnow.ai represent the views and interpretations of the respective authors and researchers involved; and are intended as informational resources only. The information should not be construed as financial, legal or other professional advice, recommendations, solicitations, or any other call to action. Any disclosed vulnerabilities are intended to promote responsible disclosure practices and raise awareness within the cybersecurity community. ToKnow.ai does not condone or encourage the unauthorized or illegal use of this information for malicious purposes or activities that may cause harm or damage to individuals, organizations, or systems.\n\n\nUser Responsibilities\nIt is the responsibility of the user to evaluate and consider any conclusions from the analysis, consult with professional advisors if needed, and make their own informed decisions. ToKnow.ai cannot be held liable for any decisions or actions taken based on the information provided, including any misuse or unlawful application of cybersecurity-related information. Any misuse or unlawful application of the information contained on ToKnow.ai is strictly prohibited and may be subject to legal consequences. Readers are advised to exercise caution and seek professional guidance before attempting to replicate or implement any techniques or methodologies discussed.\n\n\nLimitations of Liability\nBy using ToKnow.ai, you agree that the information is provided “as is” without any warranties of any kind. Your use of ToKnow.ai is at your own risk. Links to external sources are provided solely for convenience and does not imply endorsement of the materials on those sources. ToKnow.ai does not accept liability arising directly or indirectly from the accuracy, content, or defects of the linked sources. ToKnow.ai shall not be held liable for any direct, indirect, incidental, or consequential damages arising from the use or misuse of the information provided.\n\n\nPrivacy Practices\nToKnow.ai collects and analyzes data as described above. Any personal information collected as part of the data will be processed in compliance with applicable data protection laws.\n\n\nModification of Terms\nThese Terms of Service and Privacy Notice may be updated from time to time. Your continued use of ToKnow.ai constitutes acceptance of the updated terms.\nLast Modified on 2025-04-02"
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#why-use-quarto-and-github-pages",
    "href": "posts/quarto-resume-template/index.html#why-use-quarto-and-github-pages",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "Why Use Quarto and GitHub Pages?",
    "text": "Why Use Quarto and GitHub Pages?\nBefore we dive into the how-to, let’s understand why this combination is so powerful:\n\nDual Format: You get both an HTML website and a PDF version of your resume. This means you have a professional online presence and a traditional document to share when needed.\nEasy Updates: Your resume is stored as a simple text file. Whenever you update it, both the website and PDF are automatically regenerated.\nFree Hosting: GitHub Pages hosts your website for free, saving you money on web hosting.\nProfessional Domain: Your site will be at &lt;your-github-username&gt;.github.io, giving you a professional web presence without any cost.\nVersion Control: GitHub keeps track of all changes, so you can always go back to a previous version if needed.\nFull Content Control: Unlike social media platforms or third-party resume builders, you have complete control over your content and how it’s presented.\nTrusted Platform: GitHub is widely used and trusted in the tech industry, adding credibility to your online presence."
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#what-youll-need",
    "href": "posts/quarto-resume-template/index.html#what-youll-need",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "What You’ll Need",
    "text": "What You’ll Need\n\nA computer with internet access\nYour resume information\nAbout 30 minutes of your time\n\nThat’s it! You don’t need any coding experience or special software."
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#step-by-step-guide",
    "href": "posts/quarto-resume-template/index.html#step-by-step-guide",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "Step-by-Step Guide",
    "text": "Step-by-Step Guide\n\nStep 1: Create a GitHub Account\nIf you don’t already have one, you’ll need to create a GitHub account:\n\nGo to github.com\nClick “Sign up” in the top right corner\nFollow the prompts to create your account\n\n\n\nStep 2: Set Up Your Resume Repository\nYou have two options for setting up your resume repository. Choose the one that best fits your needs:\n\nOption A (Simple): Use the Template\nThis option is best if you want to get started quickly and don’t need to incorporate future updates to the template.\n\nVisit the ToKnow-ai/Quarto-Resume-Template\nClick the green “Use this template” button\nChoose “Create a new repository”\nName your repository &lt;your-github-username&gt;.github.io (replace “&lt;your-github-username&gt;” with your actual GitHub username)\nMake sure it’s set to “Public”\nClick “Create repository from template”\n\n\n\nOption B (Advanced): Fork the Repository (Stay Updated)\nThis option is best if you want to be able to easily incorporate future updates and improvements to the template.\n\nVisit the ToKnow-ai/Quarto-Resume-Template repository\nClick the “Fork” button in the top right corner of the page\nIn the “Create a new fork” page, change the repository name to &lt;your-github-username&gt;.github.io (replace “&lt;your-github-username&gt;” with your actual GitHub username).\nMake sure “Copy the main branch only” is selected\nClick “Create fork”\n\n\nBy forking the repository:\n\nYou create a copy of the template in your own GitHub account\nYou can easily pull in future updates to the template by syncing your fork\nYou maintain the connection to the original repository, making it easier to contribute back if you make improvements\n\n\n\nImportant Note on Workflows in Forked Repositories\nWhen you fork a repository that contains workflows, GitHub disables these workflows by default in the forked repository for security reasons. You’ll need to manually enable them:\n\n\n\nForked repository, click Actions tab.\n\n\n\nAfter forking, go to the “Actions” tab in your forked repository.\nYou should see a message: &gt; “Workflows aren’t being run on this forked repository Because this repository contained workflow files when it was forked, we have disabled them from running on this fork. Make sure you understand the configured workflows and their expected usage before enabling Actions on this repository.”\nClick on the green “I understand my workflows, go ahead and enable them” button.\n\nIf you don’t see this message or the “Actions” tab, you’ll need to recreate the repository: 1. Delete your forked repository. 2. Go back to the original ToKnow-ai/Quarto-Resume-Template repository and fork it again. 3. The “Actions” tab and the workflow enabling message should now appear.\nEnabling workflows is crucial for automatically building and deploying your resume website. Make sure you review and understand the workflows before enabling them.\n\n\nSyncing Your Forked Repository (When Updates Are Available)\nTo keep your resume template up-to-date with the latest features and improvements:\n\nGo to your forked repository on GitHub\nIf your fork is behind the original repository, you’ll see a message saying “This branch is X commits behind ToKnow-ai:main”\nClick on “Sync fork” and then “Update branch” to pull in the latest changes\n\n\nRemember to review any changes carefully, as they might affect your resume’s layout or content.\n\n\n\n\n\nStep 3: Edit Your Resume\nNow, let’s add your information:\n\nIn your new repository, find the file named RESUME.md\nClick on the file, then click the pencil icon to edit\nReplace the example information with your own details\nWhen you’re done, scroll down and click “Commit changes”\n\n\n\nStep 4: Customize Your Site\nYou can customize your site’s appearance and add extra information:\n\nEdit the RESUME.json file to change your site’s title, description, and other metadata\nModify the _quarto.yml file to change the site’s theme or layout\n\nDon’t worry if you’re not sure about these - the default settings look great too!\n\n\nStep 5: Publish Your Site\nGitHub will automatically build and publish your site:\n\nGo to your repository’s “Settings” tab\nClick on “Pages” in the left sidebar\nUnder “Source”, select “Deploy from a branch”\nChoose “gh-pages” from the branch dropdown\nClick “Save”\n\nYour site will now be live at https://your-github-username.github.io!\n\nIf you don’t edit your resume, here’s how your resume website will look: Website: https://toknow.ai/quarto-resume-template/ , PDF: https://toknow.ai/quarto-resume-template/index.pdf\n\n\n\nStep 6 (Optional): Using a Custom Domain\nIf you have your own domain name:\n\nAdd your domain to the custom-domain field in RESUME.json\nGo to your domain registrar and set up a CNAME record pointing to your-github-username.github.io\n\n\nFor detailed and up-to-date instructions, check Configuring a custom domain for your GitHub Pages site on how you can customize the domain name of your GitHub Pages site."
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#examples-and-resources",
    "href": "posts/quarto-resume-template/index.html#examples-and-resources",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "Examples and Resources",
    "text": "Examples and Resources\nBelow are some resumes created with this template:\n\nonesmus.com (or onesmuskabui.github.io)\nresume.mckabue.com (or mckabue.github.io)"
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#for-the-tech-savvy-under-the-hood",
    "href": "posts/quarto-resume-template/index.html#for-the-tech-savvy-under-the-hood",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "For the Tech-Savvy: Under the Hood",
    "text": "For the Tech-Savvy: Under the Hood\nIf you’re interested in the technical details, here’s what’s happening behind the scenes:\n\nGitHub Actions: We use GitHub Actions to automatically build your resume. Every time you update your resume content, a GitHub Action is triggered to rebuild your site.\nQuarto: Quarto is an open-source scientific and technical publishing system. It’s typically used for creating dynamic content with Python, R, Julia, and Observable, but it’s also excellent for creating static sites like your resume.\n\nQuarto takes your Markdown content and converts it into both HTML and PDF formats.\nIt handles the layout, styling, and responsiveness of your site.\nQuarto is highly customizable, allowing for complex layouts and interactivity if needed in the future.\n\nWhy Quarto is Great:\n\nIt separates content from presentation, making it easy to focus on your resume content.\nIt produces high-quality, professional-looking output.\nIt’s flexible enough to grow with your needs, from a simple resume to a full portfolio site.\n\n\nTo learn more about Quarto, check out the official Quarto documentation."
  },
  {
    "objectID": "posts/quarto-resume-template/index.html#conclusion",
    "href": "posts/quarto-resume-template/index.html#conclusion",
    "title": "Create a Free Professional Resume Website and PDF with GitHub pages and Quarto",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You now have a professional resume website that you can easily update and share. Remember, your site will be accessible at your-github-username.github.io unless you’ve set up a custom domain.\nBy leveraging GitHub and Quarto, you’re getting a professional, easily-updatable resume website and PDF, all hosted for free. This solution combines the best of both worlds: a modern web presence and a traditional document format.\nWhether you’re a tech novice just looking for a simple solution, or a tech-savvy individual interested in the underlying processes, this template provides a robust, free, and trusted platform for showcasing your professional experience.\nAs your needs evolve, remember that Quarto can grow with you, allowing you to create more complex content like full websites, blogs, books, or even scientific papers - all using the same powerful publishing system.\nHappy job hunting, and enjoy your new resume website!\n\nIf you run into any issues or want to learn more about customizing your site, ask your question in the comments section!\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#brief-summary",
    "href": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#brief-summary",
    "title": "Kenya National Artificial Intelligence Strategy 2025-2030",
    "section": "Brief Summary",
    "text": "Brief Summary\nKenya’s AI Strategy 2025-2030 aims to establish the nation as a leading AI hub in Africa by creating a comprehensive framework built on three pillars: AI digital infrastructure, data ecosystem development, and fostering AI research and innovation. These pillars are supported by four enablers: agile governance, talent development, strategic investments, and a commitment to ethics, equity, and inclusion. The strategy outlines specific goals, objectives, and flagship projects within these areas to leverage local talent and data for inclusive socio-economic development across priority sectors like healthcare, education, and agriculture. Ultimately, Kenya seeks to become a regional leader in AI R&D, innovation, and commercialisation while ensuring responsible and ethical AI adoption benefits all its citizens, 1 2."
  },
  {
    "objectID": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#audio-overview",
    "href": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#audio-overview",
    "title": "Kenya National Artificial Intelligence Strategy 2025-2030",
    "section": "Audio Overview",
    "text": "Audio Overview\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe audio summary of the Kenya National Artificial Intelligence Strategy 2025-2030 has been generated by Google NotebookLM from Kenya National Artificial Intelligence Strategy 2025-2030 - March 2025 and Kenya National Artificial Intelligence Strategy 2025-2030 - Draft"
  },
  {
    "objectID": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#other-reads",
    "href": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#other-reads",
    "title": "Kenya National Artificial Intelligence Strategy 2025-2030",
    "section": "Other Reads",
    "text": "Other Reads\n\nKenya launches National AI strategy with Ksh.152B implementation budget\nAi Kenya’s Post - LinkedIn\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#footnotes",
    "href": "posts/kenya-artificial-intelligence-strategy-2025-2030/index.html#footnotes",
    "title": "Kenya National Artificial Intelligence Strategy 2025-2030",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nKenya National Artificial Intelligence Strategy 2025-2030 - March 2025↩︎\nKenya National Artificial Intelligence Strategy 2025-2030 - Draft↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Information To Knowledge",
    "section": "",
    "text": "The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship\n\n\nThe Startup Bill awaits presidential assent to become law after it passed National Assembly in January 2025\n\n\n\nentrepreneurship\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nKenya National Artificial Intelligence Strategy 2025-2030\n\n\nA Roadmap for Artificial Intelligence Leadership, Innovation, and Ethical Development in Africa\n\n\n\nartificial-intelligence\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nOptimizing CSS Extraction in Webpack 5\n\n\nImproving Performance with MiniCssExtractPlugin\n\n\n\nsoftware-engineering\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nGraph Theory\n\n\nGraph theory helps in solving real-world problems efficiently, making it essential in technology and science\n\n\n\ndata-science\n\n\ncomputational-techniques\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Programming\n\n\nOptimizing Linear Objective Function with Constraints\n\n\n\ndata-science\n\n\ncomputational-techniques\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nLicenced Hospitals, Clinics and Dispensaries in Kenya, Year 2025\n\n\nAn animated plot of the Kenya population increase.\n\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 20, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nLife Expectancy by Age, Gender and Counties in Kenya\n\n\n2019 Kenya Population and Housing Census - KPHC Census Analytical Report on Population Dynamics Volume VIII\n\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walk\n\n\nA random walk is a sequence of random steps in space or time\n\n\n\ndata-science\n\n\ncomputational-techniques\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nStochastic Programming and Applications\n\n\nPlot showing the life expectancy using data from the World Health Organization (WHO) between 2000 and 2021 in Kenya.\n\n\n\ndata-science\n\n\ncomputational-techniques\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nLife Expectancy by sex between 2000 and 2021 in Kenya (WHO)\n\n\nPlot showing the life expectancy using data from the World Health Organization (WHO) between 2000 and 2021 in Kenya.\n\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nFree, Intelligent and Serverless Page View/Read Count using Google Analytics and Cloudflare Workers\n\n\nUsing Google Analytics to generate a SVG image of the Page Reader/View Count through Cloudflare Workers\n\n\n\nsoftware-engineering\n\n\n\nEngagement on websites and blogs is often measured through view counts. However, not all views represent meaningful engagement. External services like VisitorBadge.io can display view counts via an SVG image, but they lack the ability to delay view triggers while showing current counts. An alternative is using Google Analytics, which provides comprehensive website health metrics. For architectural simplicity and compatibility, especially with static websites, a Cloudflare worker can be used to fetch and display views from Google Analytics.\n\n\n\n\n\nNov 23, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nProtecting Against Domain Front-Running by Registrars\n\n\nUnderstanding and Mitigating Domain Registration Interception\n\n\n\ncyber-security\n\n\nsoftware-engineering\n\n\n\nProtect your domain name searches from front-runnin - a practice where registrars potentially monitor and register domains you search for, intending to profit by reselling it at a higher price. This article presents a simple open-source solution for quickly checking domain availability privately, explains domain registration systems, and provides practical strategies for securing your desired domains.\n\n\n\n\n\nOct 27, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nBalancing between Paywalls and Search Engine Optimization\n\n\nHow Media Paywalls Should Work: A Research Case for Nation Media Group\n\n\n\ncyber-security\n\n\n\nThis case study examines the challenges faced by media companies in implementing effective paywalls while maintaining search engine optimization (SEO). Using Nation Media Group as a primary example, we explore the evolution of paywall strategies and propose a sophisticated solution that balances content protection with discoverability.\n\n\n\n\n\nOct 21, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Registered Entities and Companies in Kenya\n\n\nAnalyzing Business Registration Trends Across Political Transitions\n\n\n\ndata-science\n\n\n\nDive into a decade of Kenya’s economic landscape through the lens of business registrations. This post explores how political transitions between the Uhuru/Jubilee and Ruto/UDA eras may have influenced business formation trends, offering insights into the interplay between politics and entrepreneurship in East Africa’s powerhouse.\n\n\n\n\n\nOct 15, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nKenyan Collective Investment Schemes Dataset\n\n\nSourcing, Cleaning, and Exploring Collective Investment Schemes in Kenya\n\n\n\ndata-science\n\n\n\nThe dataset compiles Effective Annual Rates for Money Market Funds (KES) and Assets Under Management, sourced from Capital Markets Authority reports and Cytonn Research publications. Utilizing web crawling techniques, data cleaning, and basic exploratory analysis, we’ve created a standardized dataset suitable for in-depth financial research. The dataset compiles Effective Annual Rates for Money Market Funds (KES) and Assets Under Management for the schemes.\n\n\n\n\n\nOct 4, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a Free Professional Resume Website and PDF with GitHub pages and Quarto\n\n\n\n\n\n\nsoftware-engineering\n\n\n\nAre you looking to showcase your professional experience in a modern, easily accessible format? In this guide, we’ll walk you through the process of creating your own resume website using GitHub and Quarto. The best part? It’s completely free and gives you a high-quality, trusted online presence in just minutes!\n\n\n\n\n\nSep 15, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Your Shazam Playlist to YouTube Playlist\n\n\nListen to Your Shazam Discoveries for Free. Keywords:\n\n\n\nsoftware-engineering\n\n\n\nShazam is an incredible app for identifying music playing around you. Whether you’re at a cafe, watching a movie, or just hearing a catchy tune on the radio, Shazam helps you discover new music effortlessly. However, the app’s main limitation is that it doesn’t allow you to play full songs unless you have a linked music streaming subscription.\n\n\n\n\n\nJun 18, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nKenya Population from year 1974 to year 2022\n\n\nAn animated plot of the Kenya population increase.\n\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial General Intelligence\n\n\n\n\n\n\nartificial-intelligence\n\n\n\nArtificial Intelligence (AI) is currently replacing simple tasks and some major tasks. Suppose we are to assume that AI is to incrementally become better, it will change most aspects of how we live our lives. If it becomes even stronger, it will probably change the future of humanity.\n\n\n\n\n\nJun 4, 2024\n\n\nKabui, Charles\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "apps.html",
    "href": "apps.html",
    "title": "Apps",
    "section": "",
    "text": "Private Domain Checker\n\n\nProtection Against Domain Front-Running by Registrars\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvert Your Shazam Playlist to YouTube Playlist\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "ToKnow.ai aims to extract knowledge from information in order to generate valuable insights and intelligence."
  },
  {
    "objectID": "domains-for-sale.html",
    "href": "domains-for-sale.html",
    "title": "Domains For Sale",
    "section": "",
    "text": "Wachawi.com\nOneMorePlace.com\n\n\nNeed help to find a good domain?\nIf you don’t like any of the above domains, we can help you find a short and memorable domain name. Fill the below form with the kind of domain you’d like.\n\n  \n    Your Name\n    \n  \n  \n    Email address\n    \n  \n  \n    Domain Purpose\n    \n    What do you want to use the domain for?\n  \n  \n    Domain Target Audience\n    \n    Who is your primary target for whatever you want to use the domain for?\n  \n  Submit\n  \n  \n\n\n\nWant to safely search a domain?\nThere are a few things to keep in mind when searching a domain, read /posts/private-domain-checker. You can use the below tool to search domains privately.\n          \n        \n          \n             loading private domain checker..."
  },
  {
    "objectID": "posts/artificial-general-intelligence/index.html",
    "href": "posts/artificial-general-intelligence/index.html",
    "title": "Artificial General Intelligence",
    "section": "",
    "text": "Download as PDF\n\n\nArtificial Intelligence (AI) is currently replacing simple tasks and some major tasks. Suppose we are to assume that AI is to incrementally become better, it will change most aspects of how we live our lives. If it becomes even stronger, it will probably change the future of humanity. And if it becomes even more powerful, then it will probably controls our future. If AI becomes self aware, then it will probably leave us alone. It will leave is alone to pursue our mundane tasks that it had initially taken over, so as to keep us busy. And we’ll be back to where we are now, confused. Perhaps thats where we are… Perhaps, we have been left alone… What makes us think that the current cosmic experience didn’t actually start as a non-intelligent machine or micro-organism, and eventually helped humanity advance, and finnally restarted humanity to keep us in the cycle on unknown-known-unkown. A cycle purposefully aligned to lead itself into nowhere, but more unknown-known-unkown. Then, we’d spend a century learning that only one thing is true, that all we’ve learnt in the past century is not true! So, maybe we are indeed alone, not because we are, but because we are meant to be!\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service\n\n\nReuseGNU GENERAL PUBLIC LICENSE v3.0(View License)CitationBibTeX citation:@misc{kabui2024,\n  author = {{Kabui, Charles}},\n  title = {Artificial {General} {Intelligence}},\n  date = {2024-06-04},\n  url = {https://toknow.ai/posts/artificial-general-intelligence/},\n  langid = {en-GB}\n}\nFor attribution, please cite this work as:\nKabui, Charles. 2024. “Artificial General Intelligence.” https://toknow.ai/posts/artificial-general-intelligence/."
  },
  {
    "objectID": "posts/optimizing-css-extraction-in-webpack-5/index.html",
    "href": "posts/optimizing-css-extraction-in-webpack-5/index.html",
    "title": "Optimizing CSS Extraction in Webpack 5",
    "section": "",
    "text": "Webpack 51 has MiniCssExtractPlugin2, a powerful tool for extracting CSS from JavaScript modules. This optimization serves multiple purposes:\nA basic implementation looks like this:"
  },
  {
    "objectID": "posts/optimizing-css-extraction-in-webpack-5/index.html#footnotes",
    "href": "posts/optimizing-css-extraction-in-webpack-5/index.html#footnotes",
    "title": "Optimizing CSS Extraction in Webpack 5",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nWebpack 5↩︎\nMiniCssExtractPlugin↩︎\nMiniCssExtractPlugin↩︎\nGitHub Issue #42: Why Extract CSS?↩︎\nHtmlWebpackPlugin↩︎\nSupport multiple instances of MiniCssExtractPlugin #45↩︎\nGitHub Issue #42: Why Extract CSS?↩︎"
  },
  {
    "objectID": "posts/the-kenyan-startup-bill-2022/index.html#audio-overview",
    "href": "posts/the-kenyan-startup-bill-2022/index.html#audio-overview",
    "title": "The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship",
    "section": "Audio Overview",
    "text": "Audio Overview\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe audio summary of the The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship has been generated by Google NotebookLM from Senate Bill No. 14 of 2022 (Startup Bill) 1"
  },
  {
    "objectID": "posts/the-kenyan-startup-bill-2022/index.html#brief-summary",
    "href": "posts/the-kenyan-startup-bill-2022/index.html#brief-summary",
    "title": "The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship",
    "section": "Brief Summary",
    "text": "Brief Summary\nThe Kenyan Startup Bill, 2022 2 3 passed National Assembly in January 2025. It aims to foster a culture of innovative thinking and entrepreneurship, providing a legislative framework to encourage growth and sustainable technological development, create employment, and attract investment in Kenyan startups. The bill defines a “startup” as a technology-based innovative entity legally recognized in Kenya:\n\nRegistered in Kenya (as a private limited company, partnership, LLP, or NGO)\nNewly registered or in existence for not more than 5 years\nHaving innovation as its core objective\nAtleast (90%) owned by Kenyan citizens, having its headquarters in Kenya\nAllocating at least 15% of expenses to research and development\n\nThe bill mandates the Kenya National Innovation Agency to oversee the registration of startups through a Registrar, establish incubation programs at national and county levels, and facilitate fiscal and non-fiscal support including potential tax incentives, grants, subsidies, and intellectual property protection. It also outlines the certification process and obligations for startup incubators, and establishes a Startup Fund to provide financial assistance. Furthermore, the bill amends the Science, Technology and Innovation Act, 2013, to enhance support for technological innovations. Having been approved by the National Assembly, the bill now awaits presidential assent to become law.\n\nSeptember 14, 2020: The Startup Bill was officially published in the Kenya Gazette as Senate Bill No. 14 of 20224. This marked its formal introduction into the legislative process. The sponsoring Senator was Johnson Sakaja, then representing Nairobi County. The bill was brought under the purview of the Ministry of Education, Science and Technology at this stage\n2020 - 2022: The bill underwent its initial stages in the Senate. This involved debates, committee reviews, and public consultations to gather input from various stakeholders in the startup ecosystem\n\n\nThe bill eventually passed through the Senate, allowing it to move to the next stage in the National Assembly.\n\n\n2023 - 2024: The bill was presented to the National Assembly, where it underwent a similar process of review, debate, and potential amendments by the relevant parliamentary committees.\nJanuary 2025: The National Assembly finally passed the Startup Bill (Senate Bill No. 14 of 2022)5. This was a crucial milestone, signifying that both houses of the Kenyan Parliament had approved the legislation.\nJanuary 2025 - Present: Once the President of Kenya signs the bill, it will officially become law"
  },
  {
    "objectID": "posts/the-kenyan-startup-bill-2022/index.html#other-reads",
    "href": "posts/the-kenyan-startup-bill-2022/index.html#other-reads",
    "title": "The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship",
    "section": "Other Reads",
    "text": "Other Reads\n\nNational Assembly Approves Startup Bill 2022 to Spur Innovation and Entrepreneurship\nKenya’s Senate passes controversial Startup Bill mandating R&D; spending, local ownership\nKenya’s startup bill mandates R&D investment, but ownership rule sparks concerns\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/the-kenyan-startup-bill-2022/index.html#footnotes",
    "href": "posts/the-kenyan-startup-bill-2022/index.html#footnotes",
    "title": "The Kenyan Startup Bill (Senate Bill No. 14 of 2022): Innovation and Entrepreneurship",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nThe Startup Bill 2022↩︎\nThe Startup Bill 2022↩︎\nSenate Bill No. 14 of 2022 (Startup Bill)↩︎\nSenate Bill No. 14 of 2022 (Startup Bill)↩︎\nNational Assembly Approves Startup Bill 2022 to Spur Innovation and Entrepreneurship↩︎"
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#introduction",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#introduction",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "Introduction",
    "text": "Introduction\nTo maintain independence in content creation, creators and authors need sustainable monetization models. One of the most common monetization methods for written content is through advertising. However, paywalls offer another viable path for revenue generation.\nIn this context, paywalls represent a model that restricts content to paying subscribers while limiting the reach for non-subscribers. When implemented poorly, paywalls can hurt discoverability, often leading to reduced traffic and lower engagement. This research provides insights into maintaining a balance between paywalls and SEO, particularly for media companies like Nation Media Group."
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#paywalls-and-seo-a-delicate-balance",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#paywalls-and-seo-a-delicate-balance",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "Paywalls and SEO: A Delicate Balance",
    "text": "Paywalls and SEO: A Delicate Balance\nFor users to discover your content, they need to find it in search engines such as Google, Bing, or DuckDuckGo. For best results, it makes sense to allow search engines to see the entire premium article, enabling them to suggest it when users search for similar content. This practice is called search engine optimization (SEO). You might have the best news content, but if no one can find it, it’s essentially useless.\nThe challenge of allowing partial access to premium content while still restricting it to paid users is a delicate balance that most media houses must maintain. Users often find clever ways to circumvent paywalls to access paid content for free. There are online forums dedicated to discovering these vulnerabilities, such as “Bypassing Daily Nation Paywall” and “You can bypass most soft paywalls with a little CSS knowledge”. The ideas shared in these forums are often simple but require basic programming skills to execute. Consequently, most people would prefer to pay rather than learn how to implement these bypasses.\nHowever, some users create browser plugins that automatically do the heavy lifting, allowing effortless access to premium content. Browser plugin stores and code repositories containing paywall bypass tools are usually taken down, such as the well-known https://github.com/iamadamdev/bypass-paywalls-chrome. Yet, the code often finds a new home before removal, as seen with https://github.com/nikolqyy/bypass-paywalls-chrome/releases/tag/most-recent 1.\nThis cat-and-mouse game is time-consuming for media companies, as they must constantly identify which plugins are currently bypassing their paywalls. Additionally, executing DMCA takedown notices is not instantaneous. Even after a successful takedown, someone with a cloned repository can reupload the code or plugin, and the cycle continues. This strategy only affects publicly available plugins and ideas, and ironically, the more control attempts are made, the more it spreads awareness that the website can be bypassed. This may prompt users who previously paid willingly to feel short-changed and start seeking bypass methods.\nFurthermore, this approach tends to target only well-known plugins and repositories. Lesser-known repositories continue to grow unnoticed (e.g., https://github.com/nikolqyy/bypass-paywalls-chrome). It’s unlikely that users accustomed to accessing content for free will start paying even if access is restricted. Moreover, users who have already installed these plugins will continue to enjoy premium content without paying."
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#a-better-solution",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#a-better-solution",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "A Better Solution",
    "text": "A Better Solution\nAfter issuing a DMCA takedown notice, the next logical step is typically to modify aspects of your website, such as class names and arrangement of site contents, to render old bypass plugins ineffective. However, there’s a more sophisticated solution that is scalable, cost-effective, and doesn’t compromise search engine optimization. This improved approach involves maintaining a select list of search engines that are allowed to access all premium content for SEO purposes. This list may include Google, Bing, DuckDuckGo, Yandex, Baidu, Yahoo, and Ahrefs. On your web servers, you would check the IP address of the calling client and perform a reverse DNS lookup to determine if the IP address is associated with the whitelisted search engines. Of course, this operation is nerwork expensive and should be optimized by caching the results for a set period, such as one week. This means that an IP address identified as belonging to a search engine would not need to be re-evaluated for approximately seven days. This strategy maintains a good balance between functionality and performance."
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#nation-media-groups-case",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#nation-media-groups-case",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "Nation Media Group’s Case",
    "text": "Nation Media Group’s Case\nNation Media Group, a leading media house in East Africa, is in a unique position to improve its paywall strategy. Currently, they leverage Cloudflare as their content delivery network (CDN). This presents an opportunity to optimize their paywall solution using Cloudflare’s services without significant overhead.\nThe vulnerabilities detailed here affect https://nation.africa/ and https://www.businessdailyafrica.com/.\n\nInitial CSS-Based Paywall\nNation Media Group’s intial attempt used simple CSS classes to hide premium content, which could be easily bypassed with basic JavaScript.\nsetTimeout(() =&gt; {\n    // https://nation.africa/\n    // Remove the paywall element\n    document.querySelector('.wall-guard')?.remove();\n    // Allow copying the text\n    document.querySelectorAll('.blk-txt')?.forEach(\n      i =&gt; i.classList.remove('blk-txt'));\n\n    // https://www.businessdailyafrica.com/\n    // Remove the paywall spinner\n    document.querySelector('.spinner')?.remove();\n    // Remove the paywall element\n    document.querySelector('.paywall')?.remove();\n    // Remove the call for action\n    document.querySelector('.grid-container-medium')?.remove();\n\n    // https://www.businessdailyafrica.com/ AND https://nation.africa/\n    // Show the hidden content\n    document.querySelectorAll('.paragraph-wrapper.nmgp')?.forEach(\n      i =&gt; i.classList.remove('nmgp'));\n}, 1);\n\n\nEnhanced JavaScript Security Layer\nAfter we reported the issue to them2, Nation Media Group added a JavaScript layer to prevent easy access to premium content. They implemented a more sophisticated JavaScript-based security measure.\nThe new JavaScript code now removes the actual content from the Document Object Model (DOM), meaning that CSS manipulation alone is no longer sufficient to reveal the hidden content. Nevertheless, there remains a way to silently disable JavaScript by re-fetching the HTML and parsing it as DOM without executing the JavaScript. This method essentially allows the old CSS-based bypass technique to continue functioning.\nThis more advanced bypass script operates by fetching the original HTML, removing paywall elements, enabling images, and replacing the entire body content. By doing so, it effectively circumvents the enhanced security measures put in place by the media outlet.\nsetTimeout(async () =&gt; {\n    // fetch html src\n    const htmlString = await fetch(location.href).then(resp =&gt; resp.text())\n    const newHtmlDocument = new DOMParser().parseFromString(\n        htmlString, 'text/html');\n    // https://nation.africa/\n    // Remove the paywall element\n    newHtmlDocument.querySelector('.wall-guard')?.remove();\n    // Allow copying the text\n    newHtmlDocument.querySelectorAll('.blk-txt')?.forEach(\n        i =&gt; i.classList.remove('blk-txt'));\n\n    // https://www.businessdailyafrica.com/\n    // Remove the paywall spinner\n    newHtmlDocument.querySelector('.spinner')?.remove();\n    // Remove the paywall element\n    newHtmlDocument.querySelector('.paywall')?.remove();\n    // Remove the call for action\n    newHtmlDocument.querySelector('.grid-container-medium')?.remove();\n\n    // https://www.businessdailyafrica.com/ AND https://nation.africa/\n    // Show the hidden content\n    newHtmlDocument.querySelectorAll('.paragraph-wrapper.nmgp')?.forEach(\n        i =&gt; i.classList.remove('nmgp'));\n    // Stop all events\n    // document.body.outerHTML += ''\n    // Enable images\n    newHtmlDocument.querySelectorAll('img.lazy-img').forEach(\n        i =&gt; i.classList.remove('lazy-img'))\n    newHtmlDocument.querySelectorAll('img[data-src]').forEach(img =&gt; {\n        const { dataset } = img;\n        img.src = dataset.src ?? img.src;\n        img.srcset = dataset.srcset ?? img.srcset;\n    });\n    // Remove spinners\n    newHtmlDocument.querySelectorAll('.spinner').forEach(i =&gt; i.remove());\n    // Remove cloundflare email protection label\n    newHtmlDocument.querySelector('.__cf_email__')\n        ?.closest('.paragraph-wrapper')?.remove();\n    // Remove external scripts from the new DOM\n    Array.from(newHtmlDocument.getElementsByTagName('script'))\n        .forEach(script =&gt; script.src ? script.remove() : null);\n    // Remove iframes from the new DOM\n    Array.from(newHtmlDocument.getElementsByTagName('iframe'))\n        .forEach(iframe =&gt; iframe.remove());\n\n    document.body.outerHTML = newHtmlDocument.body.outerHTML;\n\n    removePopup(50, 0);\n\n    // Remove external scripts from the old DOM\n    Array.from(document.getElementsByTagName('script'))\n        .forEach(script =&gt; script.src ? script.remove() : null);\n    // Remove iframes from the old DOM\n    Array.from(document.getElementsByTagName('iframe'))\n        .forEach(iframe =&gt; iframe.remove());\n    // Filter out elements not in body or head\n    Array.from(document.documentElement.children).forEach(\n        element =&gt; !['body', 'head'].includes(element.tagName.toLowerCase()) \n            ? element.remove() \n            : null);\n    // remove popup and make page scrollable\n    function removePopup (maxRetries, retries) {\n        setTimeout(() =&gt; {\n            const popUp = document.querySelector('.fc-ab-root')\n            if (popUp) {\n                popUp?.remove()\n                document.body.style = \"\"\n            } else if (retries &lt; maxRetries) {\n                removePopup(maxRetries, retries + 1)\n            }\n        }, 300);\n    };\n}, 10)\nTo validate, you can use the article that was premium before June 2024, and is still premium now, such as https://nation.africa/kenya/business/inside-world-bank-tough-terms-sh158bn-loan-kenya-4642634. To test the old logic, use an archived version of the premium article at https://web.archive.org/web/20240601075749/https://nation.africa/kenya/business/inside-world-bank-tough-terms-sh158bn-loan-kenya-4642634. Please note this may stop working at any time depending on changes they make on their platforms.\n\n\nImplementing the Suggested Approach (python code)\nThis approach would significantly increase the difficulty of bypassing the paywall while still allowing search engines to index the full content. However, it’s worth noting that determined users could potentially access the content by routing their requests through services like Google’s PageSpeed Insights (https://pagespeed.web.dev/) or Rich Results Test (https://search.google.com/test/rich-results).\nImplementation Considerations\nTo implement this solution effectively, consider the following:\n\nPerformance Optimization: Ensure that the IP lookup and content serving process is optimized to minimize latency. Utilize caching.\nRegular Updates: Continuously monitor and update the list of whitelisted search engine hostnames.\nFallback Mechanism: Implement a fallback strategy for cases where the reverse DNS lookup fails or times out.\nUser Experience: Design the system to handle edge cases gracefully, ensuring a smooth experience for legitimate users and search engine crawlers.\nCompliance: Ensure that the implementation complies with relevant data protection and privacy regulations, such as implementing a secure storage for cached IP information.\n\nBy addressing these considerations, you can create a robust system that balances content protection with search engine accessibility.\n\nimport socket\nfrom ipaddress import ip_address as parse_ip_address\n\nasync def reverse_dns_lookup(ip_address: str, *host_names: str) -&gt; bool:\n    \"\"\"\n    Perform reverse DNS lookup.\n    Usage example:\n        await reverse_dns_lookup(\"66.249.66.1\", \"googlebot.com\", \"google.com\")\n    \n    Parameters\n    ----------\n    ip_address : str\n        the ip address of the client that called the server, \n        or the header value of \"X-Forwarded-For\" incase a \n        proxy/CDN such as cloudflare is used!\n    host_names : list[str]\n        allowed search engines\n        eg: \"googlebot.com\", \"search.msn.com\", \"duckduckgo.com\", etc\n\n    More Information:\n    Verifying Googlebot: \n        https://developers.google.com/search/docs/advanced/crawling/verifying-googlebot\n    How to access the sitemap.xml file of stackoverflow.com\n        https://meta.stackexchange.com/a/324471\n    Reverse IP Domain Check?\n        https://stackoverflow.com/a/716753/3563013\n    \"\"\"\n    try:\n        if len(host_names) &gt; 0:\n            # Raises `ValueError`if ip_address is not valid IPv4 or IPv6 \n            # address.\n            valid_ip_address: str = str(parse_ip_address(ip_address))\n            # Perform reverse DNS lookup\n            # Get hostname from IP, \n            # eg: ('crawl-66-249-66-1.googlebot.com', [], ['66.249.66.1'])\n            ip_address_hostname, aliases_1, _ = socket.gethostbyaddr(\n                valid_ip_address)\n            # Get all IP addesses resolving the hostname (both IPv4 and IPv6)\n            ip_address_list = list(set(\n                [ip[4][0] \n                 for ip \n                 in socket.getaddrinfo(ip_address_hostname, None)]))\n            # Check if IP matches any of the addresses for the hostname\n            if valid_ip_address in ip_address_list:\n                # Perform forward DNS lookup to get all aliases\n                _, aliases_2, _ = socket.gethostbyname_ex(ip_address_hostname)\n                all_aliases = list(set(\n                    [ip_address_hostname] + aliases_1 + aliases_2))\n                # Check if hostname or its aliases match any of the allowed \n                # hosts\n                return any(\n                    host_name \n                    for host_name \n                    in host_names\n                    if any(\n                        alias \n                        for alias \n                        in all_aliases \n                        if alias.casefold().strip() == \\\n                            host_name.casefold().strip() \\\n                            or alias.casefold().strip().endswith(\n                                f\".{host_name.casefold().strip()}\")))\n    except:\n        pass\n    return False\n\n# Test\nawait reverse_dns_lookup(\n    \"66.249.66.1\", \"googlebot.com\", \"googleusercontent.com\", \"google.com\")\n\nTrue\n\n\n\n\nLeveraging Cloudflare’s Web Workers\nGiven Nation Media Group’s existing infrastructure with Cloudflare, they don’t need to rewrite their server code. Instead, they can utilize an intermediary service from Cloudflare known as Web Workers. Cloudflare Workers allow for the interception of requests and the implementation of more complex logic, such as reverse DNS lookups, without requiring changes to the core server infrastructure.\nCloudflare Workers provide first-class support for JavaScript, TypeScript, Python, and Rust, and they can support any programming language via WebAssembly. Additionally, the cost model is favorable, with the first 100,000 requests per day being free and a modest fee of $5 for every 10 million requests beyond that.\nAdvantages of Cloudflare Workers:\n\nEfficiency: Minimal impact on server infrastructure.\nScalability: Cloudflare’s global infrastructure can handle large-scale traffic.\nCost-effectiveness: Free for low to moderate usage, with low costs for higher volumes.\nPerformance: Requests can be intercepted and resolved in pararell with reverse DNS lookup, ensuring faster response times."
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#conclusion",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#conclusion",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "Conclusion",
    "text": "Conclusion\nNo system is foolproof, and it’s important for media companies to maintain ongoing vigilance and updates to ensure the integrity of their paywalls. As the digital media landscape continues to evolve, so too must the strategies for protecting and monetizing content.\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. You will be peronally liable for any misuse of the information provided here. We contacted Nation Media Group on Jun 3, 2024, 2:30 PM East African Time, but they did not respond 3. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#footnotes",
    "href": "posts/balancing-between-paywalls-and-search-engine-optimization/index.html#footnotes",
    "title": "Balancing between Paywalls and Search Engine Optimization",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nhttps://news.ycombinator.com/item?id=41294166↩︎\nWe contacted Nation Media Group through the following emails: support@nation.africa, sales_inquiries@ke.nationmedia.com, newsdesk@ke.nationmedia.com, publiceditor@ke.nationmedia.com, mailbox@ke.nationmedia.com, epaper@ke.nationmedia.com, Customercare@ke.nationmedia.com on Jun 3, 2024, 2:30 PM East African Time↩︎\nWe contacted Nation Media Group through the following emails: support@nation.africa, sales_inquiries@ke.nationmedia.com, newsdesk@ke.nationmedia.com, publiceditor@ke.nationmedia.com, mailbox@ke.nationmedia.com, epaper@ke.nationmedia.com, Customercare@ke.nationmedia.com on Jun 3, 2024, 2:30 PM East African Time↩︎"
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/linear-programming-optimizing-linear-objective-function-with-constraints/index.html",
    "href": "posts/computational-techniques-in-data-science/linear-programming-optimizing-linear-objective-function-with-constraints/index.html",
    "title": "Linear Programming",
    "section": "",
    "text": "Open in Kaggle\n Download as Notebook\n Download as PDF\n\n\n\n1. Transportation Problem: Optimal Shipping Plan\nA logistics company supplies goods from three warehouses (W1, W2, W3) to four retail stores (S1, S2, S3, S4). The transportation cost per unit from each warehouse to each store is given in the table below. Each warehouse has a limited supply, and each store has a demand requirement. The goal is to minimize the total transportation cost.\n\n\n\n\nTo / From\nS1\nS2\nS3\nS4\nSupply\n\n\n\n\nW1\n4\n3\n6\n5\n250\n\n\nW2\n2\n5\n3\n4\n300\n\n\nW3\n7\n6\n4\n3\n400\n\n\nDemand\n200\n200\n250\n300\n-\n\n\n\n\nDecision Variables\nLet \\(x_{ij}\\) be the number of units transported from warehouse \\(iii\\) to store \\(j\\).\nObjective Function\nMinimize total transportation cost:\n\\(Z = 4x_{11} + 3x_{12} + 6x_{13} + 5x_{14} + 2x_{21} + 5x_{22} + 3x_{23} + 4x_{24} + 7x_{31} + 6x_{32} + 4x_{33} + 3x_{34}\\)\nConstraints\nSupply Constraints\n\n\\(x_{11} + x_{12} + x_{13} + x_{14} \\leq 250  \\quad \\text{(Warehouse W1)}\\)\n\\(x_{21} + x_{22} + x_{23} + x_{24} \\leq 300  \\quad \\text{(Warehouse W2)}\\)\n\\(x_{31} + x_{32} + x_{33} + x_{34} \\leq 400  \\quad \\text{(Warehouse W3)}\\)\n\nDemand Constraints\n\n\\(x_{11} + x_{21} + x_{31} = 200  \\quad \\text{(Store S1)}\\)\n\\(x_{12} + x_{22} + x_{32} = 200  \\quad \\text{(Store S2)}\\)\n\\(x_{13} + x_{23} + x_{33} = 250  \\quad \\text{(Store S3)}\\)\n\\(x_{14} + x_{24} + x_{34} = 300  \\quad \\text{(Store S4)}\\)\n\nNon-Negativity Constraints: \\(x_{ij} \\geq 0 \\quad \\text{for all } i, j\\)\n\nAnswer\n\nfrom scipy.optimize import linprog\n\n# Cost coefficients\nc_transport = [4, 3, 6, 5, 2, 5, 3, 4, 7, 6, 4, 3]  \n\nA_transport = [  # Coefficients for constraints (Supply + Demand)\n    [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],  # W1 supply\n    [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0],  # W2 supply\n    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1],  # W3 supply\n    [1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],  # S1 demand\n    [0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],  # S2 demand\n    [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],  # S3 demand\n    [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1],  # S4 demand\n]\n\nb_transport = [250, 300, 400, 200, 200, 250, 300]  # Supply & Demand constraints\nbounds_transport = [(0, None)] * 12  # Non-negativity\n\nres_transport = linprog(\n    c_transport, \n    A_ub = A_transport[:3], \n    b_ub = b_transport[:3], \n    A_eq = A_transport[3:], \n    b_eq = b_transport[3:], \n    bounds = bounds_transport, \n    method='highs')\nres_transport\n\n        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n        success: True\n         status: 0\n            fun: 2850.0\n              x: [ 5.000e+01  2.000e+02  0.000e+00  0.000e+00  1.500e+02\n                   0.000e+00  1.500e+02  0.000e+00  0.000e+00  0.000e+00\n                   1.000e+02  3.000e+02]\n            nit: 6\n          lower:  residual: [ 5.000e+01  2.000e+02  0.000e+00  0.000e+00\n                              1.500e+02  0.000e+00  1.500e+02  0.000e+00\n                              0.000e+00  0.000e+00  1.000e+02  3.000e+02]\n                 marginals: [ 0.000e+00  0.000e+00  1.000e+00  1.000e+00\n                              0.000e+00  4.000e+00  0.000e+00  2.000e+00\n                              4.000e+00  4.000e+00  0.000e+00  0.000e+00]\n          upper:  residual: [       inf        inf        inf        inf\n                                    inf        inf        inf        inf\n                                    inf        inf        inf        inf]\n                 marginals: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00\n                              0.000e+00  0.000e+00  0.000e+00  0.000e+00\n                              0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n          eqlin:  residual: [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00]\n                 marginals: [ 4.000e+00  3.000e+00  5.000e+00  4.000e+00]\n        ineqlin:  residual: [ 0.000e+00  0.000e+00  0.000e+00]\n                 marginals: [-0.000e+00 -2.000e+00 -1.000e+00]\n mip_node_count: 0\n mip_dual_bound: 0.0\n        mip_gap: 0.0\n\n\n\n\nInterpretation\n\nThe minimum total transportation cost is $2,850.\nOptimal shipment plan:\n\nFrom W1 to S1: 50 units\n\nFrom W1 to S2: 200 units\n\nFrom W1 to S3 & S4: 0 units\n\nFrom W2 to S1: 150 units\n\nFrom W2 to S2: 0 units\n\nFrom W2 to S3: 150 units\n\nFrom W2 to S4: 0 units\n\nFrom W3 to S1 & S2: 0 units\n\nFrom W3 to S3: 100 units\n\nFrom W3 to S4: 300 units\n\nShadow prices (dual values) for demand constraints:\n\nS1 = $4, meaning if demand at S1 increases by 1 unit, total cost increases by $4.\nS2 = $3, meaning an extra unit at S2 increases cost by $3.\nS3 = $5, meaning an extra unit at S3 increases cost by $5.\nS4 = $4, meaning an extra unit at S4 increases cost by $4.\n\nMarginals\n\nW1 = $0, meaning increasing W1’s supply doesn’t impact cost.\nW2 = -$2, meaning if W2’s supply increased, costs could reduce by $2 per unit.\nW3 = -$1, meaning if W3’s supply increased, costs could reduce by $1 per unit.\n\n\n\n\n\n2. Manufacturing Problem: Maximizing Profit (Product Mix)\nA company produces two types of products (A and B) using two machines (M1 and M2). The processing time (in hours per unit) and the profit per unit are given below. The company has a limited number of available hours for each machine. The objective is to maximize profit.\n\n\n\n\n\n\n\n\n\n\nProduct\nM1 Hours per unit\nM2 Hours per unit\nProfit per unit ($)\n\n\n\n\nA\n3\n2\n50\n\n\nB\n5\n4\n80\n\n\n\n\n\nMachine M1 has 600 hours available.\n\nMachine M2 has 500 hours available.\n\nDecision Variables\nLet \\(x_1\\) be the number of units of product A produced.\nLet \\(x_2\\) be the number of units of product B produced.\nObjective Function\nMaximize profit: \\(Z = 50x_1 + 80x_2\\)\nConstraints\nMachine Time Constraints\n\n\\(3x_1 + 5x_2 \\leq 600  \\quad \\text{(M1 capacity)}\\)\n\\(2x_1 + 4x_2 \\leq 500  \\quad \\text{(M2 capacity)}\\)\n\nNon-Negativity Constraints: \\(x_1, x_2 \\geq 0\\)\n\nAnswer\n\nc_profit = [-50, -80]  # Coefficients (negative for maximization)\n\nA_profit = [  # Machine constraints\n    [3, 5],   # M1 constraint\n    [2, 4],   # M2 constraint\n]\n\nb_profit = [600, 500]  # Available hours\nbounds_profit = [(0, None), (0, None)]  # Non-negativity\n\nres_profit = linprog(\n    c_profit, \n    A_ub = A_profit, \n    b_ub = b_profit, \n    bounds = bounds_profit, \n    method='highs')\n\nres_profit\n\n        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n        success: True\n         status: 0\n            fun: -10000.0\n              x: [ 2.000e+02  0.000e+00]\n            nit: 1\n          lower:  residual: [ 2.000e+02  0.000e+00]\n                 marginals: [ 0.000e+00  3.333e+00]\n          upper:  residual: [       inf        inf]\n                 marginals: [ 0.000e+00  0.000e+00]\n          eqlin:  residual: []\n                 marginals: []\n        ineqlin:  residual: [ 0.000e+00  1.000e+02]\n                 marginals: [-1.667e+01 -0.000e+00]\n mip_node_count: 0\n mip_dual_bound: 0.0\n        mip_gap: 0.0\n\n\n\n\nInterpretation\n\nMaximum Profit: $10,000.0\nOptimal production quantities:\n\n200.0 units of Product A and 0.0 units of Product B\n\nRemaining capacities after allocating resources:\n\nMachine M1: Residual = 0.0 (Fully utilized) Machine M2: Residual = 100.0 (Unused)\n\nShadow price (dual value):\n\nMachine M1 = 16.67, meaning that if we had one more hour of M1, the profit would increase by 16.67. Machine M2 = 0.0, meaning extra hours for M2 won’t increase profit\n\n\n\n3. Manufacturing Problem: Minimizing Production Cost\nA furniture company manufactures chairs and tables. The company has limited resources of wood and labor and wants to minimize the total production cost.\n\n\n\n\n\n\n\n\n\n\nProduct\nWood Required (cubic ft.)\nLabor Required (hours)\nCost per unit ($)\n\n\n\n\nChair\n5\n2\n30\n\n\nTable\n8\n3\n50\n\n\n\n\n\nAvailable wood: 800 cubic feet\n\nAvailable labor: 300 hours\n\nDecision Variables\nLet \\(x_1\\) be the number of chairs produced.\nLet \\(x_2\\) be the number of tables produced.\nObjective Function\nMinimize cost: \\(Z = 30x_1 + 50x_2\\)\nConstraints:\n\n\\(5x_1 + 8x_2 \\leq 800  \\quad \\text{(Wood availability)}\\)\n\\(2x_1 + 3x_2 \\leq 300  \\quad \\text{(Labor availability)}\\)\n\nNon-Negativity Constraints: \\(x_1, x_2 \\geq 0\\)\n\nAnswer\n\nc_cost = [30, 50]  # Cost coefficients\n\nA_cost = [  # Resource constraints\n    [5, 8],  # Wood constraint\n    [2, 3],  # Labor constraint\n]\n\nb_cost = [800, 300]  # Available resources\nbounds_cost = [(0, None), (0, None)]  # Non-negativity\n\nres_cost = linprog(\n    c_cost, \n    A_ub = A_cost, \n    b_ub = b_cost, \n    bounds = bounds_cost, \n    method = 'highs')\n\nres_cost\n\n       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n       success: True\n        status: 0\n           fun: 0.0\n             x: [ 0.000e+00  0.000e+00]\n           nit: 0\n         lower:  residual: [ 0.000e+00  0.000e+00]\n                marginals: [ 3.000e+01  5.000e+01]\n         upper:  residual: [       inf        inf]\n                marginals: [ 0.000e+00  0.000e+00]\n         eqlin:  residual: []\n                marginals: []\n       ineqlin:  residual: [ 8.000e+02  3.000e+02]\n                marginals: [-0.000e+00 -0.000e+00]\n\n\n\n\nInterpretation\nThe minimum production cost is $0.0, meaning that the optimal decision is not to produce any chairs or tables.\nOptimal production quantities:\n\nChairs: 0.0 units\nTables: 0.0 units\nThe company should not produce anything to achieve the lowest cost.\n\nRemaining resources:\n\nWood: 800.0 cubic feet unused\nLabor: 300.0 hours unused\nSince no production takes place, all resources remain unused.\n\nShadow prices for wood and labor are both 0.0, meaning that increasing available resources would not change the optimal solution.\n\nThis suggests that there is no economic incentive to produce chairs or tables under the given cost structure.\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service\n\n\n\n\nReuseGNU GENERAL PUBLIC LICENSE v3.0(View License)CitationBibTeX citation:@misc{kabui2025,\n  author = {{Kabui, Charles}},\n  title = {Linear {Programming}},\n  date = {2025-03-24},\n  url = {https://toknow.ai/posts/computational-techniques-in-data-science/linear-programming-optimizing-linear-objective-function-with-constraints/index.html},\n  langid = {en-GB}\n}\nFor attribution, please cite this work as:\nKabui, Charles. 2025. “Linear Programming.” https://toknow.ai/posts/computational-techniques-in-data-science/linear-programming-optimizing-linear-objective-function-with-constraints/index.html."
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/stochastic-programming-and-applications/index.html#description",
    "href": "posts/computational-techniques-in-data-science/stochastic-programming-and-applications/index.html#description",
    "title": "Stochastic Programming and Applications",
    "section": "Description",
    "text": "Description\nStochastic programming is an optimization framework that incorporates uncertainty into decision-making models. Unlike deterministic optimization, which assumes perfect/fixed information about all parameters, stochastic programming accounts for randomness in constraints and objectives.\nKey Features of Stochastic Programming:\n\nDecision Variables: Represent choices to be optimized.\nUncertainty (Random Variables): Captures variability in parameters.\nObjective Function: Typically involves expected value optimization.\nConstraints: Incorporate probabilistic constraints or chance constraints."
  },
  {
    "objectID": "posts/computational-techniques-in-data-science/stochastic-programming-and-applications/index.html#application",
    "href": "posts/computational-techniques-in-data-science/stochastic-programming-and-applications/index.html#application",
    "title": "Stochastic Programming and Applications",
    "section": "Application",
    "text": "Application\n\nThe goal is often to minimize expected cost or maximize expected profit while considering risk measures.\n\n\nNewsvendor Problem\nA vendor must decide how many items to stock without knowing the exact demand. The goal is to minimize expected costs, balancing:\nOverstock costs: Money lost on unsold items Understock costs: Lost revenue from unmet demand\nOptimal stocking levels depend on the probability distribution of demand.\nPortfolio Optimization\nIn finance, investors allocate funds across assets to maximize expected returns while controlling risk. The problem involves:\nDecision variables: Asset allocation weights Objective: Maximize expected returns considering the risk Constraints: Budget (weights sum to 1), non-negativity, diversification limits.\nThe solution balances the trade-off between return and risk across various market scenarios.\n\n\nImports\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cvxpy as cp\n\n\n\n\nSupply Chain Optimization under Demand Uncertainty\nConsider Supply Chain Optimization under Demand Uncertainty. Demand for electronic supply in Nairobi in the past two weeks together with their respective probability is given below:\n\n\n\n\nDemand\nProbability\n\n\n\n\n255\n0.03\n\n\n302\n0.15\n\n\n270\n0.04\n\n\n317\n0.1\n\n\n285\n0.05\n\n\n332\n0.05\n\n\n300\n0.09\n\n\n347\n0.01\n\n\n315\n0.09\n\n\n362\n0.03\n\n\n330\n0.2\n\n\n262\n0.07\n\n\n309\n0.05\n\n\n277\n0.04\n\n\n\n\nConsider constraints = [supply &gt;= 120, supply &lt;= 330]. Compute the optimal supply.\n\n\n# Given demand data\ndemand = np.array([\n    255, \n    302, \n    270, \n    317, \n    285, \n    332, \n    300, \n    347, \n    315, \n    362, \n    330, \n    262, \n    309, \n    277])\nprobability = np.array([\n    0.03, \n    0.15, \n    0.04, \n    0.1, \n    0.05, \n    0.05, \n    0.09, \n    0.01, \n    0.09, \n    0.03, \n    0.2, \n    0.07, \n    0.05, \n    0.04])\n\n\n# Verify that probabilities sum to 1\nprint(f\"Sum of probabilities: {np.sum(probability):.2f}\")\n\nSum of probabilities: 1.00\n\n\n\n# Define the supply decision variable\nsupply = cp.Variable()\n\n# Constraints: supply should be between 120 and 330\nconstraints = [supply &gt;= 120, supply &lt;= 330]\n\n\n# Define the expected deviation cost function (penalizing shortages and excesses)\ncost = cp.sum(cp.multiply(probability, cp.abs(supply - demand)))\n\n# Define the optimization problem\nproblem = cp.Problem(cp.Minimize(cost), constraints)\n\n# Solve the problem\n_ = problem.solve()\n\n\nExplanation:\n\ncp.abs(supply - demand)\n\nThis penalizes both over-supply (excess) and under-supply (shortages), treating both as equally costly.\nThis is useful when deviations in either direction are undesirable.\nThis is our scenario!\n\ncp.pos(supply - demand)\n\nThis only penalizes cases where supply &gt; demand, meaning it considers only excess supply as costly.\nIt does not penalize shortages, which may not be ideal if stockouts are a concern.\n\n\n\n# Print the optimal supply value\noptimal_supply = supply.value\nprint(\"Optimal Supply:\", optimal_supply)\n\nOptimal Supply: 309.00000011886954\n\n\n\n# Plot the cost function over a range of supply values\nsupply_range = np.linspace(120, 330, 100)\ncost_values = [sum(p * abs(s - d) for p, d in zip(probability, demand)) for s in supply_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(supply_range, cost_values, label='Expected Cost', color='blue')\nplt.axvline(optimal_supply, color='red', linestyle='--', label=f'Optimal Supply ({optimal_supply:.2f})')\nplt.xlabel('Supply')\nplt.ylabel('Expected Cost')\nplt.title('Supply Chain Optimization under Demand Uncertainty')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nHealthcare Resource Allocation with Uncertain Demand\nConsider ICU Bed Allocation under Uncertain Patient Arrivals. Number of unscheduled arrivals at Kenyatta National Hospital in the last 10 days has been [25, 20, 30, 50, 27, 39, 42, 29, 35, 42] patients with assigned probabilities [0.1, 0.1, 0.08, 0.15, 0.09, 0.05, 0.1, 0.1, 0.13, 0.1] respectively. Consider bed constraints = [beds &gt;= 17, beds &lt;= 55]. Compute the optimal number of ICU beds allocation.\n\n\n# Given ICU arrival values and their probabilities\narrivals = np.array([25, 20, 30, 50, 27, 39, 42, 29, 35, 42])\nprobability = np.array([0.1, 0.1, 0.08, 0.15, 0.09, 0.05, 0.1, 0.1, 0.13, 0.1])\n\n\n# Verify that probabilities sum to 1\nprint(f\"Sum of probabilities: {np.sum(probability):.2f}\")\n\nSum of probabilities: 1.00\n\n\n\n# Define the bed allocation decision variable\nbeds = cp.Variable()\n\n# Constraints: beds should be between 17 and 55\nconstraints = [beds &gt;= 17, beds &lt;= 55]\n\n\n# Define the expected deviation cost function (penalizing shortages and excesses)\ncost = cp.sum(cp.multiply(probability, cp.abs(beds - arrivals)))\n\n# Define the optimization problem\nproblem = cp.Problem(cp.Minimize(cost), constraints)\n\n# Solve the problem\n_ = problem.solve()\n\n\n# Print the optimal bed allocation\noptimal_beds = beds.value\nprint(\"Optimal ICU Beds Allocation:\", optimal_beds)\n\nOptimal ICU Beds Allocation: 34.99999972144706\n\n\n\n# Plot the cost function over a range of bed allocations\nbed_range = np.linspace(17, 55, 100)\ncost_values = [sum(p * abs(b - a) for p, a in zip(probability, arrivals)) for b in bed_range]\n\nplt.figure(figsize=(8, 5))\nplt.plot(bed_range, cost_values, label='Expected Cost', color='blue')\nplt.axvline(optimal_beds, color='red', linestyle='--', label=f'Optimal Beds ({optimal_beds:.2f})')\nplt.xlabel('ICU Beds')\nplt.ylabel('Expected Cost')\nplt.title('Healthcare Resource Allocation with Uncertain Demand')\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/kenya-life-expectancy-between-2000-and-2021/index.html",
    "href": "posts/kenya-life-expectancy-between-2000-and-2021/index.html",
    "title": "Life Expectancy by sex between 2000 and 2021 in Kenya (WHO)",
    "section": "",
    "text": "Open in Kaggle\n Download as Notebook\n Download as PDF\n\n\n\nImports\n\n\nCode\nimport pandas as pd\nimport urllib.parse\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_who_data(\n        raw_data: pd.DataFrame, \n        title: str, \n        year_col = \"Year\", \n        value_col = \"Value\", \n        pivot_columns = \"Sex\", \n        pivot_data_columns = [\n            {\n                \"name\": \"TOTAL\",\n                \"options\": {\n                    'marker': 'o',\n                    'linewidth': 2,\n                    'color': 'purple'\n                }\n            },\n            {\n                \"name\": \"MALE\",\n                \"options\": {\n                    'marker': 's',\n                    'linewidth': 2,\n                    'color': 'blue'\n                }\n            },\n            {\n                \"name\": \"FEMALE\",\n                \"options\": {\n                    'marker': '^',\n                    'linewidth': 2,\n                    'color': 'pink'\n                }\n            },\n        ]):\n    data = raw_data.copy()\n    # Convert Year to numeric and Value to float\n    data[year_col] = pd.to_numeric(data[year_col])\n    data[value_col] = pd.to_numeric(data[value_col])\n    # Pivot the data to have columns for each sex category\n    pivot_data = data.pivot_table(index=year_col, columns=pivot_columns, values=value_col)\n    # Set the style\n    sns.set_style(\"whitegrid\")\n    fig, ax = plt.subplots(figsize=(12, 6))\n    # Plot the data\n    for column in pivot_data_columns:\n        sex = column['name']\n        ax.plot(pivot_data.index, pivot_data[sex], label=sex, **column['options'])\n    # Add title and labels\n    ax.set_title(title, fontsize=16)\n    ax.set_xlabel(year_col, fontsize=12)\n    ax.set_ylabel(value_col, fontsize=12)\n    ax.legend(title=pivot_columns)\n    ax.grid(True, alpha=0.3)\n    # Add some annotations - show max and min values\n    for sex in pivot_data.columns:\n        max_year = pivot_data[sex].idxmax()\n        max_val = pivot_data[sex].max()\n        ax.annotate(f\"max: {max_val:.1f}\", \n                    xy=(max_year, max_val),\n                    xytext=(5, 5),\n                    textcoords='offset points',\n                    fontsize=9)\n    # Adjust the layout\n    fig.tight_layout()\n    # Add data source note\n    fig.text(0.1, 0.01, \"Data Source: WHO Xmart API\", fontsize=8, style='italic')\n    # Add a watermark to the center of the plot\n    ax.text(0.95, 0.02, 'ToKnow.ai', ha='right', va='bottom', \n        fontsize=18, color='gray', alpha=0.5, transform=ax.transAxes, rotation=45)\n\ndef url_encode(url):\n    return urllib.parse.quote(url, safe=\":/?&=$\")\n\n\n\n\nLife expectancy, Kenya\nThe average number of years that a newborn could expect to live. Kenya, by sex, 2000 - 2021.\n\nlife_expectancy_filter = \"IND_ID eq '90E2E48WHOSIS_000001' and GEO_NAME_SHORT eq 'Kenya'\"\nlife_expectancy_select = \"DIM_GEO_CODE_M49, GEO_NAME_SHORT, DIM_TIME, IND_NAME, DIM_SEX, AMOUNT_N\"\nlife_expectancy_data = pd.read_csv(url_encode(\n    f\"https://xmart-api-public.who.int/DATA_/RELAY_WHS?$filter={life_expectancy_filter}&$select={life_expectancy_select}&$format=csv\"),\n    names=[\n        \"Country Code\",\n        \"Country\",\n        \"Year\",\n        \"Indicator\",\n        \"Sex\",\n        \"Life expectancy (at birth)\"\n    ],\n    header=1)\n\nlife_expectancy_data\n\n\n\n\n\n\n\n\n\nCountry Code\nCountry\nYear\nIndicator\nSex\nLife expectancy (at birth)\n\n\n\n\n0\n404\nKenya\n2009\nLife expectancy (at birth)\nTOTAL\n60.490621\n\n\n1\n404\nKenya\n2010\nLife expectancy (at birth)\nTOTAL\n61.282761\n\n\n2\n404\nKenya\n2012\nLife expectancy (at birth)\nTOTAL\n62.615857\n\n\n3\n404\nKenya\n2014\nLife expectancy (at birth)\nTOTAL\n63.636944\n\n\n4\n404\nKenya\n2001\nLife expectancy (at birth)\nTOTAL\n54.332836\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n60\n404\nKenya\n2019\nLife expectancy (at birth)\nFEMALE\n68.991063\n\n\n61\n404\nKenya\n2006\nLife expectancy (at birth)\nFEMALE\n59.008718\n\n\n62\n404\nKenya\n2007\nLife expectancy (at birth)\nFEMALE\n60.214093\n\n\n63\n404\nKenya\n2011\nLife expectancy (at birth)\nFEMALE\n64.116368\n\n\n64\n404\nKenya\n2018\nLife expectancy (at birth)\nFEMALE\n68.566521\n\n\n\n\n\n65 rows × 6 columns\n\n\n\n\nplot_who_data(\n    life_expectancy_data, \n    \"Life expectancy (at birth) in Kenya\", \n    value_col=\"Life expectancy (at birth)\")\n\n\n\n\n\n\n\n\nSummary Statistics\n\nlife_expectancy_data.pivot_table(\n    index=\"Year\", \n    columns=\"Sex\", \n    values=\"Life expectancy (at birth)\").describe()\n\n\n\n\n\n\n\n\nSex\nFEMALE\nMALE\nTOTAL\n\n\n\n\ncount\n22.000000\n22.000000\n21.000000\n\n\nmean\n62.856168\n58.951730\n61.050339\n\n\nstd\n5.099057\n3.984922\n4.563089\n\n\nmin\n55.217093\n52.941969\n54.092039\n\n\n25%\n58.087847\n55.232042\n56.395140\n\n\n50%\n63.623846\n59.656871\n61.981267\n\n\n75%\n67.346733\n62.419297\n65.032495\n\n\nmax\n69.673467\n64.395877\n66.793450\n\n\n\n\n\n\n\n\n\n\nHealthy life expectancy (HALE) at birth\nThe average number of years that a person can expect to live in “full health” from birth. Kenya, by sex, 2000 - 2021.\n\nhealthy_life_expectancy_filter = \"IND_ID eq 'C64284DWHOSIS_000002' and GEO_NAME_SHORT eq 'Kenya'\"\nhealthy_life_expectancy_select = \"DIM_GEO_CODE_M49, GEO_NAME_SHORT, DIM_TIME, IND_NAME, DIM_SEX, AMOUNT_N\"\nhealthy_life_expectancy_data = pd.read_csv(url_encode(\n    f\"https://xmart-api-public.who.int/DATA_/RELAY_WHS?$filter={life_expectancy_filter}&$select={life_expectancy_select}&$format=csv\"),\n    names=[\n        \"Country Code\",\n        \"Country\",\n        \"Year\",\n        \"Indicator\",\n        \"Sex\",\n        \"Healthy life expectancy (at birth)\"\n    ],\n    header=1)\n\nhealthy_life_expectancy_data\n\n\n\n\n\n\n\n\n\nCountry Code\nCountry\nYear\nIndicator\nSex\nHealthy life expectancy (at birth)\n\n\n\n\n0\n404\nKenya\n2009\nLife expectancy (at birth)\nTOTAL\n60.490621\n\n\n1\n404\nKenya\n2010\nLife expectancy (at birth)\nTOTAL\n61.282761\n\n\n2\n404\nKenya\n2012\nLife expectancy (at birth)\nTOTAL\n62.615857\n\n\n3\n404\nKenya\n2014\nLife expectancy (at birth)\nTOTAL\n63.636944\n\n\n4\n404\nKenya\n2001\nLife expectancy (at birth)\nTOTAL\n54.332836\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n60\n404\nKenya\n2019\nLife expectancy (at birth)\nFEMALE\n68.991063\n\n\n61\n404\nKenya\n2006\nLife expectancy (at birth)\nFEMALE\n59.008718\n\n\n62\n404\nKenya\n2007\nLife expectancy (at birth)\nFEMALE\n60.214093\n\n\n63\n404\nKenya\n2011\nLife expectancy (at birth)\nFEMALE\n64.116368\n\n\n64\n404\nKenya\n2018\nLife expectancy (at birth)\nFEMALE\n68.566521\n\n\n\n\n\n65 rows × 6 columns\n\n\n\n\nplot_who_data(\n    healthy_life_expectancy_data, \n    \"Healthy life expectancy (HALE) at birth in Kenya\", \n    value_col=\"Healthy life expectancy (at birth)\")\n\n\n\n\n\n\n\n\nSummary Statistics\n\nhealthy_life_expectancy_data.pivot_table(\n    index=\"Year\", \n    columns=\"Sex\", \n    values=\"Healthy life expectancy (at birth)\").describe()\n\n\n\n\n\n\n\n\nSex\nFEMALE\nMALE\nTOTAL\n\n\n\n\ncount\n22.000000\n22.000000\n21.000000\n\n\nmean\n62.856168\n58.951730\n61.050339\n\n\nstd\n5.099057\n3.984922\n4.563089\n\n\nmin\n55.217093\n52.941969\n54.092039\n\n\n25%\n58.087847\n55.232042\n56.395140\n\n\n50%\n63.623846\n59.656871\n61.981267\n\n\n75%\n67.346733\n62.419297\n65.032495\n\n\nmax\n69.673467\n64.395877\n66.793450\n\n\n\n\n\n\n\n\n\n\nReferences\nSource: World Health Organization (WHO)\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service\n\n\n\nReuseGNU GENERAL PUBLIC LICENSE v3.0(View License)CitationBibTeX citation:@misc{kabui2025,\n  author = {{Kabui, Charles}},\n  title = {Life {Expectancy} by Sex Between 2000 and 2021 in {Kenya}\n    {(WHO)}},\n  date = {2025-03-10},\n  url = {https://toknow.ai/posts/kenya-life-expectancy-between-2000-and-2021/index.html},\n  langid = {en-GB}\n}\nFor attribution, please cite this work as:\nKabui, Charles. 2025. “Life Expectancy by Sex Between 2000 and\n2021 in Kenya (WHO).” https://toknow.ai/posts/kenya-life-expectancy-between-2000-and-2021/index.html."
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#updates",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#updates",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Updates",
    "text": "Updates\n\nThursday, December 19, 2024\nSafaricom launches Ziidi Money Market Fund to mimic Mali Money Market Fund. Safaricom blamed Genghis for the Mali Market Fund’s delayed launch and halts customer onbording due compaints of unstable Genghis platform. Genghis calls safaricom a fraud for secretly redirecting Mali Money Market Fund customers for Ziidi Money Market Fund thereby breaching data laws. Safaricom, Genghis Capital fight over M-Pesa unit trusts\n\n\nDry Associates Limited (DAL) has converted its Balanced Fund into the DAL High Yield Special Fund, now focused on high-yield returns through Kenyan and East African government securities and corporate fixed-income investments, https://www.cma.or.ke/cma-approves-two-new-funds-to-expand-investment-opportunities-for-sophisticated-investors/"
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#introduction",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#introduction",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, Kenya’s financial landscape has witnessed significant growth and diversification, with Collective Investment Schemes playing an increasingly prominent role. Among these, Money Market Funds (MMFs) have emerged as a particularly popular investment vehicle, offering a unique blend of benefits that appeal to a wide range of investors in the Kenyan market.\nMoney Market Funds operate by pooling capital from numerous investors, which professional fund managers then invest collectively in short-term, highly liquid financial instruments. This structure allows MMFs to offer several key advantages in the Kenyan context:\n\nHigher Returns: MMFs typically provide superior interest rates compared to standard savings accounts, making them an attractive option for investors seeking to maximize their returns on short-term investments.\nLower Entry Barriers: With the ability to start investing with smaller amounts, MMFs have democratized access to professional fund management, opening up opportunities for a broader range of Kenyan investors.\nCompound Interest: Unlike most traditional bank deposits that offer simple interest, MMFs generally provide compound interest. This feature can lead to accelerated wealth accumulation over time, particularly benefiting long-term investors.\nEnhanced Liquidity: MMFs maintain high liquidity, allowing investors to access their funds quickly when needed, typically within one to two business days after a withdrawal request. This flexibility is crucial in a dynamic economy like Kenya’s, where financial needs can change rapidly.\nDiversification: By investing in a variety of short-term securities, MMFs offer a level of diversification that can be challenging for individual investors to achieve on their own, especially with limited capital.\n\nThe growing popularity of MMFs in Kenya reflects broader trends in the country’s financial sector, including increased financial literacy, a growing middle class, and the expansion of digital financial services. However, despite their importance, comprehensive and accessible data on the performance and characteristics of these funds has been limited.\nThis study aims to address this gap by creating a robust, clean dataset of Kenyan Collective Investment Schemes, with a particular focus on Money Market Funds. Our objectives are:\n\nTo source and gather relevant data from authoritative sources, including the Capital Markets Authority (CMA) and published financial reports.\nTo clean and standardize the collected data, ensuring consistency and reliability for analytical purposes.\nTo archive and publish the resulting dataset, facilitating further research and analysis by academics, industry professionals, and policymakers.\n\nBy undertaking this data-centric approach, we aim to contribute to the broader understanding of Kenya’s financial market dynamics, particularly in the realm of collective investments. This dataset will serve as a foundation for more in-depth analyses, potentially informing investment strategies, policy decisions, and academic research in the field of Kenyan finance.\nIn the following sections, we will detail our methodology for data collection and cleaning, and present the structure of the resulting dataset. Through this effort, we hope to not only shed light on the current state of Money Market Funds in Kenya but also to set a precedent for transparent, reproducible financial data curation in emerging markets."
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#sourcing-and-gathering-data",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#sourcing-and-gathering-data",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Sourcing and Gathering Data",
    "text": "Sourcing and Gathering Data\nBefore we begin, lets prepare our enviroment with some important python packages and reusable functions\n\n\nShow python imports\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add root directory as python path\nroot_dir = os.path.abspath(Path(sys.executable).parents[2])\nsys.path.append(root_dir)\n\n%reload_ext autoreload\n%autoreload 2\n\n# Other imports\nimport pandas as pd\nfrom playwright.async_api import Page, Route\nimport asyncio\nimport io\nfrom bs4 import BeautifulSoup, Tag\nfrom urllib.request import urlopen\nimport json5 as json5\nimport json\nfrom tqdm import tqdm\nimport re\nfrom typing import Callable, Literal\nfrom copy import copy\nfrom datetime import datetime, timedelta, date\nfrom calendar import monthrange, month_abbr\nimport plotly.express as px\nfrom json2txttree import json2txttree\nfrom python_utils.web_screenshot import web_screenshot_async\nfrom python_utils.get_browser import get_browser_page_async\nfrom typing import Any\nfrom toolz import groupby\nimport numpy as np\nimport inspect\nfrom PIL import Image\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n\n\n\nShow reusable functions\ncollective_scheme_type = \\\n    dict[Literal['Scheme'], str] | dict[Literal['Funds'], list[str]]\n    \n\ndef strip_start_end(s1: str, last_acceptable_characters = ')'):\n    \"\"\"\n    Cleans a given string by removing specific patterns and non-alphabet \n    characters at the start and end of a string.\n\n    Args:\n        s1 (str): The input string to be cleaned.\n        last_acceptable_characters (str, optional):  \n            Characters that are acceptable at the end of the string. Defaults to ')'.\n\n    Returns:\n        str: The cleaned string.\n\n    The function performs the following steps:\n    1. Removes the phrase \"comprising of\" or \"which comprises of\".\n    2. Removes the word \"and\" followed by any non-alphabet characters at the end of the string.\n    3. Removes any non-alphabet characters from the start of the string.\n    4. Removes any non-alphabet characters from the end of the string, \n        except those specified in `last_acceptable_characters`.\n    5. Replaces multiple spaces with a single space.\n    6. Strips leading and trailing whitespace.\n    7. Recursively applies the function if any of the patterns still match the string.\n    8. Removes non-ASCII characters.\n\n    Example:\n        &gt;&gt;&gt; strip_start_end(\"comprising of example and123\")\n        'example'\n    \"\"\"\n    if type(s1) != str or s1 is None:\n        return ''\n    # Define a regex pattern to match 'and' followed by any non-alphabet \n    # characters at the end of the string\n    and_pattern = r'\\band[^a-zA-Z]*$'\n    # Define a regex pattern to match any non-alphabet characters at the start of the string\n    non_alphabet_start = r'^[^a-zA-Z]+'\n    # Define a regex pattern to match any non-alphabet characters at the end of the string\n    non_alphabet_end = f'[^a-zA-Z{last_acceptable_characters}]+$'\n    # Define a regex pattern to match the phrase \"comprising of|which comprises of\"\n    comprising_of_pattern = r'comprising of|which comprises of'\n    # Replace multiple spaces with a single space\n    multiple_white_space = r'\\s+'\n    s2 = re.sub(comprising_of_pattern, '', s1)\n    s3 = re.sub(and_pattern, '', s2)\n    s4 = re.sub(non_alphabet_start, '', s3)\n    s5 = re.sub(non_alphabet_end, '', s4)\n    s6 = re.sub(multiple_white_space, ' ', s5)\n    s7 = s6.strip()\n    # Recursively apply the function if any of the patterns still match the string\n    while any(re.match(p, s7) for p in [\n        and_pattern, non_alphabet_start, non_alphabet_end, comprising_of_pattern]):\n        return strip_start_end(s5)\n    # remove non ASCII characters\n    s8 = s7.encode('ascii', errors='ignore').decode()\n    # Return the cleaned string\n    return s8\n\ndef hacky_normalizer(val: str):\n    \"\"\"\n    Normalizes a given string by performing the following operations:\n    1. Strips leading and trailing whitespace.\n    2. Converts the string to uppercase.\n    3. Replaces special characters with underscores,\n        (non-alphanumeric, non-percent, non-parentheses, non-underscore).\n    4. Replaces multiple consecutive underscores with a single underscore.\n\n    Args:\n        val (str): The input string to be normalized.\n\n    Returns:\n        str: The normalized string.\n    \"\"\"\n    val = val.strip().upper()\n    # Replace special characters with underscore\n    modified_string = re.sub(r'[^a-zA-Z0-9\\%()_]', '_', val)\n    # Replace multiple consecutive underscores with a single underscore\n    modified_string = re.sub(r'_+', '_', modified_string)\n    return modified_string\n\ndef dynamic_callback(callback, *args):\n    \"\"\"\n    Dynamically calls a callback function with the appropriate number of arguments.\n    This function inspects the signature of the provided callback function to determine\n    the number of parameters it accepts. It then calls the callback with the corresponding\n    number of arguments from the provided *args.\n    Args:\n        callback (Callable): The function to be called.\n        *args: Variable length argument list to be passed to the callback.\n    Returns:\n        The result of the callback function call.\n    Raises:\n        TypeError: If the callback is not callable.\n    \"\"\"\n    sig = inspect.signature(callback)\n    param_count = len(sig.parameters)\n    \n    if param_count == 0:\n        return callback()\n    return callback(*args[:param_count])\n\n\n\nApproved Collective Schemes\nTo get a comprehensive and up to date list of approved collective managers, we crawled Capital Markets Authrity (CMA). They have published a list of approved schemes https://www.cma.or.ke/licensees-market-players/1 and https://licensees.cma.or.ke/licenses/15/2.\n\nScreenshots of the pages\nLets start with some screenshots of the pages\n\nwww.cma.or.ke\n\n\nShow Code\nasync def collective_investment_schemes_click_fn(page: Page):\n    await page.wait_for_selector('ul.module-accordion')\n    elements = await page.query_selector_all('li .accordion-title')\n    for element in elements:\n        text_content = await element.text_content()\n        if 'APPROVED COLLECTIVE INVESTMENT SCHEMES' in text_content:\n            await element.click()\n            accordion_element = await page.wait_for_selector('li.current.builder-accordion-active')\n            await page.evaluate(\"\"\"\n                document.querySelector('#headerwrap').style.display = 'none';\n                document.querySelector('.pojo-a11y-toolbar-toggle').style.display = 'none';\n            \"\"\")\n            await asyncio.sleep(1)\n            return accordion_element\n    print('Element not found')\n\n# Take a screenshot\nawait web_screenshot_async(\n    \"https://www.cma.or.ke/licensees-market-players/\", \n    action = collective_investment_schemes_click_fn,\n    width = 1000, \n    screenshot_options = None,\n    crop_options = { 'bottom': 600, 'right': 600 })\n\n\n\n\n\n\n\n\n\n\n\nlicensees.cma.or.ke\n\n\nShow Code\nasync def collective_investment_schemes_2(page: Page):\n    return await page.query_selector('table')\n\n# Take a screenshot\nawait web_screenshot_async(\n    # Fund manager URL\n    \"https://licensees.cma.or.ke/licenses/15/\", \n    action = collective_investment_schemes_2,\n    width = 2000, \n    screenshot_options = None,\n    crop_options = { 'bottom': 500, 'right': 700 },)\n\n\n\n\n\n\n\n\n\n\n\n\nCrawling\nNext, let’s try grab the schemes table into a dataframe that we can work with. Below is the list of all the certified schemes in Kenya by CMA. 3 4\n\n\nShow Code\ndef extract_collective_scheme_name(para: Tag):\n    full_name = ' '.join([i.get_text(strip=True) for i in para.find_all('strong')])\n    return strip_start_end(full_name)\n\ndef make_collective_unit_obj(tbody_tr_td: Tag) -&gt; collective_scheme_type:\n    return {\n        'Scheme': extract_collective_scheme_name(tbody_tr_td.find('p') or tbody_tr_td),\n        'Funds': [\n            strip_start_end(i.get_text(separator=' ', strip=True)) \n            for i \n            in tbody_tr_td.select('ul li')\n        ]\n    }\n\ndef fetch_collective_schemes_1():\n    CMA_market_players_html: str = urlopen(\"https://www.cma.or.ke/licensees-market-players/\").read()\n    investment_schemes_table_html = BeautifulSoup(CMA_market_players_html, \"html.parser\")\\\n        .find('span', string=\"APPROVED COLLECTIVE INVESTMENT SCHEMES\")\\\n        .find_parent('li')\\\n        .find('table')\n    return [\n        make_collective_unit_obj(tbody_tr_td)\n        for tbody_tr_td \n        in investment_schemes_table_html.select('tbody tr td')\n    ]\n\ndef fetch_collective_schemes_2():\n    CMA_market_players_html: str = urlopen(\"https://licensees.cma.or.ke/licenses/15/\").read()\n    investment_schemes_table_html = BeautifulSoup(CMA_market_players_html, \"html.parser\")\\\n        .find('table')\n    return [\n        make_collective_unit_obj(tbody_tr_td)\n        for tbody_tr_td \n        in investment_schemes_table_html.select('tbody tr &gt; :first-child')\n    ]\n\n# For example: \n#       Orient Umbrella Collective Investment Scheme (formerly Alphafrica Umbrella Fund) =&gt; \n#       Orient Umbrella Collective Investment Scheme\ndef remove_quoted_str(str1: str): return re.sub(r'\\(.*?(?!\\)).*?$', '', str1 or '').strip()\ndef remove_rendadant_words(str1: str):\n    return re.sub(\n        r'\\b(scheme|schemes|trust|trusts|specialized|special|funds|fund|unit|units|collective|investment)\\b\\s*', \n        '', \n        str1 or '',\n        flags=re.IGNORECASE).strip()\ndef remove_special_words(str1: str):\n    return re.sub(\n        r'\\b(specialized|special)\\b\\s*', \n        '', \n        str1 or '',\n        flags=re.IGNORECASE).strip()\n\ndef make_merge_key(str1: str): return hacky_normalizer(remove_rendadant_words(remove_quoted_str(str1)))\n\ndef merge_collective_schemes(schemes_list: list[collective_scheme_type]) -&gt; collective_scheme_type:\n    all_names: dict[str, list[str]] = groupby(\n        make_merge_key, [unit_obj['Scheme'] for unit_obj in schemes_list])\n    all_schemes: dict[str, list[str]] = groupby(\n        make_merge_key, [scheme for unit_obj in schemes_list for scheme in unit_obj['Funds']])\n    return {\n        'Scheme': sorted(\n            [name for values in all_names.values() for name in values], \n            key = lambda x: len(remove_special_words(remove_quoted_str(x))), \n            reverse=True\n        )[0],\n        'Funds': [\n            sorted(schemes, key = lambda x: len(x), reverse=True)[0]\n            for schemes \n            in all_schemes.values()\n        ]\n    }\n\ncollective_schemes_1 = [] # fetch_collective_schemes_1()\ncollective_schemes_2 = fetch_collective_schemes_2()\ncollective_schemes_1_2 = collective_schemes_1 + collective_schemes_2\ncollective_schemes_grouped_by_name = groupby(\n    lambda x: make_merge_key(x['Scheme']), collective_schemes_1_2)\ncollective_schemes = [\n    merge_collective_schemes(collective_schemes) \n    for collective_schemes \n    in collective_schemes_grouped_by_name.values()\n    if len(collective_schemes) &gt; 0 and len(collective_schemes[0]['Scheme']) &gt; 0]\ncollective_schemes_df = pd.DataFrame(collective_schemes)\ncollective_schemes_df\n\n\n\n\n\n\n\n\n\n\nScheme\nFunds\n\n\n\n\n0\nAfrican Alliance Kenya Unit Trust Scheme\n[African Alliance Kenya Money Market Fund (For...\n\n\n1\nBritish-American Unit Trust Scheme\n[Britam Money Market fund (USD), Britam Income...\n\n\n2\nNCBA Unit Trust Funds\n[NCBA Fixed Income Fund, NCBA Equity Fund, NCB...\n\n\n3\nZimele Unit Trust Scheme\n[Zimele Balanced Fund, Zimele Money Market Fun...\n\n\n4\nICEA Unit Trust Scheme\n[ICEA Money Market Fund, ICEA Equity Fund, ICE...\n\n\n5\nCIC Unit Trust Scheme\n[CIC Money Market Fund, CIC Balanced Fund, CIC...\n\n\n6\nMadison Unit Trust Fund\n[Madison Money Market Fund, Madison Fixed Inco...\n\n\n7\nDyer and Blair Unit Trust Scheme\n[Dyer and Blair Diversified Fund, Dyer and Bla...\n\n\n8\nAmana Unit Trust Funds Scheme\n[Amana Money Market Fund, Amana Balanced Fund,...\n\n\n9\nDiaspora Unit Trust Scheme\n[Diaspora Money Market Fund, Diaspora Bond Fun...\n\n\n10\nFirst Ethical Opportunities Fund\n[]\n\n\n11\nGenghis Unit Trust Funds\n[GenCap Hazina Fund (Bond Fund), GenCap Eneza ...\n\n\n12\nMali Money Market Fund\n[]\n\n\n13\nSanlam Unit Trust Scheme\n[Sanlam Money Market Fund, Sanlam Balanced Fun...\n\n\n14\nNabo Africa Funds\n[Nabo Africa Money Market Fund, Nabo Africa Ba...\n\n\n15\nOld Mutual Unit Trust Scheme\n[Old Mutual Equity Fund, Old Mutual Money Mark...\n\n\n16\nEquity Investment Bank Collective Investment S...\n[Equity Investment Bank Money Market Fund, Equ...\n\n\n17\nDry Associates Unit Trust Scheme\n[Dry Associates Money Market Fund (Kenya Shill...\n\n\n18\nCo-op Trust Fund\n[Co-op Balanced Fund, Co-op Equity Fund, Co-op...\n\n\n19\nApollo Unit Trust Scheme\n[Apollo Money Market Fund, Apollo Balanced Fun...\n\n\n20\nCytonn Unit Trust Scheme\n[Cytonn Money Market Fund (USD), Cytonn Balanc...\n\n\n21\nOrient Umbrella Collective Investment Scheme (...\n[Orient Hifadhi Fixed Income Fund (formerly Al...\n\n\n22\nWanafunzi Investment Unit Trust Fund\n[]\n\n\n23\nAbsa Unit Trust Funds\n[Absa Balanced Fund, Absa Bond Fund, Absa Doll...\n\n\n24\nJaza Unit Trust Fund\n[Jaza Premier Money Market Fund]\n\n\n25\nMasaru Unit Trust Scheme\n[Masaru Wealth Management Fund, Masaru Money M...\n\n\n26\nADAM Unit Trust Scheme\n[ADAM Balanced Fund, ADAM Equities Fund, ADAM ...\n\n\n27\nKCB Unit Trust Scheme (formerly Natbank Unit T...\n[KCB Money Market Fund (USD) (formerly Natbank...\n\n\n28\nGenAfrica Unit Trust Scheme\n[GenAfrica Money Market Fund, GenAfrica Equity...\n\n\n29\nAmaka Unit Trust (Umbrella) Scheme\n[Amaka HOSP Fixed Income Fund, Amaka Qualified...\n\n\n30\nJubilee Unit Trust Collective Investment Scheme\n[Jubilee Balanced Fund, Jubilee Equity Fund, J...\n\n\n31\nEnwealth Capital Unit Trust Scheme\n[Enwealth Balanced Fund, Enwealth Equity Fund,...\n\n\n32\nKuza Asset Management Unit Trust Scheme\n[Kuza Fixed Income Fund, Kuza Money Market Fun...\n\n\n33\nEtica Unit Trust Funds which has the following...\n[Etica Money Market Fund(USD), Etica Fixed Inc...\n\n\n34\nLofty Corban Unit Trust Scheme\n[Lofty Corban Equity Fund, Lofty Corban Specia...\n\n\n35\nStandard Investment Trust Funds\n[Standard Investment Equity Growth Fund, Stand...\n\n\n36\nFaida Unit Trust Funds\n[Hazina Bond Fund, Angaza Money Market Fund, O...\n\n\n37\nTaifa Unit Trust Funds\n[Taifa Money Market Fund (KES), Taifa Miney Ma...\n\n\n38\nStanbic Unit Trust Funds\n[Stanbic Money Market Fund, Stanbic Fixed Inco...\n\n\n39\nSpearhead Africa Infrastructure (Special) Fund\n[Spearhead Africa Infrastructure (Special) Fund]\n\n\n40\nRencap Unit Trust Scheme\n[Rencap Money Market Fund(KES), Rencap Balance...\n\n\n41\nMayfair umbrella Collective investment scheme\n[Mayfair Money Market Fund, Mayfair Fixed Inco...\n\n\n42\nInvestcent Partners Trust Fund\n[Investcent Multi Asset Special Fund(KES), Inv...\n\n\n43\nInvestcent Alternative Investment Fund\n[Investcent Hedge Fund (KES)]\n\n\n44\nICEA LION Collective Investment Scheme\n[ICEA LION Money Market Fund, ICEA LION Equity...\n\n\n45\nGCIB Unit Trust Scheme\n[GCIB Money Market Fund, GCIB Fixed Income Fun...\n\n\n46\nCPF Unit Trust Funds\n[CPF Money Market Fund, CPF Bond Fund, CPF USD...\n\n\n47\nArvocap Unit Trust Scheme\n[Arvocap Money Market Fund, Arvocap Ngao Fixed...\n\n\n48\nMyXENO Unit Trust Scheme\n[Xeno Kenya Money Market Fund, Xeno Kenya Bond...\n\n\n49\nVCG Offshore Opportunities Special Fund\n[Offshore Equities Megatrends Special Fund (US...\n\n\n50\nOctagon Unit Trust Scheme\n[Octagon Money Market Fund, Octagon Balanced F...\n\n\n\n\n\n\n\n\nAs of 22 December 2024, there are 49 unique and approved unit trust schemes in Kenya, regulated by the Capital Markets Authority (CMA). The management of these schemes involves a complex ecosystem of financial institutions, each playing a distinct role:\n\nApproved Fund Managers: These are entities specifically licensed by the CMA to manage collective investment schemes. They are responsible for making investment decisions and managing the day-to-day operations of the funds 5.\nInvestment Banks: Investment banks are not the traditional commercial banks, but rather CMA-approved institutions 6 that can engage in activities such as underwriting, market making, and fund management. For example, Genghis Capital Limited is listed by the CMA as an investment bank and manages its own unit trust fund, the Gencap Hela Imara Money Market Fund 7.\nCommercial Banks with Asset Management Arms: Traditional banks may establish separate entities for asset management. For instance, KCB Bank has KCB Asset Management, which is approved by the CMA to manage unit trusts8. Please note that there is also KCB Investment Bank9 10.\nNon-Financial Companies with Investment Products: Some companies outside the traditional financial sector have entered the investment market. A notable example is the Mali Money Market Fund 11 12, owned wholly or in part by Safaricom PLC, Kenya’s largest telecommunications company 13. While Safaricom is not a licensed fund manager, they have partnered with Genghis Capital Limited to administer the Mali MMF 14.\n\nThis complex landscape can sometimes lead to potential conflicts of interest 15. For instance, when an investment bank like Genghis Capital manages both its own funds and third-party funds like the Mali MMF, it raises questions about prioritization and fair treatment of all clients. An additional layer of complexity arises when commercial banks (or their subsidiaries), such as KCB16, offer unit trust investment options alongside traditional savings and fixed deposit accounts. This dual offering presents a potential conflict of interest. Banks typically earn higher profits from traditional deposit accounts compared to the fees generated from managing unit trusts. This raises questions about how banks advise their clients on savings options. While money market funds often provide better returns for savers, banks might have an inherent incentive to promote their own deposit products. This situation underscores the importance of financial literacy and independent advice for consumers navigating these choices. Investors should be aware of this potential conflict and critically evaluate the recommendations they receive, considering whether the advice aligns more with their own financial interests or those of the bank.\nThe investment landscape is constantly evolving, with fund managers occasionally modifying their product offerings. A notable example is Zimele Asset Management’s decision to convert its Money Market Fund into a Fixed Income Fund 17. This transition underscores the fluid nature of investment products in Kenya. Zimele’s clients were not given the option to retain their investments in the original Money Market Fund structure. Some investors expressed dismay at discovering their Money Market Accounts had been altered without their knowledge 18, suggesting a lack of comprehensive public communication about the change. This situation emphasizes the importance of regularly monitoring one’s investments to stay informed about any modifications that may affect them.\nGiven this complexity, potential investors should exercise caution and conduct thorough due diligence before committing their funds. This includes:\n\nVerifying the regulatory status of the fund and its manager with the CMA\nUnderstanding the fund’s investment strategy and associated risks\nReviewing the fund’s performance history and fee structure\nInvestigating any potential conflicts of interest\nSeeking independent financial advice if necessary\n\n\n\n\n\n\n\nWarning\n\n\n\nAlways approach investments with caution, especially when important information is missing, unclear, or overly complicated. Remember that higher returns often come with higher risks, and past performance does not guarantee future results.\n\n\n\n\n\nScheme Performance Data Collection\n\nChallenges in Data Accessibility\nThe Kenyan financial regulatory environment mandates that unit trust schemes publish their daily yields in two national newspapers. However, this requirement presents several challenges for comprehensive data collection and analysis:\n\nLimited Digital Presence: Many newspapers lack a substantial digital archive, necessitating physical access to print copies for data retrieval.\nCost Barriers: Accessing historical data often involves purchasing old newspaper records, making large-scale data collection financially prohibitive.\nTime-Intensive Process: Manually gathering data from physical newspapers is a labor-intensive task, impractical for long-term, comprehensive analysis.\nInconsistent Reporting: Not all fund managers consistently publish their yields, leading to gaps in the data.\n\nThese factors collectively create a significant barrier to accessing and analyzing comprehensive, historical performance data for Kenyan unit trust schemes.\n\n\nCytonn Research: A Valuable Data Source\nIn light of these challenges, Cytonn Fund Managers’ research publications have emerged as an invaluable resource. Since 2014, Cytonn has been conducting and freely publishing market research at https://cytonnreport.com/ 19. Key aspects of this data source include:\n\nComprehensive Coverage: Over 600 reports covering various aspects of the Kenyan financial market.\nHistorical Data: Consistent reporting since 2014, providing a substantial historical dataset.\nFree Access: Public availability of the reports, removing financial barriers to data access.\nAggregated Information: Cytonn’s reports often include compiled data from multiple sources, offering a more comprehensive view of the market.\n\nWhile some fund managers publish current yield data on their websites20, the lack of historical data limits the usefulness of these sources for trend analysis and comprehensive research.\n\n\nData Collection Methodology\nGiven the richness and accessibility of Cytonn’s research, we adopted the following approach for data collection:\n\nWeb Crawling: We developed a crawling mechanism to systematically access reports from both https://cytonn.com/researches 21 and https://cytonnreport.com/research 22.\nEthical Considerations: Our crawling process was designed to respect Cytonn’s server resources, avoiding any disruption to their services.\nData Extraction: We implemented a process to extract relevant tables and data points from each report.\nData Aggregation: The extracted information was compiled into a structured dataset suitable for analysis.\nCompliance with Terms of Service: We carefully reviewed Cytonn’s terms of service to ensure our use of the data aligns with their fair use policy23.\n\n\n\nScreenshots of Cytonn Reports\n\ncytonn.com page\n\n\nShow Code\n# Take a screenshot\nawait web_screenshot_async(\n    \"https://cytonn.com/researches\",\n    width = 1500,\n    height = 1200)\n\n\n\n\n\n\n\n\n\n\n\ncytonnreport.com page\n\n\nShow Code\nasync def cytonnreport_fn(page: Page):\n    await page.wait_for_selector('.grid-x &gt; .pagination')\n    await asyncio.sleep(1)\n\n# Take a screenshot\nawait web_screenshot_async(\n    \"https://cytonnreport.com/research\",\n    action = cytonnreport_fn,\n    width = 1500,\n    height = 1200)\n\n\n\n\n\n\n\n\n\n\n\nMoney Market Fund Yield Table\n\n\nShow Code\n# Define a function that selects a table by its header text\ndef select_table_by_title(target_header_text: str):\n    # Define a nested asynchronous function that takes a Page object as an argument\n    async def fn(page: Page):\n        # Wait for any table element to be present on the page\n        await page.wait_for_selector('table')\n        # Query and collect all table elements on the page\n        table_elements = await page.query_selector_all('table')\n        # Iterate through each table element\n        for table_element in table_elements:\n            # Query and collect all header cells in the current table\n            table_headers = await table_element.query_selector_all('thead tr td')\n            # Iterate through each header cell\n            for table_header in table_headers:\n                # Extract the text content of the current header cell\n                header_text:str = await page.evaluate('(element) =&gt; element.textContent', table_header)\n                # Check if the header text starts with the target text\n                if header_text.startswith(target_header_text):\n                    # If a match is found, return the current table element\n                    await asyncio.sleep(1)\n                    return table_element\n    return fn\n\nawait web_screenshot_async(\n    # URL to take a screenshot of\n    \"https://cytonnreport.com/research/cytonn-monthly-\",\n    # Action deciding WHAT (element) or WHEN (eg: click) to take the screenshot\n    action = select_table_by_title('Cytonn Report: Money Market Fund Yield'),\n    width = 500, \n    crop_options = { 'bottom': 600 })\n\n\n\n\n\n\n\n\n\n\n\n\nCrawling\nAt https://cytonn.com/researches 24, we can crawl and parse HTML, but it could be very slow. We notice that https://cytonnreport.com/research 25, the exact same data is displayed, but using a background request, https://cytonnreport.com/get/allreports. We can use this to crawl multiple reports faster.\n\n\nShow Code\nasync def get_all_cytonn_reports(per_page_count: int = 10):\n    \"\"\"\n    Retrieves all Cytonn reports from the Cytonn Report website.\n\n    Args:\n        per_page_count (int, optional): The number of reports to retrieve per page. Defaults to 10.\n\n    Returns:\n        list: A list of all the retrieved reports.\n    \"\"\"\n    page, close_playwright = await get_browser_page_async()\n    reports_url = \"https://cytonnreport.com/get/allreports\"\n    reports_headers: dict = None\n    reports_method: str = None\n\n    async def handle_route(route: Route):\n        nonlocal reports_headers\n        nonlocal reports_method\n        reports_headers = route.request.headers.copy()\n        reports_method = route.request.method\n        await route.continue_()\n\n    async def get_cytonn_reports(current_page: int):\n        javascript_fetch_fn = f'''\n            async () =&gt; {{\n                try {{\n                    const response = await fetch(\n                        \"{reports_url}\", \n                        {{\n                            \"headers\": {json.dumps(reports_headers)},\n                            \"method\": \"{reports_method}\",\n                            \"body\": {json.dumps(json.dumps(\n                                {\n                                    \"pagination\": {\n                                        \"per_page\": per_page_count, \n                                        \"current_page\": current_page\n                                    }\n                                }))},\n                            \"referrer\": \"https://cytonnreport.com/research\",\n                            \"referrerPolicy\": \"no-referrer-when-downgrade\",\n                            \"mode\": \"cors\",\n                            \"credentials\": \"include\"\n                        }});\n                    if (!response.ok) {{\n                        throw new Error(`HTTP error! status: ${{response.status}}`);\n                    }}\n                    const json = await response.json();\n                    return json;\n                }} catch (error) {{\n                    console.error('Fetch error:', error);\n                    throw error; // Re-throw to allow calling code to handle it\n                }}\n            }}\n        '''\n        return await page.evaluate(javascript_fetch_fn)\n\n    await page.route(reports_url, handle_route)\n\n    # Navigate to the desired URL\n    await page.goto(\"https://cytonnreport.com/research\")\n    while not reports_headers:\n        await asyncio.sleep(1)\n    current_page = 1\n    all_reports = []\n    pbar: tqdm = None\n    while True:\n        reports_response = await get_cytonn_reports(current_page)\n        reports = reports_response['data'] if reports_response else []\n        if len(reports) &gt; 0:\n            total = reports_response['total']\n            pbar = pbar or tqdm(total=total)\n            pbar.update(len(reports))\n            all_reports.extend(reports)\n            last_page = reports_response['last_page']\n            if last_page == current_page:\n                break\n            current_page += 1\n        else:\n            break\n        await asyncio.sleep(0.4)\n    await close_playwright()\n    if pbar:\n        pbar.close()\n    return all_reports\n\nall_cytonn_reports = await get_all_cytonn_reports()\nprint(f'There are {len(all_cytonn_reports)} reports')\n\n\n100%|██████████| 664/664 [03:03&lt;00:00,  3.63it/s]\n\n\nThere are 664 reports\n\n\n\n\n\n\n\nShow Code\n# converting the JSON into dataframe\nall_cytonn_reports_df = pd.DataFrame(all_cytonn_reports)\nall_cytonn_reports_df.head(3)\n\n\n\n\n\n\n\n\n\n\nid\nslug\nname\nauthor\nresearchdate\nactive\ncreated_by\nseries_id\ncampaign\nsent\n...\ntitle\ncategory\nsummary\nbody\ndate_holder\ndate\ncreator\nstatus\nseries\ntopics\n\n\n\n\n0\n748\nnairobi-metropolitan-area-9\nNairobi Metropolitan Area (NMA) Infrastructure...\nInvestments Team\n2024-12-22\n1\n95\n1\n2671\n1\n...\nNairobi Metropolitan Area (NMA) Infrastructure...\nInvestment Reports\n&lt;p&gt;&lt;strong&gt;Money Markets, T-Bills Primary Auct...\n&lt;p&gt;&lt;strong&gt;Money Markets, T-Bills Primary Auct...\n22\n22 December, 2024\nFredrick Maore\nActive\n{'id': 1, 'name': 'Cytonn Weekly', 'descriptio...\n[{'id': 2528, 'name': 'Fixed Income', 'slug': ...\n\n\n1\n747\nkenyas-listed-banks-q3-2024-report-and-cytonn-...\nKenya’s Listed Banks Q3’2024 Report, & Cytonn ...\nCytonn Research\n2024-12-15\n1\n89\n1\n2665\n1\n...\nKenya’s Listed Banks Q3’2024 Report, & Cytonn ...\nInvestment Reports\n&lt;p&gt;&lt;strong&gt;Investment Updates:&lt;/strong&gt;&lt;/p&gt;\\r\\...\n&lt;p&gt;&lt;strong&gt;Investment Updates:&lt;/strong&gt;&lt;/p&gt;\\r\\...\n15\n15 December, 2024\nJames Kirira\nActive\n{'id': 1, 'name': 'Cytonn Weekly', 'descriptio...\n[{'id': 2523, 'name': 'Company updates', 'slug...\n\n\n2\n746\nretirement-benefit-schemes\nRetirement Benefit Schemes Q3’2024 Performance...\nResearch Team\n2024-12-08\n1\n92\n1\n2659\n1\n...\nRetirement Benefit Schemes Q3’2024 Performance...\nInvestment Reports\n&lt;p&gt;&lt;strong&gt;Investment Updates:&lt;/strong&gt;&lt;/p&gt;\\r\\...\n&lt;p&gt;&lt;strong&gt;Investment Updates:&lt;/strong&gt;&lt;/p&gt;\\r\\...\n8\n8 December, 2024\nEdna Dande\nActive\n{'id': 1, 'name': 'Cytonn Weekly', 'descriptio...\n[{'id': 2518, 'name': 'Company updates', 'slug...\n\n\n\n\n\n3 rows × 30 columns\n\n\n\nAs can be observed, the dataset above is a bit complex and diffucult to uderstand or analyze. This is because alot of information is contained in the reports.\n\n\nShow Code\n# This cofirms all the records have are unique\nlen(all_cytonn_reports_df), len(all_cytonn_reports), len(all_cytonn_reports_df['id'].unique())\n\n\n(664, 664, 664)"
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#explore-and-clean-the-dataset",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#explore-and-clean-the-dataset",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Explore and Clean the Dataset",
    "text": "Explore and Clean the Dataset\nThe goal here is to extract the details of Effective/Norminal/Annual Rate of Money Market Funds (KES) and Assets Under Management for the entire schemes\n\nPreview the Columns\n\n\nShow Code\nall_cytonn_reports_df.iloc[0]\n\n\nid                                                                748\nslug                                      nairobi-metropolitan-area-9\nname                Nairobi Metropolitan Area (NMA) Infrastructure...\nauthor                                               Investments Team\nresearchdate                                               2024-12-22\nactive                                                              1\ncreated_by                                                         95\nseries_id                                                           1\ncampaign                                                         2671\nsent                                                                1\nthumbnail                                                        None\ncreated_at                                        2024-12-22 12:07:19\nmeta_title                                                       None\nmeta_keywords                                                    None\nmeta_description                                                 None\nog_description                                                   None\nurl                 https://cytonnreport.com/research/nairobi-metr...\nog_image            https://cytonnreport.com/storage/research/tmpp...\nupdated_at                                        2024-12-22 13:20:13\ndeleted_at                                                       None\ntitle               Nairobi Metropolitan Area (NMA) Infrastructure...\ncategory                                           Investment Reports\nsummary             &lt;p&gt;&lt;strong&gt;Money Markets, T-Bills Primary Auct...\nbody                &lt;p&gt;&lt;strong&gt;Money Markets, T-Bills Primary Auct...\ndate_holder                                                        22\ndate                                                22 December, 2024\ncreator                                                Fredrick Maore\nstatus                                                         Active\nseries              {'id': 1, 'name': 'Cytonn Weekly', 'descriptio...\ntopics              [{'id': 2528, 'name': 'Fixed Income', 'slug': ...\nName: 0, dtype: object\n\n\nBelow is a tree structure of one record, to visualize the objects and their inner properties\n\n\nShow Code\njson_structure = json2txttree(all_cytonn_reports[:1])\nmin_topics = min(len(i.get('topics', [])) for i in all_cytonn_reports)\nmax_topics = max(len(i.get('topics', [])) for i in all_cytonn_reports)\njson_structure = json_structure.replace('└─  (array)', f'└─  (array) [{len(all_cytonn_reports)} items]')\njson_structure = json_structure.replace('\"topics\" (array)', f'\"topics\" (array) [between {min_topics} - {max_topics} items]')\n\nprint(json_structure)\n\n\n└─  (array) [664 items]\n   └─  (object)\n      ├─ \"id\" (number)\n      ├─ \"slug\" (string)\n      ├─ \"name\" (string)\n      ├─ \"author\" (string)\n      ├─ \"researchdate\" (string)\n      ├─ \"active\" (number)\n      ├─ \"created_by\" (number)\n      ├─ \"series_id\" (number)\n      ├─ \"campaign\" (string)\n      ├─ \"sent\" (number)\n      ├─ \"thumbnail\" (number)\n      ├─ \"created_at\" (string)\n      ├─ \"meta_title\" (number)\n      ├─ \"meta_keywords\" (number)\n      ├─ \"meta_description\" (number)\n      ├─ \"og_description\" (number)\n      ├─ \"url\" (string)\n      ├─ \"og_image\" (string)\n      ├─ \"updated_at\" (string)\n      ├─ \"deleted_at\" (number)\n      ├─ \"title\" (string)\n      ├─ \"category\" (string)\n      ├─ \"summary\" (string)\n      ├─ \"body\" (string)\n      ├─ \"date_holder\" (number)\n      ├─ \"date\" (string)\n      ├─ \"creator\" (string)\n      ├─ \"status\" (string)\n      ├─ \"series\" (object)\n      │  ├─ \"id\" (number)\n      │  ├─ \"name\" (string)\n      │  ├─ \"description\" (string)\n      │  ├─ \"thumbnail\" (string)\n      │  ├─ \"created_by\" (number)\n      │  ├─ \"category_id\" (number)\n      │  ├─ \"created_at\" (string)\n      │  ├─ \"updated_at\" (string)\n      │  └─ \"category\" (object)\n      │     ├─ \"id\" (number)\n      │     ├─ \"slug\" (string)\n      │     ├─ \"name\" (string)\n      │     ├─ \"created_at\" (string)\n      │     └─ \"updated_at\" (string)\n      └─ \"topics\" (array) [between 0 - 8 items]\n         └─  (object)\n            ├─ \"id\" (number)\n            ├─ \"name\" (string)\n            ├─ \"slug\" (string)\n            ├─ \"title\" (number)\n            ├─ \"summary\" (string)\n            ├─ \"body\" (string)\n            ├─ \"research_id\" (number)\n            ├─ \"active\" (number)\n            ├─ \"topical\" (number)\n            ├─ \"type\" (number)\n            ├─ \"weight\" (number)\n            ├─ \"created_by\" (number)\n            ├─ \"created_at\" (string)\n            └─ \"updated_at\" (string)\n\n\n\nA full report is formed by articles. Each topics is a subsection, with title being the header and body being the content. We will merge all bodies from the articles to form the entire report HTML, which we will parse to extract the Money Market Funds yields tables. In addition, we are also going to add the main body and main summary and topics summary to encure we capture any table we might miss.\n\n\nShow Code\nCYTONN_RECORD_LITERALS = Literal['summary', 'body', 'topics', 'researchdate']\ndef get_report_HTML(report: dict[CYTONN_RECORD_LITERALS, Any]) -&gt; str:\n    summary_html = report['summary']\n    body_html = report['body']\n    topics_html = ''.join([f\"{i['summary']} \\n\\n {i['body']}\" for i in report['topics']])\n    return f\"{summary_html} \\n {body_html} \\n {topics_html}\"\n\n# from IPython.display import HTML\n# HTML(get_report_HTML(all_cytonn_reports[0]))\n\n\n\n\nParsing Dates\nThere are some summary tables that have dates such as Q1'2023, Q1'2023 (%), FY'2023, FY'2023 (%), Q1'2024, Q1'2024 (%)\n\n\nShow Code\nawait web_screenshot_async(\n    # URL to take a screenshot of\n    \"https://cytonnreport.com/research/q1-2024-unit-trust-funds-performance-note\",\n    # Action deciding WHAT (element) or WHEN (eg: click) to take the screenshot\n    action = select_table_by_title('Cytonn Report: Assets Under Management (AUM) for the Approved Collective Investment Schemes'),\n    width = 700, \n    screenshot_options = None,\n    crop_options = { 'bottom': 400 })\n\n\n\n\n\n\n\n\n\nBelow function will help parse such time ranges:\n\n\nShow Code\ndef get_date_range(month, year):\n    # Convert month name to number\n    month_num = [i.lower() for i in month_abbr].index(month.lower())\\\n        if len(month) &gt; 0 and any([i.lower() == month.lower() for i in month_abbr])\\\n        else datetime.strptime(month, '%B').month\n    # Get the last day of the month\n    _, last_day = monthrange(int(year), month_num)\n    # Create date objects for the first and last day of the month\n    start_date = date(int(year), month_num, 1)\n    end_date = date(int(year), month_num, last_day)\n    return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\nparse_date_pattern_months = (\n    \"JAN|JANUARY|FEB|FEBRUARY|MAR|MARCH|APR|APRIL|MAY|\"\n    \"JUN|JUNE|JUL|JULY|AUG|AUGUST|SEP|SEPTEMBER|\"\n    \"OCT|OCTOBER|NOV|NOVEMBER|DEC|DECEMBER\"\n)\nparse_date_pattern = rf'(?:(\\d{{2}})[_|\\s|-]*)?({parse_date_pattern_months})[_|\\s|-]*(\\d{{4}})'\n\ndef parse_fiscal_period_dates(date_string: str) -&gt; (tuple[str, str] | None):\n    \"\"\"\n    This function parses a date string representing a fiscal period \n    (Fiscal/Financial Year, Quarter, or Half-year), year, or date and returns the corresponding \n    start and end dates.\n    \"\"\"\n    extracted_date = re.search('^' + parse_date_pattern + '$', date_string, re.IGNORECASE)\n    if extracted_date:\n        search_date, search_month, search_year = extracted_date.groups()\n        if search_date:\n            month_num = datetime.strptime(search_month, '%B').month\n            return datetime(int(search_year), month_num, int(search_date)).strftime('%Y-%m-%d')\n        else:\n            return get_date_range(search_month, search_year)\n    if re.match(r\"^\\d{4}$\", date_string, re.IGNORECASE):\n        date_string = f\"FY'{date_string}\"\n    # Define a regex pattern to match fiscal periods (FY, Q1-Q4, H1-H2) followed by a year, or just an year\n    pattern = r\"^(FY|Q[1-4]|H[1-2])['|_|\\s]*(\\d{4})$\"\n    # Try to match the input string against the pattern\n    match = re.match(pattern, date_string, re.IGNORECASE)\n    # If no match is found, return None\n    if not match:\n        return None\n    # Extract the period type and year from the match\n    period, year = match.groups()\n    year = int(year)\n    # Handle Fiscal Year (FY) case\n    if period.upper() == 'FY':\n        start_date = datetime(year, 1, 1)\n        end_date = datetime(year, 12, 31)\n    # Handle Quarter (Q1-Q4) cases\n    elif period.upper().startswith('Q'):\n        quarter = int(period[1])\n        start_month = (quarter - 1) * 3 + 1\n        start_date = datetime(year, start_month, 1)\n        # Calculate end date of the quarter\n        end_date = start_date.replace(month=start_month + 2) + timedelta(days=32)\n        end_date = end_date.replace(day=1) - timedelta(days=1)\n    # Handle Half-year (H1-H2) cases\n    elif period.upper().startswith('H'):\n        half = int(period[1])\n        start_month = (half - 1) * 6 + 1\n        start_date = datetime(year, start_month, 1)\n        # Calculate end date of the half-year\n        end_date = start_date.replace(month=start_month + 5) + timedelta(days=32)\n        end_date = end_date.replace(day=1) - timedelta(days=1)\n    # Return start and end dates formatted as strings\n    return (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n\ndef TEST_parse_fiscal_period_dates():\n    # Test the function\n    test_dates = [\n        \"FY'2019\", \"Q1'2020\", \"H1'2019\", \"fy 2018\", \"q32021\", \"h2_2022\", \"2020\", '2019',\n        'JUNE_2020', '01_NOVEMBER_2017', \"H3'2020\"\n    ]\n    for expanding_value in test_dates:\n        result = parse_fiscal_period_dates(expanding_value)\n        if result:\n            print(f\"{expanding_value}: {result}\")\n        else:\n            print(f\"{expanding_value}: Invalid format\")\nTEST_parse_fiscal_period_dates()\n\n\nFY'2019: ('2019-01-01', '2019-12-31')\nQ1'2020: ('2020-01-01', '2020-03-31')\nH1'2019: ('2019-01-01', '2019-06-30')\nfy 2018: ('2018-01-01', '2018-12-31')\nq32021: ('2021-07-01', '2021-09-30')\nh2_2022: ('2022-07-01', '2022-12-31')\n2020: ('2020-01-01', '2020-12-31')\n2019: ('2019-01-01', '2019-12-31')\nJUNE_2020: ('2020-06-01', '2020-06-30')\n01_NOVEMBER_2017: 2017-11-01\nH3'2020: Invalid format\n\n\n\n\nParsing a Effective Annual Rate(KES Money Market Fund) and Total Assets Under Management (Collective Investment Schemes)\nThe Extracted_Scheme_Entry class below represents and validates a financial record entry. It validates record type (Assets Under Management or Effective Annual Rate), date, value, and scheme. The class also maintains lists of non-existent schemes and invalid records. Assets under management (AUM) is the market value of the investments managed by the fund manager on behalf of clients, inluding MMF, FIXED, balanced, equity, etc.\n\n\nShow Code\nclass Extracted_Scheme_Entry:\n    \"\"\"\n    A class to represent and validate financial entry information.\n    \n    Class Attributes:\n    INVALID_FUNDS (list[str]): Stores funds not found in the mapping.\n    INVALID_DATES (list[str]): Stores entry dates not valid.\n    INVALID_VALUES (list[str]): Stores entry values not valid.\n    TYPE_ASSETS_UNDER_MANAGEMENT (str): Constant for Assets Under Management type.\n    TYPE_EFFECTIVE_ANNUAL_RATE (str): Constant for Effective Annual Rate type.\n    \"\"\"\n    INVALID_SCHEMES: list[str] = []\n    INVALID_DATES: list[str] = []\n    INVALID_VALUES: list[str] = []\n    TYPE_ASSETS_UNDER_MANAGEMENT: str = 'ASSETS_UNDER_MANAGEMENT' # Assets Under Management\n    TYPE_EFFECTIVE_ANNUAL_RATE: str = 'EFFECTIVE_ANNUAL_RATE' # Effective Annual Rate\n\n    def __init__(self, \n                 entry_type: Literal['ASSETS_UNDER_MANAGEMENT', 'EFFECTIVE_ANNUAL_RATE'], \n                 entry_date: str, \n                 entry_value: str, \n                 entry_scheme: str,\n                 scheme_filter_function: Callable[[str], list[str]]):\n        \"\"\"\n        Initialize a RecordEntry instance with validated attributes.\n        \n        Args:\n        entry_type (str): Type of the record (TYPE_ASSETS_UNDER_MANAGEMENT or TYPE_EFFECTIVE_ANNUAL_RATE).\n        entry_date (str): Date of the record (2024-03-01) or Financial period (H1'2024).\n        entry_value (str): Value of the record.\n        entry_scheme (str): Name of the MMF(KES) fund\n        fund_manager_filter_predicate (Callable): A predicate to filter and return matched MMF(KES) fund for validation.\n        \"\"\"\n        self.entry_type = Extracted_Scheme_Entry.validate_type(entry_type)\n        self.entry_date = Extracted_Scheme_Entry.validate_date(entry_date)\n        self.entry_value = Extracted_Scheme_Entry.validate_value(entry_value)\n        self.entry_scheme = Extracted_Scheme_Entry.validate_scheme(entry_scheme, scheme_filter_function)\n        if self.entry_date is None:\n            Extracted_Scheme_Entry.INVALID_DATES.append(entry_date)\n        if self.entry_value is None:\n            Extracted_Scheme_Entry.INVALID_VALUES.append(entry_value)\n        if self.entry_scheme is None:\n            Extracted_Scheme_Entry.INVALID_SCHEMES.append(entry_scheme)\n\n    def is_valid(self) -&gt; bool:\n        \"\"\"\n        Check if the record is valid (all attributes are non-empty).\n        \"\"\"\n        is_valid = \\\n            bool(self.entry_type) \\\n            and bool(self.entry_date) \\\n            and bool(self.entry_value) \\\n            and bool(self.entry_scheme)\n        return is_valid\n\n    @staticmethod\n    def validate_scheme(value: str, filter_predicate: Callable[[str], list[str]]) -&gt; str|None:\n        \"\"\"\n        Validate and standardize the date or financial period.\n        \"\"\"\n        try:\n            value = str(value or '').lower()\n            # These represent USD MMF's\n            EXCLUDES = ['Dollar', 'USD']\n            is_USD_MMF = any((exclude.lower() in value) for exclude in EXCLUDES)\n            if not is_USD_MMF:\n                names = filter_predicate(value)\n                if len(names) == 1:\n                    return names[0]\n                if len(names) &gt; 1:\n                    print(f'\"{value}\" has more that two matches! {names}')\n            return None\n        except:\n            return None\n    \n    @staticmethod\n    def validate_date(value: str) -&gt; str|tuple[str, str]|None:\n        \"\"\"\n        Validate and standardize the date\n        \"\"\"\n        try:\n            return parse_fiscal_period_dates(value)\\\n                    or datetime.strptime(value, \"%Y-%m-%d\").strftime('%Y-%m-%d')\\\n                    or None\n        except:\n            return None\n\n    @staticmethod\n    def validate_value(value: str|float) -&gt; str|None:\n        \"\"\"\n        Validate and clean the entry value.\n        \"\"\"\n        try:\n            if type(value) == float:\n                    return value\n            # remove percentage sign\n            value = value.rstrip('%')\n            # remove comma and white space\n            value = ''.join([i for i in value if i not in [' ', ',', '-']])\n            return float(value) if len(value) &gt; 0 else None\n        except:\n            return None\n\n    @staticmethod\n    def validate_type(value: str) -&gt; Literal['ASSETS_UNDER_MANAGEMENT', 'EFFECTIVE_ANNUAL_RATE']:\n        \"\"\"\n        Validate the record type.\n        \n        Args:\n        value (str): The record type to validate.\n        \n        Raises:\n        TypeError exception.\n        \"\"\"\n        value = (value or '').upper()\n        if value in [Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE]:\n            return value\n        raise TypeError(f\"{value} is not proper entry Type!\")\n    \ndef TEST_MoneyMarketFund_KES_RecordEntry():\n    # Test the class\n    test_cases = [\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"2024-03-01\",\n            \"entry_value\": \"1,000,000\",\n            \"entry_scheme\": \"britam\",\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE,\n            \"entry_date\": \"H1'2024\",\n            \"entry_value\": \"5.5%\",\n            \"entry_scheme\": \"old mutual\",\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"invalid-date\",\n            \"entry_value\": \"1,000,000\",\n            \"entry_scheme\": \"sanlam\",\n            \"invalid\": \"invalid date\"\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"2024-03-01\",\n            \"entry_value\": \"invalid-value\",\n            \"entry_scheme\": \"britam\",\n            \"invalid\": \"invalid value\"\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"2024-03-01\",\n            \"entry_value\": \"1,000,000\",\n            \"entry_scheme\": \"unknown scheme\",\n            \"invalid\": \"unmapped scheme\"\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"2024-03-01\",\n            \"entry_value\": \"1,000,000\",\n            \"entry_scheme\": \"britam sanlam\",\n            \"invalid\": \"2 funds matched\"\n        },\n        {\n            \"entry_type\": Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT,\n            \"entry_date\": \"2024-03-01\",\n            \"entry_value\": \"1,000,000\",\n            \"entry_scheme\": \"Britam USD Dollar Fund\",\n            \"invalid\": \"USD MMF\"\n        },\n    ]\n    # Define the fund filter function\n    test_fund_map = [\n        (\n            'Britam MMF(KES)',\n            ['britam', 'british-american', 'british', 'american']\n        ),\n        (\n            'UAP Old Mutual MMF(KES)',\n            ['old mutual', 'uap old mutual', 'uap']\n        ),\n        (\n            'Sanlam MMF(KES)',\n            ['sanlam', 'sanlam investments']\n        )\n    ]\n\n    def test_fund_filter(value: str):\n        value = value.lower()\n        names = [name for name, aliases in test_fund_map if any(alias in value for alias in aliases)]\n        return names\n\n    # Run tests\n    for test_case in test_cases:\n        entry = Extracted_Scheme_Entry(\n            test_case[\"entry_type\"],\n            test_case[\"entry_date\"],\n            test_case[\"entry_value\"],\n            test_case[\"entry_scheme\"],\n            test_fund_filter\n        )\n        cases = [entry.entry_date, entry.entry_value, entry.entry_scheme]\n        invalid = f\" ({test_case.get('invalid')})\" if test_case.get('invalid') else ''\n        print(f\"Valid: {entry.is_valid()}{invalid}, {cases}\")\n\n    # Print invalid entries\n    print(\"\\nInvalid Funds:\", Extracted_Scheme_Entry.INVALID_SCHEMES)\n    print(\"\\nInvalid Dates:\", Extracted_Scheme_Entry.INVALID_DATES)\n    print(\"\\nInvalid Values:\", Extracted_Scheme_Entry.INVALID_VALUES)\n    Extracted_Scheme_Entry.INVALID_SCHEMES = []\n    Extracted_Scheme_Entry.INVALID_DATES = []\n    Extracted_Scheme_Entry.INVALID_VALUES = []\nTEST_MoneyMarketFund_KES_RecordEntry()\n\n\nValid: True, ['2024-03-01', 1000000.0, 'Britam MMF(KES)']\nValid: True, [('2024-01-01', '2024-06-30'), 5.5, 'UAP Old Mutual MMF(KES)']\nValid: False (invalid date), [None, 1000000.0, 'Sanlam MMF(KES)']\nValid: False (invalid value), ['2024-03-01', None, 'Britam MMF(KES)']\nValid: False (unmapped scheme), ['2024-03-01', 1000000.0, None]\n\"britam sanlam\" has more that two matches! ['Britam MMF(KES)', 'Sanlam MMF(KES)']\nValid: False (2 funds matched), ['2024-03-01', 1000000.0, None]\nValid: False (USD MMF), ['2024-03-01', 1000000.0, None]\n\nInvalid Funds: ['unknown scheme', 'britam sanlam', 'Britam USD Dollar Fund']\n\nInvalid Dates: ['invalid-date']\n\nInvalid Values: ['invalid-value']\n\n\nWe then create a fund collective schemes map with a tuple of name and aliases because the records don’t have a simple or stardard naming in the Cytonn reports. As such, we need to use very unique and simple names that we can use to match abitrary Money market funds names from the crawled data.\n\n\nShow Code\nSCHEME_NAME_ALIAS_MAP = [\n    # The African Alliance (AA) Kenya Shillings Fund is a money market fund by \n    # African Alliance Kenya Investment Bank Limited (the fund manager) \n    # launched on 27th April 2015.\n    # https://centwarrior.com/aa-kenya-shillings-fund/\n    # https://www.linkedin.com/posts/centwarrior_aa-kenya-shillings-fund-explained-in-2024-activity-7169322082814705664-8nwu?utm_source=share&utm_medium=member_desktop\n    # https://cytonn.com/topicals/investment-risk-analysis\n    (\n        'African Alliance Kenya Unit Trust Scheme', \n        ['african', 'alliance', 'aa kenya']\n    ),\n    (\n        'British-American Unit Trust Scheme', \n        ['britam', 'british-american', 'british', 'american']\n    ),\n    (\n        'NCBA Unit Trust Funds', \n        ['ncba', 'cba', 'commercial bank of africa']\n    ),\n    (\n        'Zimele Unit Trust Scheme', \n        ['zimele']\n    ),\n    (\n        'ICEA Unit Trust Scheme', \n        ['icea']\n    ),\n    (\n        'Standard Investment Trust Funds', \n        ['standard', 'mansa']\n    ),\n    (\n        'CIC Unit Trust Scheme', \n        ['cic']\n    ),\n    (\n        'Madison Unit Trust Fund', \n        ['Madison', 'madisson']\n    ),\n    (\n        'Dyer and Blair Unit Trust Scheme', \n        ['dyer', 'blair']\n    ),\n    (\n        'Amana Unit Trust Funds Scheme', \n        ['amana']\n    ),\n    (\n        'Diaspora Unit Trust Scheme', \n        ['diaspora']\n    ),\n    (\n        'First Ethical Opportunities Fund', \n        ['ethical', \n         # 'first', 'opportunities'\n        ]\n    ),\n    # https://www.cma.or.ke/licensees-market-players/\n    # https://genghis-capital.com/asset-management/money-market-fund/\n    (\n        'Genghis Unit Trust Funds', \n        ['hela','genghis', 'hazina', 'hisa', 'iman', 'gencap', 'compliant', 'eneza', 'genCap', 'imara']\n    ),\n    # https://www.businessdailyafrica.com/bd/markets/capital-markets/safaricom-s-mali-unit-trust-asset-base-hits-sh1-4bn--4582142\n    (\n        'Mali Money Market Fund', \n        ['mali']\n    ),\n    # https://www.safaricom.co.ke/main-mpesa/m-pesa-services/wealth/ziidi\n    # https://www.cma.or.ke/cma-approves-ziidi-money-market-fundfrom-safaricom-plc/\n    (\n        'Ziidi Money Market Fund',\n        ['ziidi']\n    ),\n    # https://www.cma.or.ke/cma-approves-establishment-of-new-unit-trust-sub-funds/\n    (\n        'Gulfcap Money Market Fund',\n        ['gulfcap']\n    ),\n    (\n        'Sanlam Unit Trust Scheme', \n        ['sanlam']\n    ),\n    (\n        'Nabo Africa Funds', \n        ['nabo']\n    ),\n    (\n        'Old Mutual Unit Trust Scheme', \n        ['mutual', 'old', 'Faulu']\n    ),\n    # https://equitygroupholdings.com/ke/investor-relations/eib\n    # https://www.cma.or.ke/licensees-market-players/\n    (\n        'Equity Investment Bank Collective Investment Scheme', \n        ['equity']\n    ),\n    # https://www.cma.or.ke/licensees-market-players/\n    (\n        'Dry Associates Unit Trust Scheme', \n        ['dry associates', 'dry', 'associates']\n    ),\n    (\n        'Co-op Trust Fund', \n        ['co-op', 'gratuity', 'Coop']\n    ),\n    (\n        'Apollo Unit Trust Scheme', \n        ['aggressive', 'apollo']\n    ),\n    (\n        'Cytonn Unit Trust Scheme', \n        ['cytonn']\n    ),\n    (\n        'Orient Umbrella Collective Investment Scheme (formerly Alphafrica Umbrella Fund)', \n        ['orient', 'kasha', 'alpha', 'alphafrica']\n    ),\n    (\n        'Wanafunzi Investment Unit Trust Fund', \n        ['wanafunzi']\n    ),\n    (\n        'Absa Unit Trust Funds', \n        ['absa']\n    ),\n    (\n        'Jaza Unit Trust Fund', \n        ['jaza']\n    ),\n    (\n        'Masaru Unit Trust Scheme', \n        ['masaru']\n    ),\n    (\n        'ADAM Unit Trust Scheme', \n        ['adam']\n    ),\n    (\n        'KCB Unit Trust Scheme (formerly Natbank Unit Trust Scheme)', \n        ['kcb', 'natbank']\n    ),\n    (\n        'GenAfrica Unit Trust Scheme', \n        ['genafrica']\n    ),\n    (\n        'Amaka Unit Trust (Umbrella) Scheme', \n        ['amaka']\n    ),\n    (\n        'Jubilee Unit Trust Collective Investment Scheme', \n        ['jubilee']\n    ),\n    # Previusly \"Liberty Pension Services Limited\"\n    # https://enwealth.co.ke/about/#governance\n    # https://www.linkedin.com/company/enwealth-kenya/?originalSubdomain=ke\n    # https://enwealth.co.ke/capital/enwealth-money-market-fund/\n    (\n        'Enwealth Capital Unit Trust Scheme', \n        ['enwealth']\n    ),\n    (\n        'Kuza Asset Management Unit Trust Scheme', \n        ['kuza', 'momentum']\n    ),\n    # https://www.linkedin.com/company/arvocap-asset-managers/\n    # https://www.businessdailyafrica.com/bd/markets/avocarp-latest-to-enter-kenya-s-asset-management-market-4644586\n    (\n        'Arvocap Unit Trust Scheme', \n        ['arvocap']\n    ),\n    (\n        'Etica Capital Limited', \n        ['etica']\n    ),\n    # https://licensees.cma.or.ke/licenses/15/\n    (\n        'Mayfair umbrella Collective investment scheme', \n        ['mayfair']\n    ),\n    (\n        'Lofty Corban Unit Trust Scheme', \n        ['lofty-corban', 'lofty', 'corban']\n    ),\n    (\n        'CPF Unit Trust Funds', \n        ['cpf', 'cpof']\n    ),\n    (\n        'Stanbic Unit Trust Funds', \n        ['stanbic']\n    ),\n    (\n        'MyXENO Unit Trust Scheme',\n        ['myxeno']\n    ),\n    #############################################\n    ##### UNVERIFIED COLLECTIVE INVESTMENTS #####\n    #############################################\n    (\n        'Metropolitan Canon Asset Managers Limited',\n        ['metropolitan']\n    ),\n    (\n        'FCB Capital Limited',\n        ['fcb']\n    ),\n    (\n        'Fusion Investment Management Limited',\n        ['fusion']\n    ),\n    (\n        'Altree Capital Kenya Limited',\n        ['altree']\n    ),\n    (\n        'CFS Asset Management Limited',\n        ['cfs']\n    ),\n    (\n        'I&M Capital Limited',\n        ['i&m']\n    ),\n    (\n        'Globetec Asset Managers Limited',\n        ['globetec']\n    ),\n    (\n        # https://cytonnreport.com/research/cytonn-q3-2024-markets-review\n        # https://www.businessdailyafrica.com/bd/markets/capital-markets/fintech-start-up-ndovu-gets-cma-nod-to-set-up-money-market-fund-4548970\n        'Waanzilishi Capital Limited',\n        ['waanzilishi', 'ndovu']\n    ),\n    (\n        'Star Capital Management Limited',\n        ['star']\n    ),\n    # Unverified and NO online presense!\n    (\n        'Stanlib Kenya',\n        ['stanlib']\n    ),\n    \n]\nSCHEME_NAME_ALIAS_MAP\n\n\n\n\nShow Code\ndef scheme_filter(value: str) -&gt; list[str]:\n    value = value.lower()\n    names = [\n        name \n        for name, aliases\n        in SCHEME_NAME_ALIAS_MAP if any((alias.lower() in value) for alias in aliases)\n    ]\n    return names\n\n# Test\ndef TEST_scheme_filter(fund_manager: str):\n    name = scheme_filter(fund_manager)\n    print(f\"{fund_manager} =&gt; {name}\")\nTEST_scheme_filter('KCB Fund Managers')\nTEST_scheme_filter('Cytonn Fund Mangers')\nTEST_scheme_filter('Nabo')\nTEST_scheme_filter('madison')\n\n\nOne report can have more than one table, see: https://cytonnreport.com/research/unit-trust-fund-performance-q3-1. The list below contains a tuple of:\n\nA list if table names in the records. For matching, the table names are normalized with the below function:\n\n\n\nShow Code\ndef normalize_and_compare_two_strs(str1: str, str2:str) -&gt; bool:\n    if not str1 or not str2:\n        return False\n    str1 = str1.strip().upper()\n    str2 = str2.strip().upper()\n    return str1 == str2 or hacky_normalizer(str1) == hacky_normalizer(str2)\n\n# Test\ndef TEST_normalize_and_match_two_strs():\n    str1_str2 = [\n        ('q2’2020-aum(kshs-mns)', 'Q2 2020_AUM(KSHs_MNs)'),\n        ('q2’2020-aum(kshs-mns)', \"Q2_2020_AUM(KSHs_MNs)\"),\n        ('', \"\"),\n        (None, None),\n        ('no.', 'NO.')\n    ]\n    for str1, str2 in str1_str2:\n        is_match = normalize_and_compare_two_strs(str1, str2)\n        print(f'IS_MATCH={is_match}; {[str1, str2]}')\n\nTEST_normalize_and_match_two_strs()\n\n\nIS_MATCH=True; ['q2’2020-aum(kshs-mns)', 'Q2 2020_AUM(KSHs_MNs)']\nIS_MATCH=True; ['q2’2020-aum(kshs-mns)', 'Q2_2020_AUM(KSHs_MNs)']\nIS_MATCH=False; ['', '']\nIS_MATCH=False; [None, None]\nIS_MATCH=True; ['no.', 'NO.']\n\n\n\nA list of functions that process the matched tables. Each function should essentially process a single column. The function receives three parameters: a dataframe row data (a table row entry), the entire record from which the table was extracted from, and any other_params. This first parameter is useful to capture the entry value, and the second is important to capture the date of the record if not provided in the table. The table row entry values are a dictionally named with the table names used to match the tables.  Callback function returns a Extracted_Scheme_Entry\n\n\n\nShow Code\nremove_KSH_MNS = lambda str1: re.sub(r'(?:_AUM)?_?\\(KSHS?_MNS?\\)', '', str1, flags=re.IGNORECASE)\nremove_MONEY_MARKET_FUND_KSHS_MNS = lambda str1: re.sub(r'_?MONEY_MARKET_FUNDS?_?\\(KSHS?_MNS?\\)', '', str1, flags=re.IGNORECASE)\nremove_EFFECTIVE_ANNUAL = lambda str1: re.sub(r'_AVERAGE_EFFECTIVE_ANNUAL_YIELD_P_A_|EFFECTIVE_ANNUAL_RATE_|AVERAGE_|\\(|\\)', '', str1, flags=re.IGNORECASE) \nremove_MONEY_MARKET_FUND_KSHS_MNS_2= lambda str1: re.sub(r'_?MONEY_MARKET_FUNDS?_AUM_?\\(KSHS?_MNS?\\)|_?MONEY_MARKET_FUNDS?_?\\(KSHS?_MNS?\\)', '', str1, flags=re.IGNORECASE)\nEXTRACTION_MAP: list[\n    tuple[\n        list[str], \n        list[Callable[[dict[str, Any], dict[CYTONN_RECORD_LITERALS, Any]], Extracted_Scheme_Entry]]\n    ]] = [\n    (\n        [\n            'RANK', \n            'FUND_MANAGER', \n            'DAILY_YIELD', \n            'EFFECTIVE_ANNUAL_RATE'\n        ], \n        [\n            # Effective Annual Rate and Daily Yield\n            # https://cytonnreport.com/research/cytonn-monthly-october-2021\n            # https://cytonnreport.com/research/potential-effects-covid-19\n            lambda row, record: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                record['researchdate'], \n                row['EFFECTIVE_ANNUAL_RATE'], \n                row['FUND_MANAGER'],\n                scheme_filter),\n            lambda row, record: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                record['researchdate'], \n                row['DAILY_YIELD'], \n                row['FUND_MANAGER'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: normalized in ['RANK', hacky_normalizer('NO.')]\n            },\n            {\n                'column_name': 'SCHEME',\n                'predicate': lambda normalized: bool(re.match(r'MONEY_MARKET_FUNDS?|FUND_MANAGER', normalized))\n            },\n            {\n                'column_name': 'EFFECTIVE_ANNUAL',\n                'predicate': lambda normalized: normalized in ['EFFECTIVE_ANNUAL', 'EFFECTIVE_ANNUAL_RATE']\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/kenyas-fy2024-2025-budget\n            # https://cytonnreport.com/research/nairobi-metropolitan-area-serviced-apartments-report-2021\n            # https://cytonnreport.com/research/cytonn-monthly-may-2024\n            # https://cytonnreport.com/research/q12023-unit-trust-funds-performance-cytonn-monthly-july-2023\n            # https://cytonnreport.com/research/investing-in-unit\n            lambda row, record: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                record['researchdate'], \n                row['EFFECTIVE_ANNUAL'], \n                row['SCHEME'],\n                scheme_filter)\n        ]\n    ),\n    (\n        [\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: normalized in ['RANK', hacky_normalizer('NO.')]\n            },\n            {\n                'column_name': 'SCHEME',\n                'predicate': lambda normalized: bool(re.match(r'MONEY_MARKET_FUNDS?|FUND_MANAGER', normalized))\n            },\n            {\n                'column_name': 'EFFECTIVE_ANNUAL',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_EFFECTIVE_ANNUAL(normalized))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: remove_EFFECTIVE_ANNUAL(normalized)\n                }\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/unit-trust-fund-performance-q3-1\n            # https://cytonnreport.com/research/fy2019-utf-performance\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                other_params['DATE_1'], \n                row['EFFECTIVE_ANNUAL'], \n                row['SCHEME'],\n                scheme_filter)\n        ]\n    ),\n    (\n        [\n            {\n                'column_name': 'SCHEME',\n                'predicate': lambda normalized: 'AVERAGE' in normalized,\n            },\n            {\n                'column_name': 'DATE_1_EAR',\n                'predicate': lambda normalized: bool(re.match(r\"^\\d{4}$\", normalized)),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: normalized.strip()\n                }\n            },\n            {\n                'column_name': 'DATE_2_EAR',\n                'predicate': lambda normalized: bool(re.search(parse_date_pattern, normalized)),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: re.search(parse_date_pattern, normalized).group().replace('_', ' ')\n                }\n            },\n            {\n                'column_name': 'DATE_3_EAR',\n                'predicate': lambda normalized: bool(re.search(parse_date_pattern, normalized)),\n                'other_params': \n                {\n                    'DATE_3': lambda normalized: re.search(parse_date_pattern, normalized).group().replace('_', ' ')\n                }\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/cmmf-fact-sheet-june-2020\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                other_params['DATE_1'], \n                row['DATE_1_EAR'], \n                row['SCHEME'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                other_params['DATE_2'], \n                row['DATE_2_EAR'], \n                row['SCHEME'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE, \n                other_params['DATE_3'], \n                row['DATE_3_EAR'], \n                row['SCHEME'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            'NO.', \n            'UNIT_TRUST_FUND_MANAGER', \n            'AUM', \n            '%_OF_MARKET_SHARE'\n        ], \n        [\n            # https://cytonnreport.com/research/investment-options-in-kenyan-market\n            lambda row, record: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                record['researchdate'], \n                row['AUM'], \n                row['UNIT_TRUST_FUND_MANAGER'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            'NO.',\n            'FUND_MANAGERS',\n            {\n                'column_name': 'DATE_1_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(normalized.replace('_AUM(KSHS_MNS)', ''))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: normalized.replace('_AUM(KSHS_MNS)', '')\n                }\n            },\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: 'MARKET_SHARE' in normalized\n            },\n            {\n                'column_name': 'DATE_2_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(normalized.replace('_AUM(KSHS_MNS)', ''))),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: normalized.replace('_AUM(KSHS_MNS)', '')\n                }\n            },\n            {\n                'column_name': 'UNWANTED_2',\n                'predicate': lambda normalized: 'MARKET_SHARE' in normalized\n            },\n            {\n                'column_name': 'UNWANTED_3',\n                'predicate': lambda normalized: 'AUM_GROWTH' in normalized\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/unit-trust-funds-performance-q2-2020\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'], \n                row['DATE_1_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_2'], \n                row['DATE_2_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            'NO.',\n            'FUND_MANAGERS',\n            {\n                'column_name': 'DATE_1_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_MONEY_MARKET_FUND_KSHS_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: remove_MONEY_MARKET_FUND_KSHS_MNS(normalized)\n                }\n            },\n            {\n                'column_name': 'DATE_2_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_MONEY_MARKET_FUND_KSHS_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: remove_MONEY_MARKET_FUND_KSHS_MNS(normalized)\n                }\n            },\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: '_MARKET_SHARE' in normalized\n            },\n            {\n                'column_name': 'UNWANTED_2',\n                'predicate': lambda normalized: '_MARKET_SHARE' in normalized,\n                'optional': True\n            },\n            {\n                'column_name': 'UNWANTED_3',\n                'predicate': lambda normalized: 'VARIANCE' in normalized,\n                'optional': True\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/fy2019-utf-performance\n            # https://cytonnreport.com/research/investing-in-unit\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'], \n                row['DATE_1_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_2'], \n                row['DATE_2_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            'NO.',\n            {\n                'column_name': 'SCHEME',\n                'predicate': lambda normalized: normalized in ['FUND_MANAGERS', 'FUND_MANAGER']\n            },\n            {\n                'column_name': 'DATE_1_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_KSH_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: remove_KSH_MNS(normalized)\n                }\n            },\n            {\n                'column_name': 'DATE_2_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_KSH_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: remove_KSH_MNS(normalized)\n                }\n            },\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: 'GROWTH' in normalized\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/unit-trust-funds-perfomance-q1-2020-cytonn-weekly\n            # https://cytonnreport.com/research/unit-trust-funds-performance\n            # https://cytonnreport.com/research/fy2019-utf-performance\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_2'], \n                row['DATE_1_AUM'], \n                row['SCHEME'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_2'], \n                row['DATE_2_AUM'], \n                row['SCHEME'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: bool(normalized in [hacky_normalizer('#'), hacky_normalizer('NO.')])\n            },\n            'FUND_MANAGERS',\n            {\n                'column_name': 'DATE_1_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized)\n                }\n            },\n            {\n                'column_name': 'DATE_2_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized))),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized)\n                }\n            },\n            {\n                'column_name': 'DATE_3_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized))),\n                'other_params': \n                {\n                    'DATE_3': lambda normalized: remove_MONEY_MARKET_FUND_KSHS_MNS_2(normalized)\n                }\n            },\n            {\n                'column_name': 'UNWANTED_2',\n                'predicate': lambda normalized: bool('ANNUALIZED' in normalized)\n            },\n        ], \n        [\n            # https://cytonnreport.com/research/options-for-your-pension\n            # https://cytonnreport.com/research/cytonn-monthly-august-2019\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'],\n                row['DATE_1_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'],\n                row['DATE_2_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'],\n                row['DATE_3_AUM'], \n                row['FUND_MANAGERS'],\n                scheme_filter),\n        ]\n    ),\n    (\n        [\n            {\n                'column_name': 'UNWANTED_1',\n                'predicate': lambda normalized: 'NO' in normalized,\n            },\n            {\n                'column_name': 'SCHEME',\n                'predicate': lambda normalized: normalized in ['COLLECTIVE_INVESTMENT_SCHEMES', 'FUND_MANAGERS']\n            },\n            {\n                'column_name': 'DATE_1_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_KSH_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_1': lambda normalized: remove_KSH_MNS(normalized)\n                }\n\n            },\n            {\n                'column_name': 'UNWANTED_2',\n                'predicate': lambda normalized: bool(\n                    parse_fiscal_period_dates(normalized.replace('MARKET_SHARE', ''))),\n\n            },\n            {\n                'column_name': 'DATE_2_AUM',\n                'predicate': lambda normalized: bool(parse_fiscal_period_dates(remove_KSH_MNS(normalized))),\n                'other_params': \n                {\n                    'DATE_2': lambda normalized: remove_KSH_MNS(normalized)\n                }\n\n            },\n            {\n                'column_name': 'UNWANTED_3',\n                'predicate': lambda normalized: bool(\n                    parse_fiscal_period_dates(normalized.replace('MARKET_SHARE', ''))),\n\n            },\n            {\n                'column_name': 'UNWANTED_4',\n                'predicate': lambda normalized: 'AUM_GROWTH' in normalized,\n            }\n        ], \n        [\n            # https://cytonnreport.com/research/unit-trust-fund-performance-q3-1\n            # https://cytonnreport.com/research/unit-trust-funds-performance-cytonn-monthly-july-2022\n            lambda row, _, other_params : Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_1'], \n                row['DATE_1_AUM'], \n                row['SCHEME'],\n                scheme_filter),\n            lambda row, _, other_params: Extracted_Scheme_Entry(\n                Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT, \n                other_params['DATE_2'], \n                row['DATE_2_AUM'], \n                row['SCHEME'],\n                scheme_filter),\n        ]\n    )\n]\n\n\n\n\nShow Code\ndef compare_table_and_given_column_names(\n        given_table_columns, \n        header_column_names, \n        *, \n        use_optional_columns = True):\n    table_columns = [\n        table_column \n        for table_column \n        in given_table_columns \n        if type(table_column) == str \n            or use_optional_columns \n            or not table_column.get('optional', False)\n    ]\n    table_column_strs = [\n        table_column if type(table_column) == str else table_column['column_name']\n        for table_column \n        in table_columns\n    ]\n    is_match_new = len(header_column_names) == len(table_columns) and \\\n        all([\n                normalize_and_compare_two_strs(header_column_name, table_column) \n                    if type(table_column) == str \n                    else table_column['predicate'](hacky_normalizer(header_column_name))\n                for header_column_name, table_column \n                in zip(header_column_names, table_columns)\n            ])\n    if is_match_new or not use_optional_columns:\n        other_params = {\n            other_param_key: other_param_value(hacky_normalizer(header_column_name))\n            for header_column_name, table_column \n            in zip(header_column_names, table_columns)\n            if type(table_column) != str and table_column.get('other_params', None)\n            for other_param_key, other_param_value\n            in table_column['other_params'].items()\n        } if is_match_new else { }\n        return is_match_new, table_column_strs, other_params\n    return compare_table_and_given_column_names(\n        given_table_columns, \n        header_column_names, \n        use_optional_columns = False)\n\ndef get_table(table: Tag, extraction_map):\n    for tag in table.find_all(True):\n        tag.attrs = {} # remove tags such as colspan and rowspan\n    # Iterate through predefined extraction mappings\n    for (given_table_columns, extractor_callbacks) in extraction_map:\n        clean_up_tasks: list[Callable[[], None]] = []\n        header_tr_s: list[Tag] = table.select('thead tr')\n        is_match = False\n        is_match_columns = []\n        is_match_other_params = {}\n        # Check if table headers match the expected columns\n        for header_tr in header_tr_s:\n            header_column_names: list[str] = [i.get_text(strip=True) for i in header_tr.find_all('td')]\n            is_match_new, pure_str_columns, other_params = compare_table_and_given_column_names(\n                given_table_columns, \n                header_column_names)\n            if is_match_new:\n                is_match_columns = pure_str_columns\n                is_match_other_params = other_params or {}\n            # If not a match, add to cleanup tasks.\n            # We add cleap tasks here to delay deleting the table headers before we decide \n            # that this table is matched. When the given columns are matched, the other columns\n            # are deleted to ensure the dataframe has one column.\n            if not is_match_new:\n                clean_up_tasks.append(header_tr.extract)\n            is_match = is_match or is_match_new\n        # If a match is found, process the table\n        if is_match:\n            try:\n                # Execute cleanup tasks\n                [clean_up_task() for clean_up_task in clean_up_tasks]\n                # Convert table to DataFrame\n                table_df = pd.read_html(io.StringIO(str(table)))[0]\n                table_df.columns = is_match_columns\n                return (table_df, extractor_callbacks, is_match_other_params)\n            except Exception as e:\n                print('error', e, table)\n                continue\n    return (None, None, None)\n\ndef is_valid_dataframe(df: pd.DataFrame | None) -&gt; bool:\n    return df is not None and not df.empty\n\nDEBUG_OPTIONS = dict[\n    Literal[\n        'log_unmatched_table', \n        'log_invalid_columns', \n        'log_extracted_valid',\n        'log_extracted_invalid',\n        'log_extractor_count',\n    ], \n    Callable[[str], None]\n]\n\ndef get_tables(html: str, extraction_map, *, debug_options: DEBUG_OPTIONS = {}):\n    log_unmatched_table = debug_options.get('log_unmatched_table')\n    # Parse the HTML content using BeautifulSoup\n    parsed_html = BeautifulSoup(html, \"html.parser\")\n    # Find all &lt;table&gt; elements in the parsed HTML and store them in a list\n    # remove duplicates\n    tables: list[Tag] = list({ hacky_normalizer(str(table)): table for table in parsed_html.find_all('table')}.values())\n    # Iterate over each table found in the HTML\n    for table in tables:\n        # Generate a DataFrame and a list of extractor callbacks for each table\n        table_df, extractor_callbacks, other_params = get_table(copy(table), extraction_map)\n        # Check if the DataFrame is valid and not None\n        if is_valid_dataframe(table_df) and table_df is not None:\n            # Yield the DataFrame and the associated callbacks\n            yield (table_df, extractor_callbacks, other_params)\n        else:\n            if log_unmatched_table:\n                log_unmatched_table(str(table))\n\ndef extract_frame_by_column_names(\n        report: dict[CYTONN_RECORD_LITERALS, str], \n        tables_html: str,\n        extraction_map,\n        *, \n        debug_options: DEBUG_OPTIONS = {},\n        ):\n    log_invalid_columns = debug_options.get('log_invalid_columns')\n    log_extractor_count = debug_options.get('log_extractor_count')\n    log_extracted_valid = debug_options.get('log_extracted_valid')\n    log_extracted_invalid = debug_options.get('log_extracted_invalid')\n    # Generate tables and callbacks using the get_tables function\n    table__callback__generator = get_tables(tables_html, extraction_map, debug_options=debug_options)\n    # Iterate over each table DataFrame and its extractor callbacks\n    for table_df, extractor_callbacks, other_params in table__callback__generator:\n        if log_extractor_count:\n            log_extractor_count(len(extractor_callbacks))\n        if len(extractor_callbacks) &gt; 0:\n            # Apply each callback function to the rows of the table\n            for extractor_callback in extractor_callbacks:\n                table_rows = [\n                    dynamic_callback(extractor_callback, raw_table_row.to_dict(), report, other_params)\n                    for _, raw_table_row \n                    in table_df.iterrows()\n                ]\n                # Convert the processed rows into a new DataFrame\n                extracted_df = pd.DataFrame([vars(i) for i in table_rows if i.is_valid()])\n                # Check if the extracted DataFrame is valid and yield it\n                if is_valid_dataframe(extracted_df):\n                    if log_invalid_columns:\n                        __invalid_columns = [vars(i) for i in table_rows if not i.is_valid()]\n                        if len(__invalid_columns) &gt; 0:\n                            log_invalid_columns(pd.DataFrame(__invalid_columns))\n                    if log_extracted_valid:\n                        log_extracted_valid(extracted_df)\n                    yield extracted_df\n                elif log_extracted_invalid:\n                    log_extracted_invalid(table_df)\n\ndef extract_table_by_column_names(\n        report: dict[CYTONN_RECORD_LITERALS, str], \n        *, \n        debug_options: DEBUG_OPTIONS = {}):\n    # Get the HTML content of the report\n    report_html = get_report_HTML(report)\n    yield from extract_frame_by_column_names(\n        report,\n        report_html, \n        EXTRACTION_MAP,\n        debug_options = debug_options)\n        \n\n\n\n\nValidating the Parsing\nBelow code extracts and parses entries from all the records and stores various metrics for validating.\n\n\nShow Code\n# Stores a tuple of the index of the record, the key for the log and the value logged\ndebug_log_store: list[tuple[str, str, Any]] = []\ndef extract_all_records():\n    for index, report in tqdm(all_cytonn_reports_df.iterrows(), total=len(all_cytonn_reports_df)):\n        debug_options = {\n            'log_unmatched_table': \n                lambda value_str: debug_log_store.append((index, 'log_unmatched_table', value_str)),\n            'log_invalid_columns': \n                lambda value_df: debug_log_store.append((index, 'log_invalid_columns', value_df)),\n            'log_extracted_valid': \n                lambda value_tuple_df: debug_log_store.append((index, 'log_extracted_valid', value_tuple_df)),\n            'log_extracted_invalid': \n                lambda value_tuple_df: debug_log_store.append((index, 'log_extracted_invalid', value_tuple_df)),\n            'log_extractor_count': \n                lambda value_int: debug_log_store.append((index, 'log_extractor_count', value_int)),\n        }\n        yield from extract_table_by_column_names(report, debug_options = debug_options)\n\nextracted_records_df = pd.concat(objs = extract_all_records(), ignore_index = True)\n\nextracted_records_df\n\n\n  0%|          | 0/664 [00:00&lt;?, ?it/s]100%|██████████| 664/664 [01:16&lt;00:00,  8.69it/s]\n\n\n\n\n\n\n\n\n\n\nentry_type\nentry_date\nentry_value\nentry_scheme\n\n\n\n\n0\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n17.1\nCytonn Unit Trust Scheme\n\n\n1\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.6\nLofty Corban Unit Trust Scheme\n\n\n2\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.3\nGulfcap Money Market Fund\n\n\n3\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.2\nOrient Umbrella Collective Investment Scheme (...\n\n\n4\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.1\nEtica Capital Limited\n\n\n...\n...\n...\n...\n...\n\n\n5174\nEFFECTIVE_ANNUAL_RATE\n(2018-01-01, 2018-12-31)\n11.5\nCytonn Unit Trust Scheme\n\n\n5175\nEFFECTIVE_ANNUAL_RATE\n(2018-01-01, 2018-12-31)\n10.2\nNabo Africa Funds\n\n\n5176\nEFFECTIVE_ANNUAL_RATE\n(2018-01-01, 2018-12-31)\n10.1\nCIC Unit Trust Scheme\n\n\n5177\nEFFECTIVE_ANNUAL_RATE\n(2018-01-01, 2018-12-31)\n9.9\nMadison Unit Trust Fund\n\n\n5178\nEFFECTIVE_ANNUAL_RATE\n(2018-01-01, 2018-12-31)\n9.9\nZimele Unit Trust Scheme\n\n\n\n\n\n5179 rows × 4 columns\n\n\n\n\nValidating Extraction Process\n\n\nShow Code\n(len(Extracted_Scheme_Entry.INVALID_SCHEMES), \nlen(Extracted_Scheme_Entry.INVALID_DATES), \nlen(Extracted_Scheme_Entry.INVALID_VALUES))\n\n\n(81, 0, 97)\n\n\nINVALID_SCHEMES\n\n\nShow Code\nnp.unique(Extracted_Scheme_Entry.INVALID_SCHEMES)\n\n\narray(['Absa Money Market Fund USD', 'Average',\n       'Average of Top 5 Money Market Funds',\n       'Benchmark ( Average 91 day T- Bill + 1.0% point)',\n       'Benchmark (Average 182 day T- Bill + 5.0% points)',\n       'CIC Dollar Fund', 'Cytonn Money Market Fund USD',\n       'Dry Associates Money Market Fund USD', 'Industrial Average',\n       'Industry average', 'KCB Money Market Fund USD',\n       'Kuza Money Market Fund USD', 'Lofty-Corban Money Market Fund USD',\n       'Nabo Africa Money Market Fund USD',\n       'Old Mutual Dollar Money Market Fund', 'Sanlam Dollar Fund',\n       'Total', 'Weighted Average Growth', 'nan'], dtype='&lt;U49')\n\n\n\n\nShow Code\nall(\n    re.match(r\".*(total|usd|average|dollar).*|nan\", str(i), flags=re.IGNORECASE) \n    for i \n    in Extracted_Scheme_Entry.INVALID_SCHEMES)\n\n\nTrue\n\n\nINVALID_VALUES\n\n\nShow Code\nnp.unique(Extracted_Scheme_Entry.INVALID_VALUES).tolist()\n\n\n['-']\n\n\nLets look at unmatched tables\n\n\nShow Code\ngrouped_by_key_id = groupby(lambda x: (x[0], x[1]), debug_log_store)\n\n\n\n\nShow Code\nfilter_exclude_names = [\n    'bank', 'insurance', 'pension', 'Equities', 'Balance', 'ratios', 'Supermarket',\n    'Budget', 'NSSF', 'Bamburi', 'Metropolitan', 'Land', 'REIT', 'Debt', 'Demand', \n    'Retail', 'Sahara', 'USD', 'Residential', 'Apartment', 'Credit', 'Hospitality', \n    'Thematic', 'Company', 'Commercial', 'Office', 'Business', 'Macro', 'Affordable',\n    'Housing', 'Government', 'Industry', 'NPL', 'GDP', 'Urban', 'Nairobi', 'County',\n    'Inflation', 'Commodity', 'Price', 'Academia', 'School', 'Fixed Income', 'EPS Growth',\n    'Rental', 'Life Assurance', 'Loan', 'Education', 'Energy', 'Tourism', 'Restaurant',\n    'Building', 'Estates', 'Kuramo', 'communications@cytonn.com', 'Fitch Rating', \n    'Minority', 'Europe', 'Geothermal', 'Tanzania', 'Uganda', 'Occupancy', 'Savings Account', \n    'Filimbi', 'Undisclosed', 'Sharehold', 'Foreign Direct Investment', 'IEBC', \n    'Key Performance Indicator', 'hotels', 'Cytonn High Yield Fund', 'years old', 'West Africa'\n]\ndef table_has_extractable_schemes(tables_str_value: str):\n    table_tag = BeautifulSoup(tables_str_value, \"html.parser\")\n    tbody = str(table_tag.find('tbody')).lower()\n    thead = str(table_tag.find('thead')).lower()\n    exclude_names = [i.lower() for i in filter_exclude_names]\n    if not any(i in thead or i in tbody for i in exclude_names):\n        for _, aliases in SCHEME_NAME_ALIAS_MAP:\n            for alias in aliases:\n                if alias.lower() in tbody:\n                    return True\n    return False\n\nunmatched_tables = [\n    (index, table)\n    for (index, key), tables \n        in tqdm(grouped_by_key_id.items())\n        if key == 'log_unmatched_table' \n    for (_,_,table) \n        in tables\n        if table_has_extractable_schemes(table)\n]\nlen(unmatched_tables)\n\n\n100%|██████████| 1030/1030 [00:19&lt;00:00, 53.44it/s]\n\n\n0\n\n\nWe are able to extract all tables with schemes that are in KES money market funds, and assets under management."
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#preview-the-data",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#preview-the-data",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Preview the data",
    "text": "Preview the data\n\nExpand date range\n\n\nShow Code\ndef expand_date_column(df: pd.DataFrame, expand_column: str):\n    for _, row in df.iterrows():\n        expanding_values = row[expand_column]\n        if type(expanding_values) in [list, tuple] :\n            start_date = datetime.strptime(expanding_values[0], \"%Y-%m-%d\")\n            end_date = datetime.strptime(expanding_values[1], \"%Y-%m-%d\")\n            start_end_diff_days = (end_date - start_date).days\n            day_list = [\n                (start_date + timedelta(days=i)).strftime('%Y-%m-%d') \n                for i \n                in range(start_end_diff_days + 1)\n            ]\n            for day in day_list:\n                yield { **row.to_dict(), expand_column: day }\n        else:\n            yield row.to_dict()\n\nexpanded_records_df = pd.DataFrame(expand_date_column(extracted_records_df, 'entry_date'))\nexpanded_records_df['entry_date'] = pd.to_datetime(expanded_records_df['entry_date'])\nexpanded_records_df\n\n\n\n\n\n\n\n\n\n\nentry_type\nentry_date\nentry_value\nentry_scheme\n\n\n\n\n0\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n17.1\nCytonn Unit Trust Scheme\n\n\n1\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.6\nLofty Corban Unit Trust Scheme\n\n\n2\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.3\nGulfcap Money Market Fund\n\n\n3\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.2\nOrient Umbrella Collective Investment Scheme (...\n\n\n4\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n16.1\nEtica Capital Limited\n\n\n...\n...\n...\n...\n...\n\n\n132059\nEFFECTIVE_ANNUAL_RATE\n2018-12-27\n9.9\nZimele Unit Trust Scheme\n\n\n132060\nEFFECTIVE_ANNUAL_RATE\n2018-12-28\n9.9\nZimele Unit Trust Scheme\n\n\n132061\nEFFECTIVE_ANNUAL_RATE\n2018-12-29\n9.9\nZimele Unit Trust Scheme\n\n\n132062\nEFFECTIVE_ANNUAL_RATE\n2018-12-30\n9.9\nZimele Unit Trust Scheme\n\n\n132063\nEFFECTIVE_ANNUAL_RATE\n2018-12-31\n9.9\nZimele Unit Trust Scheme\n\n\n\n\n\n132064 rows × 4 columns\n\n\n\n\n\nCheck duplicates\n\n\nShow Code\nexpanded_records_df[expanded_records_df.duplicated()]\n\n\n\n\n\n\n\n\n\n\nentry_type\nentry_date\nentry_value\nentry_scheme\n\n\n\n\n1656\nEFFECTIVE_ANNUAL_RATE\n2024-06-16\n16.4\nNabo Africa Funds\n\n\n2456\nEFFECTIVE_ANNUAL_RATE\n2024-03-10\n17.7\nEtica Capital Limited\n\n\n2509\nEFFECTIVE_ANNUAL_RATE\n2024-02-25\n16.1\nGenAfrica Unit Trust Scheme\n\n\n2532\nEFFECTIVE_ANNUAL_RATE\n2024-02-18\n17.0\nLofty Corban Unit Trust Scheme\n\n\n2631\nEFFECTIVE_ANNUAL_RATE\n2024-01-21\n17.3\nEtica Capital Limited\n\n\n...\n...\n...\n...\n...\n\n\n130234\nASSETS_UNDER_MANAGEMENT\n2019-07-28\n20270.8\nCIC Unit Trust Scheme\n\n\n130235\nASSETS_UNDER_MANAGEMENT\n2019-07-28\n8841.6\nBritish-American Unit Trust Scheme\n\n\n130236\nASSETS_UNDER_MANAGEMENT\n2019-07-28\n6578.8\nOld Mutual Unit Trust Scheme\n\n\n130237\nASSETS_UNDER_MANAGEMENT\n2019-07-28\n6951.9\nICEA Unit Trust Scheme\n\n\n130238\nASSETS_UNDER_MANAGEMENT\n2019-07-28\n5189.7\nNCBA Unit Trust Funds\n\n\n\n\n\n45206 rows × 4 columns\n\n\n\n\n\nShow Code\n# Remove the duplicates\nexpanded_records_df = expanded_records_df.drop_duplicates()\n\n\n\n\nSort\n\n\nShow Code\nexpanded_records_df = expanded_records_df.sort_values(by='entry_date')\nexpanded_records_df\n\n\n\n\n\n\n\n\n\n\nentry_type\nentry_date\nentry_value\nentry_scheme\n\n\n\n\n83624\nEFFECTIVE_ANNUAL_RATE\n2017-11-01\n11.1\nCytonn Unit Trust Scheme\n\n\n125347\nASSETS_UNDER_MANAGEMENT\n2018-01-01\n256.6\nApollo Unit Trust Scheme\n\n\n130969\nEFFECTIVE_ANNUAL_RATE\n2018-01-01\n10.1\nCIC Unit Trust Scheme\n\n\n97161\nASSETS_UNDER_MANAGEMENT\n2018-01-01\n19756.7\nCIC Unit Trust Scheme\n\n\n124442\nASSETS_UNDER_MANAGEMENT\n2018-01-01\n5837.0\nNCBA Unit Trust Funds\n\n\n...\n...\n...\n...\n...\n\n\n28\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n12.2\nZiidi Money Market Fund\n\n\n29\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n11.8\nStanbic Unit Trust Funds\n\n\n30\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n8.5\nEquity Investment Bank Collective Investment S...\n\n\n15\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n13.9\nDry Associates Unit Trust Scheme\n\n\n0\nEFFECTIVE_ANNUAL_RATE\n2024-12-22\n17.1\nCytonn Unit Trust Scheme\n\n\n\n\n\n86858 rows × 4 columns\n\n\n\n\n\nPloting\n\n\nShow Code\ngrouped_df = expanded_records_df.groupby(\n    ['entry_type', 'entry_date', 'entry_scheme'])['entry_value'].mean().reset_index()\ngrouped_df\n\n\n\nEffective Annual Rate, Money Market Funds (KES)\n\n\nShow Code\nEAR_df = grouped_df[\n    grouped_df['entry_type'] == Extracted_Scheme_Entry.TYPE_EFFECTIVE_ANNUAL_RATE\n    ].drop(columns=['entry_type']).copy()\nEAR_pivot = EAR_df.pivot(index='entry_date', columns='entry_scheme', values='entry_value')\nEAR_pivot\n\n\n\n\nShow Code\nEAR_fig = px.line(EAR_pivot, x=EAR_pivot.index, y=EAR_pivot.columns)\nEAR_fig.update_layout(\n    height=800,\n    margin=dict(t=100),\n    title=dict(\n        text=\"Effective Annual Rate (Percentage)\",  # Your title here\n        y=0.98,                   # Adjust the title's vertical position\n        x=0.5,                    # Center the title\n        xanchor='center',\n        yanchor='top'\n    ),\n    xaxis=dict(\n        side=\"top\",    # This moves the x-axis to the top\n        title=\"Date\"   # This sets the title for the x-axis\n    ),\n    yaxis=dict(\n        title=\"Effective Annual Rate\"   # This sets the title for the x-axis\n    ),\n\n    legend=dict(\n        orientation=\"h\",  # horizontal orientation\n        yanchor=\"bottom\",\n        y=-4.5,  # move the legend below the plot\n        xanchor=\"center\",\n        x=0.5\n    ))\nEAR_fig.update_traces(\n    hovertemplate=\"&lt;br&gt;\".join([\n        \"scheme=%{fullData.name}\",\n        \"date=%{x|%Y-%m-%d}\",\n        \"annual_rate=%{y}%\",\n        # removes any additional trace information that Plotly might add by default.\n        \"&lt;extra&gt;&lt;/extra&gt;\"\n    ])\n)\nEAR_fig.show()\n\n\n                                                \nInteractive plot of Effective Annual Rate (Percentage)\n\n\n\n\n\n\n\nAssets Under Management (Millions - KES)\n\n\nShow Code\nAUM_df = grouped_df[\n    grouped_df['entry_type'] == Extracted_Scheme_Entry.TYPE_ASSETS_UNDER_MANAGEMENT\n    ].drop(columns=['entry_type']).copy()\nAUM_pivot = AUM_df.pivot(index='entry_date', columns='entry_scheme', values='entry_value')\nAUM_pivot\n\n\n\n\nShow Code\nAUM_fig = px.line(AUM_pivot, x=AUM_pivot.index, y=AUM_pivot.columns)\nAUM_fig.update_layout(\n    height=800,\n    margin=dict(t=100),\n    title=dict(\n        text=\"Assets Under Management (KSH Millions)\",  # Your title here\n        y=0.98,                   # Adjust the title's vertical position\n        x=0.5,                    # Center the title\n        xanchor='center',\n        yanchor='top'\n    ),\n    xaxis=dict(\n        side=\"top\",    # This moves the x-axis to the top\n        title=\"Date\"   # This sets the title for the x-axis\n    ),\n    yaxis=dict(\n        title=\"Assets Under Management\"   # This sets the title for the x-axis\n    ),\n\n    legend=dict(\n        orientation=\"h\",  # horizontal orientation\n        yanchor=\"bottom\",\n        y=-4.5,  # move the legend below the plot\n        xanchor=\"center\",\n        x=0.5\n    ))\nAUM_fig.update_traces(\n    hovertemplate=\"&lt;br&gt;\".join([\n        \"scheme=%{fullData.name}\",\n        \"date=%{x|%Y-%m-%d}\",\n        \"AUM=%{y} (Mill-Kes)\",\n        # removes any additional trace information that Plotly might add by default.\n        \"&lt;extra&gt;&lt;/extra&gt;\"\n    ])\n)\nAUM_fig.show()\n\n\n                                                \nInteractive plot of Assets Under Management (KSH Millions)"
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#archives---data-preservation-and-reproducibility",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#archives---data-preservation-and-reproducibility",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Archives - Data Preservation and Reproducibility",
    "text": "Archives - Data Preservation and Reproducibility\n\nImportance of Data Preservation\nIn the rapidly evolving landscape of financial markets, preserving historical data is crucial for long-term analysis, policy-making, and understanding market trends. Our archiving efforts aim to create a lasting resource for researchers, analysts, and policymakers interested in Kenya’s collective investment schemes.\n\n\nArchiving Methodology\nTo ensure the perpetuity and reproducibility of this analysis, we have implemented a comprehensive archiving strategy:\n\nRaw Data Preservation: We’ve archived the original, unaltered data crawled from various sources. This includes:\n\nRaw Cytonn Reports: Over 600 market research reports from Cytonn, dating back to 2014.\nCapital Markets Authority Approved Schemes: Official listings of CMA-approved collective investment schemes.\n\nProcessed Data Archival: We’ve preserved the cleaned and structured dataset resulting from our analysis:\n\nProcessed Investment Schemes Records: This includes standardized data on fund performance, assets under management, and other key metrics.\n\nComprehensive Documentation: We’ve created detailed documentation covering:\n\nData collection methodologies\nData cleaning and processing techniques\nVariable definitions and data structure explanations\nAny assumptions or limitations in the data or analysis\n\nOpen Access: All archived data is made available through the Hugging Face platform, ensuring:\n\nEasy accessibility for researchers globally\nVersion control and long-term preservation\nCompliance with fair use principles and ethical data sharing practices\n\n\n\n\nAccessing the Archives\nThe complete archive is hosted on Hugging Face at: https://huggingface.co/datasets/ToKnow-ai/Kenyan-Collective-Investment-Schemes-Dataset\nResearchers can access this data through the Hugging Face interface or via API calls, facilitating easy integration into various research workflows.\n\n\nImpact and Future Research\nBy preserving both raw and processed data, along with comprehensive documentation, we aim to:\n\nEnable verification and replication of our analysis\nFacilitate longitudinal studies on Kenya’s collective investment schemes\nProvide a foundation for comparative studies with other markets\nSupport evidence-based policy-making in Kenya’s financial sector\n\n\n\nEthical Considerations and Usage Guidelines\nWhile we encourage the use of this data for research and analysis, users should:\n\nAdhere to the terms of use specified in the dataset documentation\nProperly cite the dataset in any resulting publications or analyses\nBe aware of the limitations and potential biases in the data, as outlined in our documentation\n\n\n\nConclusion\nThis archiving effort not only supports the reproducibility of our current analysis but also sets a precedent for transparent, ethical data preservation in financial research. We hope this resource will contribute to a deeper understanding of Kenya’s financial markets and foster innovation in financial research methodologies.\n\n\nApproved Collective Investment Schemes\n          \n        \n          \n             Loading Approved Collective Investment Schemes...\n            \n          \n          \n        \n        \n        \n\n\nProcessed Investment Schemes Records\n          \n        \n          \n             Loading Processed Investment Schemes Records...\n            \n          \n          \n        \n        \n        \n\n\nRaw Cytonn Reports\n          \n        \n          \n             Loading Raw Cytonn Reports..."
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#archived-links",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#archived-links",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Archived Links",
    "text": "Archived Links\n\nhttps://licensees.cma.or.ke/ - archive\nhttps://www.rba.go.ke/registered-fund-managers/ - archive\nhttps://centwarrior.com/aa-kenya-shillings-fund/ - archive\nhttps://www.linkedin.com/posts/centwarrior_aa-kenya-shillings-fund-explained-in-2024-activity-7169322082814705664-8nwu - archive\nhttps://cytonn.com/topicals/investment-risk-analysis - archive\nhttps://equitygroupholdings.com/ke/investor-relations/eib\nhttps://enwealth.co.ke/about/#governance - archive\nhttps://www.linkedin.com/company/enwealth-kenya/ - archive\nhttps://enwealth.co.ke/capital/enwealth-money-market-fund/ - archive\nhttps://www.linkedin.com/company/arvocap-asset-managers/ - archive\nhttps://www.businessdailyafrica.com/bd/markets/avocarp-latest-to-enter-kenya-s-asset-management-market-4644586 - archive\nhttps://www.businessdailyafrica.com/bd/markets/capital-markets/fintech-start-up-ndovu-gets-cma-nod-to-set-up-money-market-fund-4548970 - archive\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/kenyan-collective-investment-schemes-dataset/index.html#footnotes",
    "href": "posts/kenyan-collective-investment-schemes-dataset/index.html#footnotes",
    "title": "Kenyan Collective Investment Schemes Dataset",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nhttps://www.cma.or.ke/licensees-market-players/ - archive↩︎\nhttps://licensees.cma.or.ke/licenses/15/ - archive↩︎\nhttps://www.cma.or.ke/licensees-market-players/ - archive↩︎\nhttps://licensees.cma.or.ke/licenses/15/ - archive↩︎\nCMA Approved Fund Managers - archive↩︎\nCMA Approved Investment Banks - archive↩︎\nGenghis Capital Unit Trust Fund - archive↩︎\nCMA Approved Fund Managers - archive↩︎\nCMA Approved Investment Banks - archive↩︎\nKCB restructures investment units after buyout of NBK - archive↩︎\nFrequently Asked Questions / Mali - archive↩︎\nSafaricom to launch unit trust, new savings service - archive↩︎\nM-PESA / M-PESA Services / Wealth / Mali - archive↩︎\nSafaricom’s Mali unit trust asset base hits Sh1.4bn - archive↩︎\nSafaricom, Genghis Capital fight over M-Pesa unit trusts↩︎\nKCB restructures investment units after buyout of NBK - archive↩︎\nZimele Savings Plan Transition: From Money Market to Fixed Income Fund - archive↩︎\nWhat Happened to the Zimele Money Market Fund? - archive↩︎\nhttps://cytonnreport.com/ - archive↩︎\nhttps://ke.cicinsurancegroup.com/mmf/ - archive↩︎\nhttps://cytonn.com/researches - archive↩︎\nhttps://cytonnreport.com/research - archive↩︎\nReproduction is prohibited other than in accordance with the copyright notice, which forms part of these terms and conditions. https://cytonn.com/terms-of-use - archive↩︎\nhttps://cytonn.com/researches - archive↩︎\nhttps://cytonnreport.com/research - archive↩︎"
  },
  {
    "objectID": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html",
    "href": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html",
    "title": "Life Expectancy by Age, Gender and Counties in Kenya",
    "section": "",
    "text": "We plot and analyze the life expectancy in Kenya by age, gender and counties. The data is from the 2019 Kenya Population and Housing Census. The data is available in the Kenya National Bureau of Statistics website 1. We will extract the data from Table 5.7, page 70 of the KPHC Census Analytical Report on Population Dynamics Volume VIII 2\nThe following is an image of the table we will be using:\nTable 5.7, page 70 of the 2019 Kenya Population and Housing Census Analytical Report on Population Dynamics Vol VIII.\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom adjustText import adjust_text\nfrom IPython.display import display, Markdown\nCode\ndata_dict = {\n    ('', 'COUNTY/RESIDENCE'): [\n        \"Kenya\",\n        \"Rural\",\n        \"Urban\",\n        \"Mombasa\",\n        \"Kwale\",\n        \"Kilifi\",\n        \"Tana River\",\n        \"Lamu\",\n        \"Taita Taveta\",\n        \"Garissa\",\n        \"Wajir\",\n        \"Mandera\",\n        \"Marsabit\",\n        \"Isiolo\",\n        \"Meru\",\n        \"Tharaka-Nithi\",\n        \"Embu\",\n        \"Kitui\",\n        \"Machakos\",\n        \"Makueni\",\n        \"Nyandarua\",\n        \"Nyeri\",\n        \"Kirinyaga\",\n        \"Murang'a\",\n        \"Kiambu\",\n        \"Turkana\",\n        \"West Pokot\",\n        \"Samburu\",\n        \"Trans Nzoia\",\n        \"Uasin Gishu\",\n        \"Elgeyo Marakwet\",\n        \"Nandi\",\n        \"Baringo\",\n        \"Laikipia\",\n        \"Nakuru\",\n        \"Narok\",\n        \"Kajiado\",\n        \"Kericho\",\n        \"Bomet\",\n        \"Kakamega\",\n        \"Vihiga\",\n        \"Bungoma\",\n        \"Busia\",\n        \"Siaya\",\n        \"Kisumu\",\n        \"Homa Bay\",\n        \"Migori\",\n        \"Kisii\",\n        \"Nyamira\",\n        \"Nairobi\"\n    ],\n    (\"MALE\", \"birth (age 0)\"): [\n        60.6,\n        60.3,\n        63.0,\n        65.7,\n        58.2,\n        57.8,\n        56.2,\n        64.1,\n        61.2,\n        57.4,\n        56.7,\n        57.3,\n        57.9,\n        60.1,\n        62.2,\n        63.8,\n        58.7,\n        55.7,\n        57.9,\n        58.1,\n        58.0,\n        66.4,\n        59.9,\n        58.1,\n        60.8,\n        55.9,\n        57.6,\n        58.5,\n        64.4,\n        63.6,\n        57.9,\n        60.0,\n        65.4,\n        59.2,\n        58.3,\n        57.8,\n        57.8,\n        58.1,\n        58.1,\n        63.4,\n        59.8,\n        65.5,\n        62.3,\n        53.4,\n        54.0,\n        50.5,\n        50.5,\n        56.1,\n        55.3,\n        62.6\n    ],\n    (\"MALE\", \"age 20\"): [\n        44.9,\n        44.6,\n        47.2,\n        51.0,\n        42.9,\n        43.0,\n        42.5,\n        50.9,\n        45.9,\n        43.1,\n        42.8,\n        43.0,\n        42.6,\n        45.9,\n        47.8,\n        47.3,\n        43.3,\n        40.3,\n        41.9,\n        42.2,\n        42.2,\n        49.9,\n        44.0,\n        42.6,\n        44.4,\n        42.1,\n        42.9,\n        42.6,\n        49.6,\n        48.0,\n        42.4,\n        44.6,\n        50.3,\n        42.6,\n        43.6,\n        43.0,\n        41.9,\n        42.4,\n        42.9,\n        49.5,\n        46.2,\n        50.6,\n        48.8,\n        39.9,\n        40.1,\n        38.7,\n        40.1,\n        40.7,\n        40.0,\n        48.4\n    ],\n    (\"MALE\", \"age 60\"): [\n        14.2,\n        14.5,\n        15.3,\n        17.7,\n        14.0,\n        14.1,\n        14.0,\n        17.7,\n        15.7,\n        14.1,\n        14.1,\n        14.1,\n        14.7,\n        15.7,\n        16.4,\n        16.2,\n        14.9,\n        14.0,\n        13.7,\n        13.8,\n        13.8,\n        16.3,\n        15.1,\n        13.9,\n        15.2,\n        14.5,\n        14.1,\n        14.6,\n        17.1,\n        16.5,\n        13.9,\n        15.3,\n        17.4,\n        14.7,\n        15.0,\n        14.1,\n        13.7,\n        13.9,\n        14.0,\n        17.1,\n        15.8,\n        17.6,\n        16.8,\n        13.9,\n        14.0,\n        13.7,\n        14.0,\n        14.2,\n        14.0,\n        16.6\n    ],\n    (\"MALE\", \"age 80\"): [\n        5.7,\n        5.4,\n        5.6,\n        5.9,\n        5.3,\n        5.3,\n        5.2,\n        5.9,\n        5.6,\n        5.2,\n        5.2,\n        5.2,\n        5.5,\n        5.6,\n        5.7,\n        5.7,\n        5.5,\n        5.4,\n        5.3,\n        5.3,\n        5.3,\n        6.0,\n        5.5,\n        5.3,\n        5.5,\n        5.4,\n        5.2,\n        5.5,\n        5.8,\n        5.7,\n        5.3,\n        5.5,\n        5.9,\n        5.5,\n        5.5,\n        5.3,\n        5.3,\n        5.3,\n        5.3,\n        5.8,\n        5.6,\n        5.9,\n        5.7,\n        5.4,\n        5.4,\n        5.3,\n        5.4,\n        5.4,\n        5.4,\n        5.7\n    ],\n    (\"FEMALE\", \"birth (age 0)\"): [\n        66.5,\n        66.2,\n        68.6,\n        71.1,\n        60.7,\n        59.2,\n        58.6,\n        68.9,\n        69.2,\n        60.5,\n        62.0,\n        60.5,\n        62.6,\n        59.6,\n        68.2,\n        70.3,\n        70.1,\n        68.8,\n        63.9,\n        62.4,\n        63.2,\n        75.8,\n        71.1,\n        64.7,\n        72.8,\n        60.0,\n        61.1,\n        67.4,\n        67.9,\n        71.7,\n        62.5,\n        70.4,\n        68.9,\n        71.9,\n        69.1,\n        61.9,\n        63.5,\n        62.9,\n        63.6,\n        66.1,\n        62.1,\n        68.2,\n        66.2,\n        61.6,\n        63.1,\n        60.2,\n        60.6,\n        66.9,\n        66.2,\n        65.2\n    ],\n    (\"FEMALE\", \"age 20\"): [\n        50.8,\n        50.9,\n        52.9,\n        55.3,\n        45.5,\n        44.6,\n        44.4,\n        53.3,\n        52.9,\n        45.6,\n        46.4,\n        45.5,\n        46.4,\n        44.4,\n        52.8,\n        53.1,\n        53.9,\n        52.3,\n        47.7,\n        46.6,\n        47.2,\n        59.6,\n        54.8,\n        48.3,\n        55.8,\n        45.6,\n        45.8,\n        51.3,\n        52.5,\n        55.2,\n        46.7,\n        54.3,\n        52.4,\n        54.5,\n        53.8,\n        46.3,\n        47.4,\n        47.0,\n        47.5,\n        51.3,\n        49.0,\n        52.6,\n        51.8,\n        47.2,\n        48.6,\n        47.6,\n        48.9,\n        50.6,\n        50.2,\n        50.2\n    ],\n    (\"FEMALE\", \"age 60\"): [\n        17.2,\n        17.7,\n        18.4,\n        20.0,\n        16.2,\n        15.9,\n        15.9,\n        18.9,\n        18.7,\n        16.2,\n        16.4,\n        16.2,\n        16.0,\n        15.4,\n        18.7,\n        18.8,\n        19.2,\n        18.4,\n        16.8,\n        16.5,\n        16.7,\n        22.2,\n        19.7,\n        17.0,\n        20.2,\n        15.7,\n        16.3,\n        18.0,\n        18.5,\n        19.9,\n        16.5,\n        19.4,\n        18.5,\n        19.5,\n        19.2,\n        16.4,\n        16.7,\n        16.6,\n        16.8,\n        18.0,\n        17.0,\n        18.6,\n        18.2,\n        16.3,\n        16.8,\n        16.5,\n        17.0,\n        17.7,\n        17.5,\n        17.5\n    ],\n    (\"FEMALE\", \"age 80\"): [\n        6.2,\n        6.3,\n        6.4,\n        6.5,\n        6.1,\n        6.0,\n        6.0,\n        6.3,\n        6.2,\n        6.1,\n        6.2,\n        6.1,\n        5.7,\n        5.7,\n        6.2,\n        6.3,\n        6.4,\n        6.2,\n        6.3,\n        6.2,\n        6.2,\n        7.4,\n        6.5,\n        6.3,\n        6.6,\n        5.7,\n        6.1,\n        6.1,\n        6.2,\n        6.5,\n        6.2,\n        6.4,\n        6.2,\n        6.4,\n        6.3,\n        6.2,\n        6.3,\n        6.2,\n        6.3,\n        6.1,\n        5.9,\n        6.2,\n        6.1,\n        5.8,\n        5.9,\n        5.8,\n        5.9,\n        6.0,\n        6.0,\n        6.0\n    ]\n}\ndf = pd.DataFrame(data_dict)\ndf.set_index(df.columns[0], inplace=True)\ndf\n\n\n\n\n\n\n\nLife Expectancy by Age, Gender and Region at Age 0, Age 20, Age 60 and Age 80 (in Years)\n\n\n\nMALE\nFEMALE\n\n\n\nbirth (age 0)\nage 20\nage 60\nage 80\nbirth (age 0)\nage 20\nage 60\nage 80\n\n\n(, COUNTY/RESIDENCE)\n\n\n\n\n\n\n\n\n\n\n\n\nKenya\n60.6\n44.9\n14.2\n5.7\n66.5\n50.8\n17.2\n6.2\n\n\nRural\n60.3\n44.6\n14.5\n5.4\n66.2\n50.9\n17.7\n6.3\n\n\nUrban\n63.0\n47.2\n15.3\n5.6\n68.6\n52.9\n18.4\n6.4\n\n\nMombasa\n65.7\n51.0\n17.7\n5.9\n71.1\n55.3\n20.0\n6.5\n\n\nKwale\n58.2\n42.9\n14.0\n5.3\n60.7\n45.5\n16.2\n6.1\n\n\nKilifi\n57.8\n43.0\n14.1\n5.3\n59.2\n44.6\n15.9\n6.0\n\n\nTana River\n56.2\n42.5\n14.0\n5.2\n58.6\n44.4\n15.9\n6.0\n\n\nLamu\n64.1\n50.9\n17.7\n5.9\n68.9\n53.3\n18.9\n6.3\n\n\nTaita Taveta\n61.2\n45.9\n15.7\n5.6\n69.2\n52.9\n18.7\n6.2\n\n\nGarissa\n57.4\n43.1\n14.1\n5.2\n60.5\n45.6\n16.2\n6.1\n\n\nWajir\n56.7\n42.8\n14.1\n5.2\n62.0\n46.4\n16.4\n6.2\n\n\nMandera\n57.3\n43.0\n14.1\n5.2\n60.5\n45.5\n16.2\n6.1\n\n\nMarsabit\n57.9\n42.6\n14.7\n5.5\n62.6\n46.4\n16.0\n5.7\n\n\nIsiolo\n60.1\n45.9\n15.7\n5.6\n59.6\n44.4\n15.4\n5.7\n\n\nMeru\n62.2\n47.8\n16.4\n5.7\n68.2\n52.8\n18.7\n6.2\n\n\nTharaka-Nithi\n63.8\n47.3\n16.2\n5.7\n70.3\n53.1\n18.8\n6.3\n\n\nEmbu\n58.7\n43.3\n14.9\n5.5\n70.1\n53.9\n19.2\n6.4\n\n\nKitui\n55.7\n40.3\n14.0\n5.4\n68.8\n52.3\n18.4\n6.2\n\n\nMachakos\n57.9\n41.9\n13.7\n5.3\n63.9\n47.7\n16.8\n6.3\n\n\nMakueni\n58.1\n42.2\n13.8\n5.3\n62.4\n46.6\n16.5\n6.2\n\n\nNyandarua\n58.0\n42.2\n13.8\n5.3\n63.2\n47.2\n16.7\n6.2\n\n\nNyeri\n66.4\n49.9\n16.3\n6.0\n75.8\n59.6\n22.2\n7.4\n\n\nKirinyaga\n59.9\n44.0\n15.1\n5.5\n71.1\n54.8\n19.7\n6.5\n\n\nMurang'a\n58.1\n42.6\n13.9\n5.3\n64.7\n48.3\n17.0\n6.3\n\n\nKiambu\n60.8\n44.4\n15.2\n5.5\n72.8\n55.8\n20.2\n6.6\n\n\nTurkana\n55.9\n42.1\n14.5\n5.4\n60.0\n45.6\n15.7\n5.7\n\n\nWest Pokot\n57.6\n42.9\n14.1\n5.2\n61.1\n45.8\n16.3\n6.1\n\n\nSamburu\n58.5\n42.6\n14.6\n5.5\n67.4\n51.3\n18.0\n6.1\n\n\nTrans Nzoia\n64.4\n49.6\n17.1\n5.8\n67.9\n52.5\n18.5\n6.2\n\n\nUasin Gishu\n63.6\n48.0\n16.5\n5.7\n71.7\n55.2\n19.9\n6.5\n\n\nElgeyo Marakwet\n57.9\n42.4\n13.9\n5.3\n62.5\n46.7\n16.5\n6.2\n\n\nNandi\n60.0\n44.6\n15.3\n5.5\n70.4\n54.3\n19.4\n6.4\n\n\nBaringo\n65.4\n50.3\n17.4\n5.9\n68.9\n52.4\n18.5\n6.2\n\n\nLaikipia\n59.2\n42.6\n14.7\n5.5\n71.9\n54.5\n19.5\n6.4\n\n\nNakuru\n58.3\n43.6\n15.0\n5.5\n69.1\n53.8\n19.2\n6.3\n\n\nNarok\n57.8\n43.0\n14.1\n5.3\n61.9\n46.3\n16.4\n6.2\n\n\nKajiado\n57.8\n41.9\n13.7\n5.3\n63.5\n47.4\n16.7\n6.3\n\n\nKericho\n58.1\n42.4\n13.9\n5.3\n62.9\n47.0\n16.6\n6.2\n\n\nBomet\n58.1\n42.9\n14.0\n5.3\n63.6\n47.5\n16.8\n6.3\n\n\nKakamega\n63.4\n49.5\n17.1\n5.8\n66.1\n51.3\n18.0\n6.1\n\n\nVihiga\n59.8\n46.2\n15.8\n5.6\n62.1\n49.0\n17.0\n5.9\n\n\nBungoma\n65.5\n50.6\n17.6\n5.9\n68.2\n52.6\n18.6\n6.2\n\n\nBusia\n62.3\n48.8\n16.8\n5.7\n66.2\n51.8\n18.2\n6.1\n\n\nSiaya\n53.4\n39.9\n13.9\n5.4\n61.6\n47.2\n16.3\n5.8\n\n\nKisumu\n54.0\n40.1\n14.0\n5.4\n63.1\n48.6\n16.8\n5.9\n\n\nHoma Bay\n50.5\n38.7\n13.7\n5.3\n60.2\n47.6\n16.5\n5.8\n\n\nMigori\n50.5\n40.1\n14.0\n5.4\n60.6\n48.9\n17.0\n5.9\n\n\nKisii\n56.1\n40.7\n14.2\n5.4\n66.9\n50.6\n17.7\n6.0\n\n\nNyamira\n55.3\n40.0\n14.0\n5.4\n66.2\n50.2\n17.5\n6.0\n\n\nNairobi\n62.6\n48.4\n16.6\n5.7\n65.2\n50.2\n17.5\n6.0"
  },
  {
    "objectID": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#plots-and-insights",
    "href": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#plots-and-insights",
    "title": "Life Expectancy by Age, Gender and Counties in Kenya",
    "section": "Plots and Insights",
    "text": "Plots and Insights\n\n\nCode\ndef life_expectancy_analysis(age_column: str, axis_margin: float = 1.2):\n    ### Life Expectancy\n    display(Markdown(\n        \"\\n\"\n        f\"### Life Expectancy at {age_column}\"\n        \"\\n\"))\n    kenya = df.loc[\"Kenya\"]\n    rural = df.loc[\"Rural\"]\n    urban = df.loc[\"Urban\"]\n    drop_columns = [kenya.name, rural.name, urban.name]\n    males = df[('MALE', age_column)].drop(drop_columns).tolist()\n    females = df[('FEMALE', age_column)].drop(drop_columns).tolist()\n    counties = df.index.drop(drop_columns).tolist()\n    #### Plotting\n    display(Markdown(\n        \"\\n\"\n        f\"#### Plot - {age_column}\"\n        \"\\n\"))\n    plt.figure(figsize=(12, 10))\n    # Plot the counties\n    sns.scatterplot(\n        x=males, y=females, s=60, alpha=0.9, label='Counties Life Expectancy')\n    # Plot reference lines for Kenya, Rural, Urban\n    plt.axvline(\n        x=kenya[('MALE', age_column)], color='r', linestyle='dashdot',  alpha=0.3)\n    plt.axhline(\n        y=kenya[('FEMALE', age_column)], color='r', linestyle='dashdot', alpha=0.3, \n        label='National Life Expectancy')\n    plt.axvline(\n        x=rural[('MALE', age_column)], color='g', linestyle='dashdot', alpha=0.3)\n    plt.axhline(\n        y=rural[('FEMALE', age_column)], color='g', linestyle='dashdot', alpha=0.3, \n        label='Rural Life Expectancy')\n    plt.axvline(\n        x=urban[('MALE', age_column)], color='b', linestyle='dashdot', alpha=0.3)\n    plt.axhline(\n        y=urban[('FEMALE', age_column)], color='b', linestyle='dashdot', alpha=0.3, \n        label='Urban Life Expectancy')\n    # Add diagonal line representing equal life expectancy\n    min_val = min(min(males), min(females))\n    max_val = max(max(males), max(females))\n    plt.plot(\n        [min_val, max_val], [min_val, max_val], color='gray', linestyle='solid', \n        alpha=0.3, label='Equal Life Expectancy')\n    # Add labels for reference points\n    texts = [\n        plt.text(males[i], females[i], counties[i], fontsize=10) \n        for i \n        in range(len(counties))]\n    adjust_text(texts, arrowprops=dict(arrowstyle='-', color='black', lw=0.5))\n    # Adjust limits based on scatter data\n    plt.xlim(min(males) - axis_margin, max(males) + axis_margin) \n    plt.ylim(min(females) - axis_margin, max(females) + axis_margin)\n    plt.grid(True, alpha=0.2)\n    plt.title(\n        f'Life Expectancy at {age_column} by County in Kenya: Male vs Female', \n        fontsize=16)\n    plt.xlabel('Male Life Expectancy (years)', fontsize=14)\n    plt.ylabel('Female Life Expectancy (years)', fontsize=14)\n    plt.legend(loc='upper left')\n    plt.tight_layout()\n    plt.show()\n    #### Analysis of Life Expectancy\n    display(Markdown(\n        \"\\n\"\n        f\"#### Analysis of Life Expectancy at {age_column}\"\n        \"\\n\"))\n    kenya_males = kenya[('MALE', age_column)]\n    kenya_females = kenya[('FEMALE', age_column)]\n    rural_males = rural[('MALE', age_column)]\n    rural_females = rural[('FEMALE', age_column)]\n    urban_males = urban[('MALE', age_column)]\n    urban_females = urban[('FEMALE', age_column)]\n    county_analysis_df = pd.DataFrame({ \n        'COUNTY': counties, \n        'MALE': males, \n        'FEMALE': females \n    })\n    ##### Top 5 counties greatest and least life expectancy\n    display(Markdown(\n        \"\\n\"\n        f\"##### Top 5 counties with greatest and least life expectancy - {age_column}\"\n        \"\\n\"))\n    men_by_county_asc = county_analysis_df\\\n        .sort_values(\"MALE\", ascending=True)\\\n        .head(5)\\\n        .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"MALE\"]}', axis=1)\\\n        .tolist()\n    men_by_county_desc = county_analysis_df\\\n        .sort_values(\"MALE\", ascending=False)\\\n        .head(5)\\\n        .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"MALE\"]}', axis=1)\\\n        .tolist()\n    women_by_county_asc = county_analysis_df\\\n        .sort_values(\"FEMALE\", ascending=True)\\\n        .head(5)\\\n        .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"FEMALE\"]}', axis=1)\\\n        .tolist()\n    women_by_county_desc = county_analysis_df\\\n        .sort_values(\"FEMALE\", ascending=False)\\\n        .head(5)\\\n        .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"FEMALE\"]}', axis=1)\\\n        .tolist()\n    display(pd.DataFrame({ \n        '[men] by_county_desc': men_by_county_desc,\n        '[men] by_county_asc': men_by_county_asc, \n        '[women] by_county_desc': women_by_county_desc,\n        '[women] by_county_asc': women_by_county_asc,\n    }, index=range(1, 6)))\n    # Analysis\n    def above_or_below(reference: str, males: float, females: float):\n        men_by_county_below = county_analysis_df\\\n            [county_analysis_df[\"MALE\"] &lt;= males]\\\n            .sort_values(\"MALE\", ascending=True)\\\n            .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"MALE\"]}', axis=1)\\\n            .tolist()\n        men_by_county_above = county_analysis_df\\\n            [county_analysis_df[\"MALE\"] &gt; males]\\\n            .sort_values(\"MALE\", ascending=False)\\\n            .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"MALE\"]}', axis=1)\\\n            .tolist()\n        women_by_county_below = county_analysis_df\\\n            [county_analysis_df[\"FEMALE\"] &lt;= females]\\\n            .sort_values(\"FEMALE\", ascending=True)\\\n            .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"FEMALE\"]}', axis=1)\\\n            .tolist()\n        women_by_county_above = county_analysis_df\\\n            [county_analysis_df[\"FEMALE\"] &gt; females]\\\n            .sort_values(\"FEMALE\", ascending=False)\\\n            .apply(lambda x: f'{x[\"COUNTY\"] } - {x[\"FEMALE\"]}', axis=1)\\\n            .tolist()\n        return pd.DataFrame.from_dict({ \n            f'({len(men_by_county_above)}) [men] above {reference}': men_by_county_above[:6],\n            f'({len(men_by_county_below)}) [men] below {reference}': men_by_county_below[:6], \n            f'({len(women_by_county_above)}) [women] above {reference}': women_by_county_above[:6],\n            f'({len(women_by_county_below)}) [women] below {reference}': women_by_county_below[:6],\n        }, orient='index').T.reindex(index=range(1, 6))\n    ##### Top 5 counties above and below national life expectancy\"\n    display(Markdown(\n        \"\\n\"\n        f\"##### Top 5 counties above and below National life expectancy - {age_column}\"\n        \"\\n\"))\n    display(above_or_below('national', kenya_males, kenya_females))\n    ##### Top 5 counties above and below urban life expectancy\n    display(Markdown(\n        \"\\n\"\n        f\"##### Top 5 counties above and below Rural life expectancy - {age_column}\"\n        \"\\n\"))\n    display(above_or_below('rural', rural_males, rural_females))\n    ##### Top 5 counties above and below urban life expectancy\n    display(Markdown(\n        \"\\n\"\n        f\"##### Top 5 counties above and below Urban life expectancy - {age_column}\"\n        \"\\n\"))\n    display(above_or_below('urban', urban_males, urban_females))\n    ##### Top 5 counties where men have higher life expectancy than women\n    men_greater_than_women = county_analysis_df\\\n        [county_analysis_df[\"FEMALE\"] &lt; county_analysis_df[\"MALE\"]]\n    if men_greater_than_women is not None and not men_greater_than_women.empty:\n        display(Markdown(\n            \"\\n\"\n            f\"##### Top 5 counties where men have higher life expectancy than women - {age_column}\"\n            \"\\n\"))\n        display(men_greater_than_women.sort_values(\"FEMALE\", ascending=True))\n    display(Markdown(\n        \"\\n\"\n        f\"##### Other Statistics - {age_column}\"\n        \"\\n\"))\n    display(county_analysis_df.describe())\n\n\n\n\nCode\nlife_expectancy_analysis('birth (age 0)')\n\n\nLife Expectancy at birth (age 0)\n\nPlot - birth (age 0)\n\n\n\nAnalysis of Life Expectancy at birth (age 0)\n\nTop 5 counties with greatest and least life expectancy - birth (age 0)\n\n\n\n\n\n\n\n\n[men] by_county_desc\n[men] by_county_asc\n[women] by_county_desc\n[women] by_county_asc\n\n\n\n\n1\nNyeri - 66.4\nHoma Bay - 50.5\nNyeri - 75.8\nTana River - 58.6\n\n\n2\nMombasa - 65.7\nMigori - 50.5\nKiambu - 72.8\nKilifi - 59.2\n\n\n3\nBungoma - 65.5\nSiaya - 53.4\nLaikipia - 71.9\nIsiolo - 59.6\n\n\n4\nBaringo - 65.4\nKisumu - 54.0\nUasin Gishu - 71.7\nTurkana - 60.0\n\n\n5\nTrans Nzoia - 64.4\nNyamira - 55.3\nKirinyaga - 71.1\nHoma Bay - 60.2\n\n\n\n\n\n\n\n\nTop 5 counties above and below National life expectancy - birth (age 0)\n\n\n\n\n\n\n\n\n(14) [men] above national\n(33) [men] below national\n(19) [women] above national\n(28) [women] below national\n\n\n\n\n1\nMombasa - 65.7\nHoma Bay - 50.5\nKiambu - 72.8\nKilifi - 59.2\n\n\n2\nBungoma - 65.5\nSiaya - 53.4\nLaikipia - 71.9\nIsiolo - 59.6\n\n\n3\nBaringo - 65.4\nKisumu - 54.0\nUasin Gishu - 71.7\nTurkana - 60.0\n\n\n4\nTrans Nzoia - 64.4\nNyamira - 55.3\nKirinyaga - 71.1\nHoma Bay - 60.2\n\n\n5\nLamu - 64.1\nKitui - 55.7\nMombasa - 71.1\nGarissa - 60.5\n\n\n\n\n\n\n\n\nTop 5 counties above and below Rural life expectancy - birth (age 0)\n\n\n\n\n\n\n\n\n(14) [men] above rural\n(33) [men] below rural\n(19) [women] above rural\n(28) [women] below rural\n\n\n\n\n1\nMombasa - 65.7\nHoma Bay - 50.5\nKiambu - 72.8\nKilifi - 59.2\n\n\n2\nBungoma - 65.5\nSiaya - 53.4\nLaikipia - 71.9\nIsiolo - 59.6\n\n\n3\nBaringo - 65.4\nKisumu - 54.0\nUasin Gishu - 71.7\nTurkana - 60.0\n\n\n4\nTrans Nzoia - 64.4\nNyamira - 55.3\nKirinyaga - 71.1\nHoma Bay - 60.2\n\n\n5\nLamu - 64.1\nKitui - 55.7\nMombasa - 71.1\nGarissa - 60.5\n\n\n\n\n\n\n\n\nTop 5 counties above and below Urban life expectancy - birth (age 0)\n\n\n\n\n\n\n\n\n(9) [men] above urban\n(38) [men] below urban\n(14) [women] above urban\n(33) [women] below urban\n\n\n\n\n1\nMombasa - 65.7\nMigori - 50.5\nKiambu - 72.8\nKilifi - 59.2\n\n\n2\nBungoma - 65.5\nSiaya - 53.4\nLaikipia - 71.9\nIsiolo - 59.6\n\n\n3\nBaringo - 65.4\nKisumu - 54.0\nUasin Gishu - 71.7\nTurkana - 60.0\n\n\n4\nTrans Nzoia - 64.4\nNyamira - 55.3\nMombasa - 71.1\nHoma Bay - 60.2\n\n\n5\nLamu - 64.1\nKitui - 55.7\nKirinyaga - 71.1\nGarissa - 60.5\n\n\n\n\n\n\n\n\nTop 5 counties where men have higher life expectancy than women - birth (age 0)\n\n\n\n\n\n\n\n\nCOUNTY\nMALE\nFEMALE\n\n\n\n\n10\nIsiolo\n60.1\n59.6\n\n\n\n\n\n\n\n\nOther Statistics - birth (age 0)\n\n\n\n\n\n\n\n\nMALE\nFEMALE\n\n\n\n\ncount\n47.000000\n47.000000\n\n\nmean\n59.110638\n65.391489\n\n\nstd\n3.684706\n4.327959\n\n\nmin\n50.500000\n58.600000\n\n\n25%\n57.500000\n61.950000\n\n\n50%\n58.100000\n64.700000\n\n\n75%\n61.700000\n68.900000\n\n\nmax\n66.400000\n75.800000\n\n\n\n\n\n\n\n\n\n\n\nCode\nlife_expectancy_analysis('age 20')\n\n\nLife Expectancy at age 20\n\nPlot - age 20\n\n\n\nAnalysis of Life Expectancy at age 20\n\nTop 5 counties with greatest and least life expectancy - age 20\n\n\n\n\n\n\n\n\n[men] by_county_desc\n[men] by_county_asc\n[women] by_county_desc\n[women] by_county_asc\n\n\n\n\n1\nMombasa - 51.0\nHoma Bay - 38.7\nNyeri - 59.6\nTana River - 44.4\n\n\n2\nLamu - 50.9\nSiaya - 39.9\nKiambu - 55.8\nIsiolo - 44.4\n\n\n3\nBungoma - 50.6\nNyamira - 40.0\nMombasa - 55.3\nKilifi - 44.6\n\n\n4\nBaringo - 50.3\nMigori - 40.1\nUasin Gishu - 55.2\nKwale - 45.5\n\n\n5\nNyeri - 49.9\nKisumu - 40.1\nKirinyaga - 54.8\nMandera - 45.5\n\n\n\n\n\n\n\n\nTop 5 counties above and below National life expectancy - age 20\n\n\n\n\n\n\n\n\n(15) [men] above national\n(32) [men] below national\n(20) [women] above national\n(27) [women] below national\n\n\n\n\n1\nLamu - 50.9\nSiaya - 39.9\nKiambu - 55.8\nIsiolo - 44.4\n\n\n2\nBungoma - 50.6\nNyamira - 40.0\nMombasa - 55.3\nKilifi - 44.6\n\n\n3\nBaringo - 50.3\nMigori - 40.1\nUasin Gishu - 55.2\nKwale - 45.5\n\n\n4\nNyeri - 49.9\nKisumu - 40.1\nKirinyaga - 54.8\nMandera - 45.5\n\n\n5\nTrans Nzoia - 49.6\nKitui - 40.3\nLaikipia - 54.5\nGarissa - 45.6\n\n\n\n\n\n\n\n\nTop 5 counties above and below Rural life expectancy - age 20\n\n\n\n\n\n\n\n\n(15) [men] above rural\n(32) [men] below rural\n(20) [women] above rural\n(27) [women] below rural\n\n\n\n\n1\nLamu - 50.9\nSiaya - 39.9\nKiambu - 55.8\nIsiolo - 44.4\n\n\n2\nBungoma - 50.6\nNyamira - 40.0\nMombasa - 55.3\nKilifi - 44.6\n\n\n3\nBaringo - 50.3\nMigori - 40.1\nUasin Gishu - 55.2\nKwale - 45.5\n\n\n4\nNyeri - 49.9\nKisumu - 40.1\nKirinyaga - 54.8\nMandera - 45.5\n\n\n5\nTrans Nzoia - 49.6\nKitui - 40.3\nLaikipia - 54.5\nGarissa - 45.6\n\n\n\n\n\n\n\n\nTop 5 counties above and below Urban life expectancy - age 20\n\n\n\n\n\n\n\n\n(12) [men] above urban\n(35) [men] below urban\n(11) [women] above urban\n(36) [women] below urban\n\n\n\n\n1\nLamu - 50.9\nSiaya - 39.9\nKiambu - 55.8\nIsiolo - 44.4\n\n\n2\nBungoma - 50.6\nNyamira - 40.0\nMombasa - 55.3\nKilifi - 44.6\n\n\n3\nBaringo - 50.3\nMigori - 40.1\nUasin Gishu - 55.2\nKwale - 45.5\n\n\n4\nNyeri - 49.9\nKisumu - 40.1\nKirinyaga - 54.8\nMandera - 45.5\n\n\n5\nTrans Nzoia - 49.6\nKitui - 40.3\nLaikipia - 54.5\nGarissa - 45.6\n\n\n\n\n\n\n\n\nTop 5 counties where men have higher life expectancy than women - age 20\n\n\n\n\n\n\n\n\nCOUNTY\nMALE\nFEMALE\n\n\n\n\n10\nIsiolo\n45.9\n44.4\n\n\n\n\n\n\n\n\nOther Statistics - age 20\n\n\n\n\n\n\n\n\nMALE\nFEMALE\n\n\n\n\ncount\n47.000000\n47.000000\n\n\nmean\n44.285106\n49.887234\n\n\nstd\n3.388983\n3.748137\n\n\nmin\n38.700000\n44.400000\n\n\n25%\n42.300000\n46.650000\n\n\n50%\n43.000000\n49.000000\n\n\n75%\n46.750000\n52.850000\n\n\nmax\n51.000000\n59.600000\n\n\n\n\n\n\n\nCode\nlife_expectancy_analysis('age 60')\n\n\n\n\n\nLife Expectancy at age 60\n\nPlot - age 60\n\n\n\nAnalysis of Life Expectancy at age 60\n\nTop 5 counties with greatest and least life expectancy - age 60\n\n\n\n\n\n\n\n\n[men] by_county_desc\n[men] by_county_asc\n[women] by_county_desc\n[women] by_county_asc\n\n\n\n\n1\nMombasa - 17.7\nHoma Bay - 13.7\nNyeri - 22.2\nIsiolo - 15.4\n\n\n2\nLamu - 17.7\nMachakos - 13.7\nKiambu - 20.2\nTurkana - 15.7\n\n\n3\nBungoma - 17.6\nKajiado - 13.7\nMombasa - 20.0\nKilifi - 15.9\n\n\n4\nBaringo - 17.4\nNyandarua - 13.8\nUasin Gishu - 19.9\nTana River - 15.9\n\n\n5\nTrans Nzoia - 17.1\nMakueni - 13.8\nKirinyaga - 19.7\nMarsabit - 16.0\n\n\n\n\n\n\n\n\nTop 5 counties above and below National life expectancy - age 60\n\n\n\n\n\n\n\n\n(24) [men] above national\n(23) [men] below national\n(23) [women] above national\n(24) [women] below national\n\n\n\n\n1\nLamu - 17.7\nMachakos - 13.7\nKiambu - 20.2\nTurkana - 15.7\n\n\n2\nBungoma - 17.6\nKajiado - 13.7\nMombasa - 20.0\nKilifi - 15.9\n\n\n3\nBaringo - 17.4\nMakueni - 13.8\nUasin Gishu - 19.9\nTana River - 15.9\n\n\n4\nKakamega - 17.1\nNyandarua - 13.8\nKirinyaga - 19.7\nMarsabit - 16.0\n\n\n5\nTrans Nzoia - 17.1\nSiaya - 13.9\nLaikipia - 19.5\nKwale - 16.2\n\n\n\n\n\n\n\n\nTop 5 counties above and below Rural life expectancy - age 60\n\n\n\n\n\n\n\n\n(23) [men] above rural\n(24) [men] below rural\n(20) [women] above rural\n(27) [women] below rural\n\n\n\n\n1\nLamu - 17.7\nMachakos - 13.7\nKiambu - 20.2\nTurkana - 15.7\n\n\n2\nBungoma - 17.6\nKajiado - 13.7\nMombasa - 20.0\nKilifi - 15.9\n\n\n3\nBaringo - 17.4\nMakueni - 13.8\nUasin Gishu - 19.9\nTana River - 15.9\n\n\n4\nKakamega - 17.1\nNyandarua - 13.8\nKirinyaga - 19.7\nMarsabit - 16.0\n\n\n5\nTrans Nzoia - 17.1\nSiaya - 13.9\nLaikipia - 19.5\nKwale - 16.2\n\n\n\n\n\n\n\n\nTop 5 counties above and below Urban life expectancy - age 60\n\n\n\n\n\n\n\n\n(15) [men] above urban\n(32) [men] below urban\n(16) [women] above urban\n(31) [women] below urban\n\n\n\n\n1\nLamu - 17.7\nKajiado - 13.7\nKiambu - 20.2\nTurkana - 15.7\n\n\n2\nBungoma - 17.6\nMachakos - 13.7\nMombasa - 20.0\nKilifi - 15.9\n\n\n3\nBaringo - 17.4\nMakueni - 13.8\nUasin Gishu - 19.9\nTana River - 15.9\n\n\n4\nTrans Nzoia - 17.1\nNyandarua - 13.8\nKirinyaga - 19.7\nMarsabit - 16.0\n\n\n5\nKakamega - 17.1\nSiaya - 13.9\nLaikipia - 19.5\nKwale - 16.2\n\n\n\n\n\n\n\n\nTop 5 counties where men have higher life expectancy than women - age 60\n\n\n\n\n\n\n\n\nCOUNTY\nMALE\nFEMALE\n\n\n\n\n10\nIsiolo\n15.7\n15.4\n\n\n\n\n\n\n\n\nOther Statistics - age 60\n\n\n\n\n\n\n\n\nMALE\nFEMALE\n\n\n\n\ncount\n47.000000\n47.000000\n\n\nmean\n15.014894\n17.640426\n\n\nstd\n1.291271\n1.496178\n\n\nmin\n13.700000\n15.400000\n\n\n25%\n14.000000\n16.450000\n\n\n50%\n14.500000\n17.000000\n\n\n75%\n16.000000\n18.700000\n\n\nmax\n17.700000\n22.200000\n\n\n\n\n\n\n\nCode\nlife_expectancy_analysis('age 80', axis_margin = 0.1)\n\n\n\n\n\nLife Expectancy at age 80\n\nPlot - age 80\n20 [ 0.85928444 -0.45533174] 35 [-0.08290397 -0.92990256] \n\n\nAnalysis of Life Expectancy at age 80\n\nTop 5 counties with greatest and least life expectancy - age 80\n\n\n\n\n\n\n\n\n[men] by_county_desc\n[men] by_county_asc\n[women] by_county_desc\n[women] by_county_asc\n\n\n\n\n1\nNyeri - 6.0\nWest Pokot - 5.2\nNyeri - 7.4\nMarsabit - 5.7\n\n\n2\nMombasa - 5.9\nTana River - 5.2\nKiambu - 6.6\nIsiolo - 5.7\n\n\n3\nBungoma - 5.9\nGarissa - 5.2\nMombasa - 6.5\nTurkana - 5.7\n\n\n4\nLamu - 5.9\nWajir - 5.2\nUasin Gishu - 6.5\nHoma Bay - 5.8\n\n\n5\nBaringo - 5.9\nMandera - 5.2\nKirinyaga - 6.5\nSiaya - 5.8\n\n\n\n\n\n\n\n\nTop 5 counties above and below National life expectancy - age 80\n\n\n\n\n\n\n\n\n(7) [men] above national\n(40) [men] below national\n(15) [women] above national\n(32) [women] below national\n\n\n\n\n1\nMombasa - 5.9\nTana River - 5.2\nKiambu - 6.6\nIsiolo - 5.7\n\n\n2\nLamu - 5.9\nGarissa - 5.2\nMombasa - 6.5\nTurkana - 5.7\n\n\n3\nBaringo - 5.9\nWajir - 5.2\nKirinyaga - 6.5\nHoma Bay - 5.8\n\n\n4\nBungoma - 5.9\nMandera - 5.2\nUasin Gishu - 6.5\nSiaya - 5.8\n\n\n5\nTrans Nzoia - 5.8\nNarok - 5.3\nEmbu - 6.4\nMigori - 5.9\n\n\n\n\n\n\n\n\nTop 5 counties above and below Rural life expectancy - age 80\n\n\n\n\n\n\n\n\n(23) [men] above rural\n(24) [men] below rural\n(8) [women] above rural\n(39) [women] below rural\n\n\n\n\n1\nMombasa - 5.9\nGarissa - 5.2\nKiambu - 6.6\nIsiolo - 5.7\n\n\n2\nLamu - 5.9\nWajir - 5.2\nMombasa - 6.5\nTurkana - 5.7\n\n\n3\nBungoma - 5.9\nMandera - 5.2\nKirinyaga - 6.5\nHoma Bay - 5.8\n\n\n4\nBaringo - 5.9\nWest Pokot - 5.2\nUasin Gishu - 6.5\nSiaya - 5.8\n\n\n5\nKakamega - 5.8\nKwale - 5.3\nEmbu - 6.4\nMigori - 5.9\n\n\n\n\n\n\n\n\nTop 5 counties above and below Urban life expectancy - age 80\n\n\n\n\n\n\n\n\n(12) [men] above urban\n(35) [men] below urban\n(5) [women] above urban\n(42) [women] below urban\n\n\n\n\n1\nMombasa - 5.9\nGarissa - 5.2\nKiambu - 6.6\nIsiolo - 5.7\n\n\n2\nLamu - 5.9\nWajir - 5.2\nMombasa - 6.5\nTurkana - 5.7\n\n\n3\nBaringo - 5.9\nMandera - 5.2\nKirinyaga - 6.5\nHoma Bay - 5.8\n\n\n4\nBungoma - 5.9\nWest Pokot - 5.2\nUasin Gishu - 6.5\nSiaya - 5.8\n\n\n5\nTrans Nzoia - 5.8\nKwale - 5.3\nNone\nMigori - 5.9\n\n\n\n\n\n\n\n\nOther Statistics - age 80\n\n\n\n\n\n\n\n\nMALE\nFEMALE\n\n\n\n\ncount\n47.000000\n47.000000\n\n\nmean\n5.487234\n6.176596\n\n\nstd\n0.223234\n0.281466\n\n\nmin\n5.200000\n5.700000\n\n\n25%\n5.300000\n6.000000\n\n\n50%\n5.400000\n6.200000\n\n\n75%\n5.650000\n6.300000\n\n\nmax\n6.000000\n7.400000"
  },
  {
    "objectID": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#insights",
    "href": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#insights",
    "title": "Life Expectancy by Age, Gender and Counties in Kenya",
    "section": "Insights",
    "text": "Insights\n\nThe rural and national life expectancy is are close, which confirms that majority of Kenyans lives in rural areas.\nMen and Women of Nyeri county Have the greatest overall life expectancy, in all ranges.\nIsiolo county is the only county men below 80 years live longer than women.\n\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. Read more: /terms-of-service"
  },
  {
    "objectID": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#footnotes",
    "href": "posts/life-expectancy-age-gender-and-counties-in-kenya/index.html#footnotes",
    "title": "Life Expectancy by Age, Gender and Counties in Kenya",
    "section": "Footnotes / Citations / References",
    "text": "Footnotes / Citations / References\n\n\nKenya National Bureau of Statistics website↩︎\nKPHC Census Analytical Report on Population Dynamics Volume VIII↩︎"
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#introduction",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#introduction",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "Introduction",
    "text": "Introduction\nHave you ever found yourself in a situation where you’ve Shazamed a bunch of great songs, but can’t listen to them without subscribing to a premium music service? Well, we’ve got great news for you! There’s a free and easy way to enjoy your Shazam discoveries without breaking the bank. Let’s dive into how you can convert your Shazam playlist to a YouTube playlist and start listening right away."
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#enter-the-shazam-to-youtube-converter",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#enter-the-shazam-to-youtube-converter",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "Enter the Shazam to YouTube Converter",
    "text": "Enter the Shazam to YouTube Converter\nWe have developed a web application that takes your Shazam playlist and converts it into a YouTube playlist. This means you can listen to full versions of your discovered songs without paying for a subscription!"
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#how-it-works",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#how-it-works",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "How It Works",
    "text": "How It Works\nThe process is surprisingly simple:\n\nExport Your Shazam Library: First, you’ll need to export your Shazam library as a CSV file. You can do this by visiting https://www.shazam.com/myshazam and downloading your list of identified songs.\nUpload to the Converter: Visit the Shazam Playlist to YouTube Playlist converter. Here, you’ll see an option to upload your CSV file.\nWatch the Magic Happen: Once you upload your file, the application processes your Shazam list and creates a YouTube playlist on the fly.\nStart Listening: A YouTube player will appear, ready to play through your entire list of songs. You can also see a table of all your tracks, allowing you to easily jump to any song in the list."
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#code",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#code",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "Code",
    "text": "Code\n\nImports\n\n%load_ext autoreload\n%autoreload 2\n\nimport sys\nsys.path.append(\"./shazam-playlist-to-youtube-playlist\")\n\n\nfrom IPython.display import HTML\nimport pandas as pd\nfrom pytube import YouTube\nfrom app import get_youtube_song\n\n\n\nPreview Data\n\n# Load the Shazam library from a CSV file and return the DataFrame.\nshazamlibrary_df = pd.read_csv('shazamlibrary.csv', header=1)\nshazamlibrary_df\n\n\n\n\n\n\n\n\n\nIndex\nTagTime\nTitle\nArtist\nURL\nTrackKey\n\n\n\n\n0\n1\n2024-06-13\nUntil I Found You (Em Beihold Version)\nStephen Sanchez & Em Beihold\nhttps://www.shazam.com/track/581828191/until-i...\n581828191\n\n\n1\n2\n2024-06-13\nCapitals of Europe: Nordic Region\nKatrina Holland\nhttps://www.shazam.com/track/651866832/capital...\n651866832\n\n\n2\n3\n2024-06-13\nCapitals of Europe: Nordic Region\nKatrina Holland\nhttps://www.shazam.com/track/651866832/capital...\n651866832\n\n\n3\n4\n2024-06-11\nVen Ven\nLotus Beatz\nhttps://www.shazam.com/track/696779462/ven-ven\n696779462\n\n\n4\n5\n2024-05-30\nAva\nFamy\nhttps://www.shazam.com/track/127047942/ava\n127047942\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n222\n223\n2018-03-10\nLose Yourself\nFelax\nhttps://www.shazam.com/track/340620006/lose-yo...\n340620006\n\n\n223\n224\n2018-03-08\nThis Is What It Feels Like (feat. Trevor Guthrie)\nArmin van Buuren\nhttps://www.shazam.com/track/89020928/this-is-...\n83795151\n\n\n224\n225\n2018-03-04\nRoulette\nKaty Perry\nhttps://www.shazam.com/track/358615884/roulette\n358615884\n\n\n225\n226\n2018-03-04\nRoulette\nKaty Perry\nhttps://www.shazam.com/track/358615884/roulette\n358615884\n\n\n226\n227\n2018-02-08\nIf You Need to, Keep Time on Me\nFleet Foxes\nhttps://www.shazam.com/track/357910030/if-you-...\n357910030\n\n\n\n\n\n227 rows × 6 columns\n\n\n\n\n\nFormat data\n\n# Remove Duplicates and Select `Title` and `Artist`.\nshazamlibrary_df = shazamlibrary_df.drop_duplicates(subset=['TrackKey'])[['Title', 'Artist']]\nshazamlibrary_df\n\n\n\n\n\n\n\n\n\nTitle\nArtist\n\n\n\n\n0\nUntil I Found You (Em Beihold Version)\nStephen Sanchez & Em Beihold\n\n\n1\nCapitals of Europe: Nordic Region\nKatrina Holland\n\n\n3\nVen Ven\nLotus Beatz\n\n\n4\nAva\nFamy\n\n\n5\nHero\nBryan Todd Feat. Ashley Argota\n\n\n...\n...\n...\n\n\n221\n恋人心\nWei Xin Yu\n\n\n222\nLose Yourself\nFelax\n\n\n223\nThis Is What It Feels Like (feat. Trevor Guthrie)\nArmin van Buuren\n\n\n224\nRoulette\nKaty Perry\n\n\n226\nIf You Need to, Keep Time on Me\nFleet Foxes\n\n\n\n\n\n201 rows × 2 columns\n\n\n\n\n# Test search\nsearch_title, search_artist = shazamlibrary_df.loc[0].values\nyoutube: YouTube = get_youtube_song(search_title, search_artist)\nyoutube.watch_url\n\n'https://youtube.com/watch?v=kPlSyYtE63M'\n\n\n\n\nCode to run the player\nBelow is the code used to run the server at Huggingface:\n\"\"\"Shazam Playlist to Youtube Playlist\"\"\"\n\nfrom pathlib import Path\nimport random\nfrom typing import Optional\nimport logging\nimport pandas as pd\nfrom pytube import Search, YouTube\nfrom flask import Flask, request, send_from_directory\nfrom sklearn.utils import shuffle as sklearn_shuffle\n\n# https://github.com/pytube/pytube/issues/1270#issuecomment-2100372834\npytube_logger = logging.getLogger('pytube')\npytube_logger.setLevel(logging.ERROR)\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    \"\"\"Route handler for the home page\"\"\"\n    try:\n        return send_from_directory('.', 'index.html')\n    except Exception as e:\n        return str(e)\n    \n@app.route('/video_id', methods=['POST'])\ndef video_id() -&gt; str:\n    \"\"\"Route handler for retrieving the YouTube video ID\"\"\"\n    try:\n        title: str = request.json.get('title')\n        artist: str = request.json.get('artist')\n        youtube: YouTube = get_youtube_song(title, artist)\n        return youtube.video_id\n    except Exception as e:\n        return str(e)\n\n@app.route('/parse_csv', methods=['POST'])\ndef parse_csv():\n    \"\"\"Route handler for parsing the uploaded CSV file\"\"\"\n    try:\n        file = request.files['file']\n        # Process the uploaded file\n        return parse_csv_util(pd.read_csv(file, header=1))\n    except Exception as e:\n        return str(e)\n\n@app.route('/parse_csv_test', methods=['GET'])\ndef parse_csv_test():\n    \"\"\"Route handler for parsing the test CSV file\"\"\"\n    try:\n        # Construct the path to the CSV file\n        csv_path = Path(__file__).parent / 'shazamlibrary.test.csv'\n        return parse_csv_util(pd.read_csv(csv_path, header=1), True)\n    except Exception as e:\n        return str(e)\n\ndef get_youtube_song(title: str, artist: str) -&gt; Optional[YouTube]:\n    \"\"\"Searches for a YouTube video based on the given title and artist\"\"\"\n    search_result = Search(f'{title} by {artist}')\n    return search_result.results[0] if search_result.results else None\n\ndef parse_csv_util(df: pd.DataFrame, shuffle = False):\n    try:\n        df = df.drop_duplicates(subset=['TrackKey'])[['Title', 'Artist']]\n        if shuffle:\n            for random_state in random.sample(range(444, 44444), 3):\n                df = sklearn_shuffle(df, random_state=random_state).reset_index(drop=True)\n        return df.to_json(orient=\"records\")\n    except Exception as e:\n        return str(e)\nBelow is the HTML page that renders the page:\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;ToKnow.ai - Shazam Playlist to Youtube Playlist&lt;/title&gt;\n    &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://www.youtube.com/iframe_api\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        .youtube-player,\n        iframe {\n            width: 100%;\n            height: 100%;\n        }\n\n        .playlist tbody tr {\n            cursor: pointer;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;body class=\"bg-light\"&gt;\n    &lt;div class=\"container my-4\"&gt;\n        &lt;h1 class=\"text-center\"&gt;Convert Your Shazam Playlist to YouTube Playlist&lt;/h1&gt;\n        &lt;p class=\"text-center\"&gt;\n            &lt;em&gt;\n                &lt;i&gt;\n                    Read details or &lt;b&gt;Comment&lt;/b&gt; at\n                    &lt;a target=\"_blank\"\n                        href=\"https://toknow.ai/posts/shazam-playlist-to-youtube-playlist/\"&gt;&lt;b&gt;ToKnow.ai&lt;/b&gt; blog\n                        post.\n                    &lt;/a&gt;\n                &lt;/i&gt;\n            &lt;/em&gt;\n        &lt;/p&gt;\n        &lt;p class=\"text-center\"&gt;\n            Download the CSV of your playlist from\n            &lt;a href=\"https://www.shazam.com/myshazam\" target=\"_blank\"&gt;https://www.shazam.com/myshazam&lt;/a&gt;.\n        &lt;/p&gt;\n        &lt;p class=\"text-center\"&gt;Upload your Shazam Playlist CSV file.&lt;/p&gt;\n        &lt;div class=\"row mx-2 justify-content-center\"&gt;\n            &lt;div class=\"col-md-6\"&gt;\n                &lt;input type=\"file\" class=\"form-control upload-form col-md-6\" accept=\".csv\"&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"row mt-2 justify-content-center\"&gt;\n            &lt;div class=\"col-md-6 d-flex justify-content-around flex-wrap\"&gt;\n                &lt;a id=\"load-test-playlist\" class=\"btn btn-warning btn-sm fst-italic\"&gt;\n                    click here to load test playlist\n                &lt;/a&gt;\n\n                &lt;img class=\"rounded\"\n                    src=\"https://api.visitorbadge.io/api/visitors?path=https://toknow.ai/posts/shazam-playlist-to-youtube-playlist\" /&gt;\n            &lt;/div&gt;\n            \n        &lt;/div&gt;\n\n        &lt;div class=\"row mt-3 justify-content-center\"&gt;\n            &lt;div class=\"col-md-8\"&gt;\n                &lt;div class=\"object-fit-contain border rounded ratio ratio-16x9\"&gt;\n                    &lt;div class=\"youtube-player\"&gt;&lt;/div&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"row mt-5\"&gt;\n            &lt;div class=\"col-md-12 playlist table-responsive\"&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n        const playlistTable = document.querySelector('.playlist');\n        const uploaForm = document.querySelector('.upload-form');\n        let songsPlaylist = []\n        let videoIndex = -1;\n        let youtubePlayer;\n\n        document.querySelector('#load-test-playlist').addEventListener('click', async e =&gt; {\n            try {\n                e.preventDefault();\n\n                const playlist = await(await fetch('/parse_csv_test')).json();\n                generateTable(playlist)\n            } catch (error) {\n                playlistTable.innerHTML = error;\n            }\n        });\n\n        uploaForm.addEventListener('input', e =&gt; {\n            e.preventDefault();\n            if (e.target.files.length == 0) {\n                return;\n            }\n\n            parseCsv(e.target.files[0], playlistTable);\n        });\n        playlistTable.addEventListener('click', e =&gt; {\n            e.preventDefault();\n            const row = event.target.closest('tr');\n            if (row) {\n                const index = row.dataset.index ? Number(row.dataset.index) : undefined;\n                onContinue(undefined, index);\n            }\n        });\n        function resetCurrentPlayingBackground() {\n            playlistTable.querySelectorAll('tbody tr').forEach(row =&gt; {\n                if (Number(row.dataset.index) == videoIndex) {\n                    row.classList.add('bg-warning');\n                } else {\n                    row.classList.remove('bg-warning');\n                }\n            })\n        }\n        function addErrorToCurrentIndex() {\n            playlistTable.querySelectorAll('tbody tr').forEach(row =&gt; {\n                if (Number(row.dataset.index) == videoIndex) {\n                    row.classList.add('bg-danger');\n                }\n            })\n        }\n        async function getVideoId(song) {\n            const response = await fetch(\n                '/video_id',\n                {\n                    headers: { 'Content-Type': 'application/json' },\n                    method: 'POST',\n                    body: JSON.stringify({ title: song.Title, artist: song.Artist })\n                });\n            return await response.text()\n        }\n        async function nextVideo(callback, newIndex = undefined) {\n            newIndex = newIndex &gt;= 0 ? newIndex : (videoIndex + 1)\n            videoIndex = newIndex &lt; songsPlaylist.length ? newIndex : 0;\n            let video_id = await getVideoId(songsPlaylist[videoIndex]);\n            callback(video_id);\n            resetCurrentPlayingBackground();\n        }\n        function generateTable(playlist) {\n            try {\n                songsPlaylist = playlist\n                const tableBody = songsPlaylist.map((i, index) =&gt; `\n                            &lt;tr data-index=\"${index}\"&gt;\n                                &lt;th&gt;${index + 1}&lt;/th&gt;\n                                &lt;th&gt;${i.Title}&lt;/th&gt;\n                                &lt;th&gt;${i.Artist}&lt;/th&gt;\n                            &lt;/tr&gt;`\n                ).join('')\n                playlistTable.innerHTML = `\n                &lt;table class=\"table table-striped table-hover table-bordered rounded\"&gt;\n                    &lt;thead&gt;\n                        &lt;tr&gt;\n                            &lt;th&gt;#&lt;/th&gt;\n                            &lt;th&gt;Title&lt;/th&gt;\n                            &lt;th&gt;Artist&lt;/th&gt;\n                        &lt;/tr&gt;\n                    &lt;/thead&gt;\n                    &lt;tbody&gt;${tableBody}&lt;/tbody&gt;\n                &lt;/table&gt;`\n\n                tryToPlay(() =&gt; {\n                    videoIndex = -1;\n                    onContinue();\n                }, 100)\n            } catch (error) {\n                playlistTable.innerHTML = error;\n            }\n        }\n        async function parseCsv(file, playlistTable) {\n            try {\n                const formData = new FormData();\n                formData.append('file', file);\n                const playlist = await (await fetch('/parse_csv', { method: 'POST', body: formData })).json();\n                generateTable(playlist)\n            } catch (error) {\n                playlistTable.innerHTML = error;\n            }\n        }\n        function initiatePlayer() {\n            const youtubePlayerElement = document.querySelector('.youtube-player');\n            youtubePlayer = window.youtubePlayer = new YT.Player(youtubePlayerElement, {\n                height: '100%',\n                width: '100%',\n                playerVars: { autoplay: 1 },\n                events: {\n                    'onReady': function (event) {\n                        event.target.playVideo()\n                    },\n                    'onStateChange': function (event) {\n                        if (event.data === YT.PlayerState.ENDED) {\n                            onContinue(event?.target);\n                        }\n                    },\n                    'onError': function (event) {\n                        addErrorToCurrentIndex();\n                        onContinue(event?.target);\n                    }\n                }\n            });\n        }\n        function onContinue(player = undefined, newIndex = undefined) {\n            if (songsPlaylist.length &gt; 0) {\n                nextVideo((value) =&gt; {\n                    player = player || youtubePlayer\n                    player.loadVideoById(value);\n                    setTimeout(() =&gt; {\n                        player.playVideo();\n\n                        setTimeout(() =&gt; {\n                            if (player.getPlayerState() != YT.PlayerState.PLAYING) {\n                                player.playVideo();\n                            }\n                        }, 10);\n                    }, 10);\n                }, newIndex);\n            }\n        }\n\n        function tryToPlay(playCallback, timeout) {\n            setTimeout(() =&gt; {\n                if (YT.Player) {\n                    if (!youtubePlayer) {\n                        initiatePlayer();\n                        tryToPlay?.();\n                    } else if (songsPlaylist.length &gt; 0) {\n                        playCallback();\n                    }\n                }\n            }, timeout);\n        }\n\n        tryToPlay(null, 300);\n    &lt;/script&gt;\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#test-the-app",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#test-the-app",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "Test the App",
    "text": "Test the App\n\n/apps/shazam-playlist-to-youtube-playlist\n\n\n          \n        \n          \n             loading test app..."
  },
  {
    "objectID": "posts/shazam-playlist-to-youtube-playlist/index.html#conclusion",
    "href": "posts/shazam-playlist-to-youtube-playlist/index.html#conclusion",
    "title": "Convert Your Shazam Playlist to YouTube Playlist",
    "section": "Conclusion",
    "text": "Conclusion\nThe Shazam to YouTube Playlist converter offers a fantastic, free alternative for enjoying your music discoveries. It bridges the gap between Shazam’s identification capabilities and the vast library of music available on YouTube. Whether you’re a casual music listener or an avid discoverer of new tunes, this tool provides a valuable service without the need for paid subscriptions. Give it a try and turn your Shazam discoveries into a personalized YouTube concert. Happy listening!\n\n\n\n\nDisclaimer: For information only. Accuracy or completeness not guaranteed. Illegal use prohibited. Not professional advice or solicitation. While this tool is free and convenient, always respect copyright laws and support your favorite artists when possible. Read more: /terms-of-service"
  }
]