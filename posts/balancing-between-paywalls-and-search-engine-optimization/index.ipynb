{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- metadata: title -->\n",
    "# Balancing between Paywalls and Search Engine Optimization\n",
    "\n",
    "<!-- metadata: subtitle -->\n",
    "> ### How Media Paywals Should work: A Research Case for Nation Media Group\n",
    "\n",
    "<!-- metadata: keywords, is_array=true -->\n",
    "**Keywords:**\n",
    "  - nation-media-group\n",
    "  - paywalls\n",
    "  - search-engine-optimization\n",
    "  - cloudflare\n",
    "\n",
    "<!-- metadata: categories, is_array=true -->\n",
    "**Categories:**\n",
    "  - cyber-security"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "<!-- metadata: disclaimer, strip_markdown=false -->\n",
    "Please note that this is meant for educational purposes only. You will be peronally liable for any misuse of the information provided here. We [contacted Nation Media Group](https://www.nationmedia.com/contact/) on Jun 3, 2024, 2:30â€¯PM East African Time, but they did not respond. ^[We [contacted Nation Media Group](https://www.nationmedia.com/contact/) through the following emails: <support@nation.africa>, <sales_inquiries@ke.nationmedia.com>, <newsdesk@ke.nationmedia.com>, <publiceditor@ke.nationmedia.com>, <mailbox@ke.nationmedia.com>, <epaper@ke.nationmedia.com>, <Customercare@ke.nationmedia.com>]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Nation Media Group logo](nation-media-group.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maintain independence of content creation, creators and authors need some form of monetization. For text content, one of the most common is advertizing. creators and authors can advertize directly or signup for services such as google adsense. Another form of monetization is publishing premium content that is initially only available to paid users. this is especially common for news media outlets because the most recent contents is usually the most relevant.\n",
    "\n",
    "However, for users to discover your content, they need to find it in search engines such as google or bing or duck duck go. For bet results, it makes sense to allow search engines to see the whole premium article to allow them to suggest it when users search for something similar. This is called search engine optimization. You might have the best news content, but if no one can find them, then its probably useless!\n",
    "\n",
    "The conflict to allow partial access to premium content while still restricting the content to unpaid users is a thin balance that most media house have to maintain. Users often find cleaver ways to circumnavigent the paywalls to see the paid content for free. There are online forums dedicated to discovering these vulnerabilities, such as \"[Bypassing Daily Nation Paywall](https://www.reddit.com/r/Kenya/comments/s96k01/bypassing_daily_nation_paywall/?rdt=48760)\" and \"[You can bypass most soft paywalls with a little CSS knowledge](https://www.reddit.com/r/educationalgifs/comments/lk1not/you_can_bypass_most_soft_paywalls_with_a_little/)\". The raw ideas shared in these forums often simple but require some basic programming skills to execute. In that sense, most people would prefer to just pay that learn how to execute the ideas.\n",
    "\n",
    "However, some users also create browser plugins that automatically do the heavy lifting for the user, allowing them to automatically view the premium content without any effort. Website Browser plugin stores and code repositories that contain plugins that allow paywall bypass are usually taken down and shut down, such as the famous <https://github.com/iamadamdev/bypass-paywalls-chrome>, but not before the code has found a new home, such as <https://github.com/nikolqyy/bypass-paywalls-chrome/releases/tag/most-recent> ^[https://news.ycombinator.com/item?id=41294166].\n",
    "\n",
    "As imagined, this is very time consuming, because you'd have to find out which plugins are currently available to bypass your paywalls. Also, it is not instant. it takes a while to execute a DMCA takedown notice. And even after its successful, someone who has a cloned repo will reupload the code and or plugin and the process continues. There is also a downside with the fact that this only affect publicly available plugins and ideas. Also, the more you try to control, the more it spreads that your website can be bypassed, thereby prompting users who used to happily pay feel like they have been short charged, and start looking for ways to bypass. This strategy also only tends to favor famous plugins and code repositories. The least known repositories are left to grow  because you dont know they exist (eg: <https://github.com/nikolqyy/bypass-paywalls-chrome>), and its very unlikely users who are used to access content for free are going to pay for it even if you disallow the access. there is also the fact that users who have already installed the pluggins will continue to enjoy the premium content without paying. \n",
    "\n",
    "## The Better Solution\n",
    "\n",
    "After a DMCA takedown notice, the most logical next thing is to change some aspects of your website such as class names and aragements of the site contents to make the old plugins not to work. but there is a slightly better solution, one that is scallable, cheap and doesnt compromise on search engine optimization.\n",
    "\n",
    "The solution involves keeping a select list of search engines allowing to read all the premium content for search engine optimization. These may include `google`, `bing`, `duckduckgo`, `yandex`, `baidu`, `yahoo` and `ahrefs`. In your web servers, you would check the ip address of the calling client and do a reverse DNS lookup to findout if the IP address is associated with the whitelisted search engines.\n",
    "\n",
    "This ofcourse is an expensive operation and should be optimized by caching the result for about a week. this means an IP adress that has been found to be associated with a search engine should not be re-evaluated again for about a week. this keeps a good balance between functionality and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nation Media Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic here affects <https://nation.africa/> and <https://www.businessdailyafrica.com/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: \"Show python imports\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add root directory as python path\n",
    "root_dir = os.path.abspath(Path(sys.executable).parents[2])\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The old vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old vulnerability only required css to bypass. One would only need to edit the DOM (Document Object Model) and remove some classes and some elements and the premium contents would be visible. The code below would fully display the premium content to any user! See below javascript code\n",
    "\n",
    "```js\n",
    "setTimeout(() => {\n",
    "    // https://nation.africa/\n",
    "    // Remove the paywall element\n",
    "    document.querySelector('.wall-guard')?.remove();\n",
    "    // Allow copying the text\n",
    "    document.querySelectorAll('.blk-txt')?.forEach(\n",
    "      i => i.classList.remove('blk-txt'));\n",
    "\n",
    "    // https://www.businessdailyafrica.com/\n",
    "    // Remove the paywall spinner\n",
    "    document.querySelector('.spinner')?.remove();\n",
    "    // Remove the paywall element\n",
    "    document.querySelector('.paywall')?.remove();\n",
    "    // Remove the call for action\n",
    "    document.querySelector('.grid-container-medium')?.remove();\n",
    "\n",
    "    // https://www.businessdailyafrica.com/ AND https://nation.africa/\n",
    "    // Show the hidden content\n",
    "    document.querySelectorAll('.paragraph-wrapper.nmgp')?.forEach(\n",
    "      i => i.classList.remove('nmgp'));\n",
    "}, 1)\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "After reporting the issue to them, they added a javascript layer to prevent easy access to the premium content. There is now a javascript code that runs to remove the actual content from the DOM, which means that CSS alone will not show the content.\n",
    "However, there is a way we can silently disable javascript, by refetching the html again and parsing the html text as DOM but without running javascript. this essentially allowes the old CSS method to continue working. See below javascript code:\n",
    "\n",
    "```js\n",
    "setTimeout(async () => {\n",
    "    // remove popup and make page scrollable\n",
    "    const removePopup = (maxRetries, retries) => {\n",
    "        setTimeout(() => {\n",
    "            const popUp = document.querySelector('.fc-ab-root')\n",
    "            if (popUp) {\n",
    "                popUp?.remove()\n",
    "                document.body.style = \"\"\n",
    "            } else if (retries < maxRetries) {\n",
    "                removePopup(maxRetries, retries + 1)\n",
    "            }\n",
    "        }, 300);\n",
    "    };\n",
    "    // re-fetch html from the current link\n",
    "    const htmlString = await fetch(location.href).then(resp => resp.text())\n",
    "    // parse the HTML without Javascript!\n",
    "    const newHtmlDocument = new DOMParser().parseFromString(htmlString, 'text/html');\n",
    "    // https://nation.africa/\n",
    "    // Remove the paywall element\n",
    "    newHtmlDocument.querySelector('.wall-guard')?.remove();\n",
    "    // Allow copying the text\n",
    "    newHtmlDocument.querySelectorAll('.blk-txt')?.forEach(i\n",
    "     => i.classList.remove('blk-txt'));\n",
    "\n",
    "    // https://www.businessdailyafrica.com/\n",
    "    // Remove the paywall spinner\n",
    "    newHtmlDocument.querySelector('.spinner')?.remove();\n",
    "    // Remove the paywall element\n",
    "    newHtmlDocument.querySelector('.paywall')?.remove();\n",
    "    // Remove the call for action\n",
    "    newHtmlDocument.querySelector('.grid-container-medium')?.remove();\n",
    "\n",
    "    // https://www.businessdailyafrica.com/ AND https://nation.africa/\n",
    "    // Show the hidden content\n",
    "    newHtmlDocument.querySelectorAll('.paragraph-wrapper.nmgp')?.forEach(\n",
    "      i => i.classList.remove('nmgp'));\n",
    "    // Enable images\n",
    "    newHtmlDocument.querySelectorAll('img.lazy-img').forEach(\n",
    "      i => i.classList.remove('lazy-img'))\n",
    "    newHtmlDocument.querySelectorAll('img[data-src]').forEach(img => {\n",
    "        const { dataset } = img;\n",
    "        img.src = dataset.src ?? img.src;\n",
    "        img.srcset = dataset.srcset ?? img.srcset;\n",
    "    });\n",
    "    // Remove spinners\n",
    "    newHtmlDocument.querySelectorAll('.spinner').forEach(i => i.remove());\n",
    "    // Remove cloundflare email protection label\n",
    "    newHtmlDocument.querySelector('.__cf_email__')?.closest('.paragraph-wrapper')?.remove();\n",
    "\n",
    "    document.body.outerHTML = newHtmlDocument.body.outerHTML;\n",
    "\n",
    "    removePopup(50, 0)\n",
    "}, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"https://web.archive.org/web/20240601075749/https://nation.africa/kenya/business/inside-world-bank-tough-terms-sh158bn-loan-kenya-4642634\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appropriate Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My suggested fix involves using cloudflare, which nation.africa is already using for DNS and CDN management. create a web worker that checks the IP address. if the ip address is from search engines, then return the extra paid content for SEO, otherwise reduct the extra content. with this, it would still be possible to see the content by routing the request with a https://pagespeed.web.dev/ , which makes it harder than simple jatascript and css!\n",
    "\n",
    "The IP check involves an IP reverse lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python reverse lookup code, and some tests! test with major search engines!\n",
    "\n",
    "\"AllowedSearchBots\": [\n",
    "    \"googlebot.com\",\n",
    "    \"google.com\",\n",
    "    \"search.msn.com\",\n",
    "    \"duckduckgo.com\",\n",
    "    \"yandex.ru\",\n",
    "    \"yandex.net\",\n",
    "    \"yandex.com\",\n",
    "    \"crawl.baidu.com\",\n",
    "    \"crawl.baidu.jp\",\n",
    "    \"crawl.yahoo.net\",\n",
    "    \"ahrefs.com\"\n",
    "  ],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "import socket\n",
    "from ipaddress import ip_address as parse_ip_address\n",
    "\n",
    "async def reverse_dns_lookup(ip_address: str, *host_names: tuple[str, ...]) -> bool:\n",
    "    \"\"\"\n",
    "    Perform reverse DNS lookup.\n",
    "    Usage example:\n",
    "        await reverse_dns_lookup(\"66.249.66.1\", \"googlebot.com\", \"google.com\")\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ip_address : str\n",
    "        the ip address of the client that called the server, \n",
    "        or the header value of \"X-Forwarded-For\" incase a \n",
    "        proxy/CDN such as cloudflare is used!\n",
    "    host_names : list[str]\n",
    "        allowed search engines\n",
    "        eg: \"googlebot.com\", \"search.msn.com\", \"duckduckgo.com\", etc\n",
    "\n",
    "    More Information:\n",
    "    Verifying Googlebot: \n",
    "        https://developers.google.com/search/docs/advanced/crawling/verifying-googlebot\n",
    "    How to access the sitemap.xml file of stackoverflow.com\n",
    "        https://meta.stackexchange.com/a/324471\n",
    "    Reverse IP Domain Check?\n",
    "        https://stackoverflow.com/a/716753/3563013\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(host_names) > 0:\n",
    "            # Raises `ValueError`if ip_address is not valid IPv4 or IPv6 address.\n",
    "            valid_ip_address: str = str(parse_ip_address(ip_address))\n",
    "            # Perform reverse DNS lookup\n",
    "            # Get hostname from IP, eg: ('crawl-66-249-66-1.googlebot.com', [], ['66.249.66.1'])\n",
    "            ip_address_hostname, aliases_1, _ = socket.gethostbyaddr(valid_ip_address)\n",
    "            # Get all IP addesses resolving the hostname (both IPv4 and IPv6)\n",
    "            ip_address_list = list(set(\n",
    "                [ip[4][0] for ip in socket.getaddrinfo(ip_address_hostname, None)]))\n",
    "            # Check if IP matches any of the addresses for the hostname\n",
    "            if valid_ip_address in ip_address_list:\n",
    "                # Perform forward DNS lookup to get all aliases\n",
    "                _, aliases_2, _ = socket.gethostbyname_ex(ip_address_hostname)\n",
    "                all_aliases = list(set([ip_address_hostname] + aliases_1 + aliases_2))\n",
    "                # Check if hostname or its aliases match any of the allowed hosts\n",
    "                return any(\n",
    "                    i for i in host_names \n",
    "                    if any(\n",
    "                        j for j in all_aliases \n",
    "                        if i.casefold().endswith(j.casefold()) \\\n",
    "                            or j.casefold().endswith(i.casefold())))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "await reverse_dns_lookup(\"66.249.66.1\", \"googlebot.com\", \"googleusercontent.com\", \"google.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can tell, doing this for every request is resource intensive, and it is best to cache this for about 7 days. a verified ip address should be allowed to query for a week without firther checks for a week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatives to doing this on the server is doing this on==in a CDN like cloudflare, using web workers in this case. this saves server resources and for a start, its free. web workers intercept a request to the server,  and is able to modify the request and the response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
